{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling  \n",
    "from transformers import Trainer, TrainingArguments \n",
    "import torch  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer \n",
    "import torch.nn.functional as F \n",
    "import os\n",
    "#os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "from torch import nn\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create the list of pins, users and actions\n",
    "pins = list(range(1,1001))\n",
    "users = list(range(1,101))\n",
    "actions = ['click', 'closeup', 'save']\n",
    "\n",
    "# Creating empty DataFrame\n",
    "df = pd.DataFrame(columns=['user', 'pin', 'action', 'timestamp'])\n",
    "\n",
    "# Populating the DataFrame\n",
    "for user in users:\n",
    "    num_pins = np.random.randint(3, 21)  # user engages with min 3 pins and max 20 pins\n",
    "    engaged_pins = np.random.choice(pins, num_pins, replace=False)  # engaged pins for this user\n",
    "    engaged_actions = np.random.choice(actions, num_pins)  # actions for this user\n",
    "    timestamps = [datetime.now() - timedelta(days=x) for x in range(num_pins)]  # random timestamps for user engagement\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        'user': user,\n",
    "        'pin': engaged_pins,\n",
    "        'action': engaged_actions,\n",
    "        'timestamp': timestamps\n",
    "    })\n",
    "    \n",
    "    df = pd.concat([df, temp_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-25 22:27:12.452102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>534</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-24 22:27:12.452679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-23 22:27:12.452684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>357</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-22 22:27:12.452686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>834</td>\n",
       "      <td>closeup</td>\n",
       "      <td>2023-07-21 22:27:12.452688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user  pin   action                  timestamp\n",
       "0    1  709    click 2023-07-25 22:27:12.452102\n",
       "1    1  534    click 2023-07-24 22:27:12.452679\n",
       "2    1  299    click 2023-07-23 22:27:12.452684\n",
       "3    1  357    click 2023-07-22 22:27:12.452686\n",
       "4    1  834  closeup 2023-07-21 22:27:12.452688"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map actions to integers\n",
    "action_to_int = {'click': 0, 'closeup': 1, 'save': 2}\n",
    "df['action'] = df['action'].map(action_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split users into train and test users\n",
    "train_users, test_users = train_test_split(users, test_size=0.2)\n",
    "\n",
    "train_df = df[df['user'].isin(train_users)]\n",
    "test_df = df[df['user'].isin(test_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the max and min timestamp in train_df\n",
    "#cutoff_times is 2 weeks prior to the max timestamp\n",
    "max_timestamp = train_df['timestamp'].max()\n",
    "min_timestamp = train_df['timestamp'].min()\n",
    "cut_off_train = max_timestamp - timedelta(days=14)\n",
    "\n",
    "# Create X_train and y_train\n",
    "X_train = train_df[train_df['timestamp'] > cut_off_train]\n",
    "y_train = train_df[(train_df['timestamp'] <= cut_off_train)]\n",
    "\n",
    "# Create X_test and y_test\n",
    "X_test = test_df[test_df['timestamp'] > cut_off_train]\n",
    "y_test = test_df[(test_df['timestamp'] <= cut_off_train)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-11 22:27:12.452703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>665</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-11 22:27:12.458357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>781</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-10 22:27:12.458358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-09 22:27:12.458360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-11 22:27:12.461942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  pin  action                  timestamp\n",
       "14    1  863       1 2023-07-11 22:27:12.452703\n",
       "14    2  665       0 2023-07-11 22:27:12.458357\n",
       "15    2  781       1 2023-07-10 22:27:12.458358\n",
       "16    2  353       0 2023-07-09 22:27:12.458360\n",
       "14    4  272       1 2023-07-11 22:27:12.461942"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#if the list is empty add 5 0's or list is less then length 5 add 0 to make it 5 and it if greater then 5 trim it\n",
    "\n",
    "def pad_or_trim(lst):\n",
    "    if len(lst) == 0:\n",
    "        return [0,0,0,0,0]\n",
    "    elif len(lst) < 5:\n",
    "        return [0]*(5-len(lst)) + lst\n",
    "    else:\n",
    "        return lst[:5]\n",
    "\n",
    "X_train = X_train.sort_values(by=['timestamp'])\n",
    "X_train = X_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_train['pin'] = X_train['pin'].apply(lambda x: pad_or_trim(x))\n",
    "X_train['action'] = X_train['action'].apply(pad_or_trim)\n",
    "X_train['timestamp'] = X_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_train = y_train.sort_values(by=['timestamp'])\n",
    "y_train = y_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_train['pin'] = y_train['pin'].apply(pad_or_trim)\n",
    "y_train['action'] = y_train['action'].apply(pad_or_trim)\n",
    "y_train['timestamp'] = y_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "\n",
    "X_test = X_test.sort_values(by=['timestamp'])   \n",
    "X_test = X_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_test['pin'] = X_test['pin'].apply(pad_or_trim)\n",
    "X_test['action'] = X_test['action'].apply(pad_or_trim)\n",
    "X_test['timestamp'] = X_test['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_test = y_test.sort_values(by=['timestamp'])\n",
    "y_test = y_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_test['pin'] = y_test['pin'].apply(pad_or_trim)\n",
    "y_test['action'] = y_test['action'].apply(pad_or_trim)\n",
    "y_test['timestamp'] = y_test['timestamp'].apply(pad_or_trim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[532, 164, 688, 779, 19]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-12 22:27:12.458356, 2023-07-13 22:27:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                       pin           action  \\\n",
       "1     2  [532, 164, 688, 779, 19]  [2, 2, 0, 1, 0]   \n",
       "\n",
       "                                           timestamp  \n",
       "1  [2023-07-12 22:27:12.458356, 2023-07-13 22:27:...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 353, 781, 665]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-09 22:27:12.458360, 2023-07-10 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                    pin           action  \\\n",
       "1     2  [0, 0, 353, 781, 665]  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                           timestamp  \n",
       "1  [0, 0, 2023-07-09 22:27:12.458360, 2023-07-10 ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ALL THE PARAMETERS FOR THE MODEL\n",
    "hidden_size = 12 #hidden size of transformer and the final output coming from the transformer\n",
    "pin_embedding_dim = 12 #dimension of the pin embedding\n",
    "action_embedding_dim = 3 #dimension of the action embedding\n",
    "num_attention_heads = 12 #number of attention heads in transformer that is concatenated together to form the final attention head of dimension 768\n",
    "pins_vocab = 1001 #number of pins\n",
    "actions_vocab = 3 #number of actions\n",
    "num_layers = 12 #number of transformer layers in the model which is a replication of the same transformer layer whose input is the output of the previous layer and the output is the input to the next layer\n",
    "dropout = 0.1\n",
    "max_length = 5 #maximum length of the input sequence\n",
    "batch_size = 8 #batch size for training\n",
    "epochs = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  \"\"\"\n",
    "  Creates a single Dense Embedding for each token --> Token Embedding + Positional Embedding\n",
    "  \"\"\"\n",
    "  def __init__(self,pins_vocab,pin_embedding_dim,actions_vocab, action_embedding_dim, hidden_size):\n",
    "    super().__init__()\n",
    "    self.pin_embedding = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "    self.action_type_embedding = nn.Embedding(actions_vocab, action_embedding_dim)\n",
    "    self.linear = nn.Linear(pin_embedding_dim+action_embedding_dim, hidden_size)\n",
    "    self.position_embedding = nn.Embedding(5, hidden_size)\n",
    "    self.layer_norm = nn.LayerNorm(hidden_size, eps= 1e-12)\n",
    "    self.dropout = nn.Dropout()\n",
    "\n",
    "  def forward(self,pin_ids,action_ids):\n",
    "    # pin_ids = [batch_size, seq_len]\n",
    "    # action_ids = [batch_size, seq_len]\n",
    "    # position_ids = [batch_size, seq_len]\n",
    "    batch_size, seq_len = pin_ids.shape\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).expand((batch_size, seq_len))\n",
    "    # position_ids = [batch_size, seq_len]\n",
    "    pin_embeddings = self.pin_embedding(pin_ids)\n",
    "    # pin_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    action_embeddings = self.action_type_embedding(action_ids)\n",
    "    # action_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    position_embeddings = self.position_embedding(position_ids)\n",
    "    #print(position_embeddings.shape)\n",
    "\n",
    "    #concatenate all the pin and action embeddings\n",
    "    embeddings = torch.cat([pin_embeddings, action_embeddings], dim=-1)\n",
    "    #apply a linear layer to get the embeddings to the desired hidden size\n",
    "\n",
    "    embeddings = self.linear(embeddings)\n",
    "    #print(embeddings.shape)\n",
    "\n",
    "    #add the position embeddings\n",
    "    embeddings = embeddings + position_embeddings\n",
    "\n",
    "    #embeddings = pin_embeddings + action_embeddings + position_embeddings\n",
    "    # embeddings = [batch_size, seq_len, hidden_size]\n",
    "    embeddings = self.layer_norm(embeddings)\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(5, dtype=torch.long).expand((8, 5))\n",
    "embedded =  nn.Embedding(5, 3)\n",
    "position_embeddings = embedded(position_ids)\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 12])\n",
      "torch.Size([2, 5, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0000, -0.0000,  0.0000, -1.9729,  1.9645,  0.7143,\n",
       "          -1.0371, -0.0000, -1.7302,  3.0014,  2.3532],\n",
       "         [ 2.3744, -5.0341,  0.0000,  1.6825, -0.7166,  0.0000, -1.3007,\n",
       "           0.0000,  0.1960, -2.4292,  0.7411,  0.8138],\n",
       "         [ 0.0000, -0.5715,  0.0000, -3.3566,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000, -0.0000,  0.0000,  1.5766],\n",
       "         [ 0.6187, -0.0000, -0.0000,  0.0000,  0.7198,  0.0000, -0.0000,\n",
       "           2.2676,  2.7325, -0.7301, -0.0492, -2.4983],\n",
       "         [ 0.0000, -0.7059,  0.0000, -0.0000,  0.0000,  1.4348, -0.0000,\n",
       "           0.0000, -0.0000, -0.0000,  1.0154,  1.0361]],\n",
       "\n",
       "        [[ 0.4384, -2.0169, -0.0000,  0.0000, -1.9729,  0.0000,  0.7143,\n",
       "          -1.0371, -2.6834, -1.7302,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  1.4207,  1.6825, -0.7166,  0.0000, -1.3007,\n",
       "           0.0000,  0.0000, -0.0000,  0.7411,  0.8138],\n",
       "         [ 1.6773, -0.0000,  1.8410, -3.3566,  0.0000,  0.6709, -0.0000,\n",
       "          -0.0000,  0.0000, -3.0320,  0.8062,  0.0000],\n",
       "         [ 0.0000, -0.0000, -0.0000,  1.6821,  0.7198,  2.2245, -0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0492, -0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  1.4348, -0.0000,\n",
       "           0.0000, -0.1687, -4.0064,  1.0154,  1.0361]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = Embeddings(1000,256,3,3,12)\n",
    "\n",
    "#sample embeddings\n",
    "\n",
    "pin_ids = torch.tensor([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "print(pin_ids.shape)\n",
    "action_ids = torch.tensor([[1,2,0,2,1],[1,2,0,2,1]])\n",
    "embedded(pin_ids,action_ids)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pinformer_dataset(Dataset):\n",
    "  \"\"\"\n",
    "  Creates a dataset for the pinformer model\n",
    "  \"\"\"\n",
    "  def __init__(self,df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    pin_ids = torch.tensor(self.df.iloc[idx]['pin'])\n",
    "    action_ids = torch.tensor(self.df.iloc[idx]['action'])\n",
    "\n",
    "    target_pin_ids = torch.tensor(self.df.iloc[idx]['pin']) \n",
    "    #pick one random non-zero pin from the list of pins and make it the target pin\n",
    "    target_pin_ids = target_pin_ids[target_pin_ids != 0]\n",
    "    target_pin_ids = target_pin_ids[torch.randperm(len(target_pin_ids))]\n",
    "    target_pin_ids = target_pin_ids[0]\n",
    "\n",
    "    return pin_ids,action_ids, target_pin_ids\n",
    "\n",
    "train_df = pinformer_dataset(X_train)\n",
    "test_df = pinformer_dataset(X_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_df, batch_size=8, shuffle=False) #shuffle is false because we want to preserve the order of the sequence so user 1 embeddings are in sequence and user 2 embeddings are in sequence and so on and can be pulled \n",
    "test_data_loader = DataLoader(test_df, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, embed_dim, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim #dimension of one head \n",
    "    #infeatures=embed_dim\n",
    "    #outfeatures=head_dim\n",
    "    self.q = nn.Linear(embed_dim, head_dim)\n",
    "    self.k = nn.Linear(embed_dim, head_dim)\n",
    "    self.v = nn.Linear(embed_dim, head_dim)\n",
    "    \n",
    "  \n",
    "  def causal_mask(self,batch_size,size, dtype):  \n",
    "    mask = torch.tril(torch.ones(size,size)).unsqueeze(0)\n",
    "    return mask\n",
    "    \n",
    "  \n",
    "      \n",
    "  def scaled_dot_product_attention(self,query, key, value):\n",
    "    dim_k = query.size(dim=-1)  \n",
    "    #print(dim_k)    \n",
    "    #print(f'Dimension of the q,k,v Matrix [Batch_size, seq_len, Head_dim] of One Head {dim_k}')\n",
    "    scores = torch.bmm(query,key.transpose(1,2))/ sqrt(dim_k)  #[(1,5,768)*(1,768,5)]/sqrt(768) >>> [batch_size,5,5] \n",
    "    \n",
    "    mask = self.causal_mask(scores.size(0),scores.size(1),dtype=torch.int32)\n",
    "    #print(mask)\n",
    "    scores = scores.masked_fill(mask==0, float(0)) \n",
    "    weights = F.softmax(scores, dim=-1) #[batch_size,5,5]\n",
    "    #print(weights)\n",
    "    #print(f'Softmax for each column across one row {weights}')\n",
    "    weights_dot_values = torch.bmm(weights,value)  #[batch_size,5,5]*[batch_size,5,64] >>> [batch_size,5,64]\n",
    "    #print(f'Last Step is to multiply weights and values {weights_dot_values.shape}')\n",
    "    return weights_dot_values \n",
    "\n",
    "  def forward(self, hidden_state):\n",
    "    #head_state = [batch_size, seq_len, embed_dim]\n",
    "    #print(f'Input Embedding for Each Token with X Matrix {hidden_state.size()}')\n",
    "    #q = X*W_q\n",
    "    q = self.q(hidden_state) #q = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Query Matrix W_q {q.size()}')\n",
    "    k = self.k(hidden_state) #k = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Key Matrix W_k {k.size()}')\n",
    "    v = self.k(hidden_state) #v = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Value Matrix W_k {v.size()}')\n",
    "    #print('-----------------Calculating Self Attention--------------------')\n",
    "    attn_outputs = self.scaled_dot_product_attention(q,k,v) #attn_outputs = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the attention Output with one Head and Head Dimension {self.head_dim} is {attn_outputs.size()}')\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 12])\n",
      "torch.Size([2, 5, 12])\n",
      "torch.Size([2, 5, 12])\n",
      "torch.Size([2, 5, 12])\n",
      "Softmax for each column across one row tensor([[[0.1813, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1159, 0.1269, 0.2524, 0.2524, 0.2524],\n",
      "         [0.2127, 0.2061, 0.1703, 0.2055, 0.2055],\n",
      "         [0.2413, 0.1478, 0.2441, 0.0915, 0.2753],\n",
      "         [0.0726, 0.1731, 0.1666, 0.3286, 0.2592]],\n",
      "\n",
      "        [[0.0613, 0.2347, 0.2347, 0.2347, 0.2347],\n",
      "         [0.2511, 0.1284, 0.2068, 0.2068, 0.2068],\n",
      "         [0.1504, 0.1901, 0.1602, 0.2497, 0.2497],\n",
      "         [0.3503, 0.2598, 0.0594, 0.0975, 0.2329],\n",
      "         [0.2109, 0.1663, 0.1790, 0.1324, 0.3115]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2134,  0.5469, -0.6747,  0.0682,  0.3882,  0.7637, -0.4783,\n",
       "           0.1805,  0.0934, -1.1307,  0.9239,  0.1898],\n",
       "         [-0.1293,  0.4728, -0.6514,  0.3049,  0.2166,  0.6398, -0.6145,\n",
       "           0.0852,  0.2100, -1.0837,  0.8449,  0.1860],\n",
       "         [-0.2304,  0.5642, -0.6642,  0.0091,  0.4302,  0.7815, -0.4727,\n",
       "           0.1852,  0.0538, -1.1268,  0.9252,  0.1836],\n",
       "         [-0.1992,  0.2660, -0.7336, -0.1756,  0.3842,  0.7326, -0.3967,\n",
       "           0.1669,  0.3257, -1.0714,  0.5576,  0.0425],\n",
       "         [-0.0995,  0.6450, -0.6679,  0.4365,  0.2551,  0.6681, -0.7024,\n",
       "           0.0183,  0.1056, -1.1319,  1.0081,  0.2581]],\n",
       "\n",
       "        [[ 0.2846,  0.3132, -0.2668,  0.6293,  0.4161,  0.3875, -0.3811,\n",
       "           0.0883,  0.3151, -0.6666,  0.0197,  0.5414],\n",
       "         [ 0.4155,  0.2769, -0.4094,  0.5609,  0.6215,  0.5521, -0.1947,\n",
       "           0.2095,  0.2913, -0.5987, -0.0062,  0.5744],\n",
       "         [ 0.3952,  0.2424, -0.3790,  0.5326,  0.5539,  0.4447, -0.3030,\n",
       "           0.1818,  0.3622, -0.6710, -0.0013,  0.5596],\n",
       "         [ 0.2920, -0.0640, -0.5971,  0.1407,  0.6794,  0.6278, -0.1808,\n",
       "           0.4006,  0.5099, -0.8138, -0.0444,  0.3538],\n",
       "         [ 0.3904,  0.1354, -0.4661,  0.4074,  0.5259,  0.6000, -0.3675,\n",
       "           0.0689,  0.4255, -0.6773, -0.0829,  0.3549]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a single attention head\n",
    "attention_head = AttentionHead(hidden_size,12)\n",
    "\n",
    "#sample input first calculate the embeddings for the input\n",
    "pin_ids = torch.tensor([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "print(pin_ids.shape)\n",
    "action_ids = torch.tensor([[1,2,0,2,1],[1,2,0,2,1]])\n",
    "embedded(pin_ids,action_ids)\n",
    "\n",
    "#pass the embeddings to the attention head\n",
    "attention_head(embedded(pin_ids,action_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super().__init__()\n",
    "    embed_dim = hidden_size\n",
    "    num_heads = num_attention_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "    self.heads = [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] #initializing all the heads\n",
    "    self.w_0 = nn.Linear(embed_dim,embed_dim) #the purpose of this linear layer is to concatenate all the heads together to form the final output of the multihead attention\n",
    "\n",
    "  def forward(self,hidden_state):\n",
    "    '''\n",
    "    hidden_state: Input Embedding with dimensions [batch_size, seq_len, embedding_dimension]\n",
    "    '''\n",
    "    attention_outputs = [head(hidden_state) for head in self.heads] #Calculating Self-Attention on each head\n",
    "    contcat_attn_outputs_allheads = torch.cat(attention_outputs, dim=-1) #[batch_size,seq_len, embed_dim]\n",
    "    Z =   self.w_0(contcat_attn_outputs_allheads) #[batch_size, seq_len, embed_dim]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,hidden_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(hidden_size, 3072)\n",
    "    self.linear2 = nn.Linear(3072, hidden_size)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  \n",
    "  def forward(self, attention_outputs):\n",
    "    output_l1 = self.linear1(attention_outputs)\n",
    "    activated_outputs = self.gelu(output_l1)\n",
    "    output_l2 = self.linear2(activated_outputs)\n",
    "    output = self.dropout(output_l2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super(TransformerDecoderLayer,self).__init__()\n",
    "    self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "    self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "    self.multi_attention = MultiHeadAttention(hidden_size, num_attention_heads)\n",
    "    self.feedforward = FeedForward(hidden_size)\n",
    "\n",
    "  def forward(self, input_embeddings):\n",
    "     #pre-layer normalization approach\n",
    "     \n",
    "     #Step 1: Applying Layer Normalization to Input Embeddings\n",
    "     normalized_input_embeddings = self.layer_norm1(input_embeddings)\n",
    "     \n",
    "     #Step 2: Applying MultiHeadAttention to Normalized Output\n",
    "     multi_head_attn = self.multi_attention(normalized_input_embeddings)\n",
    "     \n",
    "     #Step 3: Add input embeddings to the Multihead Attention Output\n",
    "     skip_connection_1 = input_embeddings + multi_head_attn\n",
    "\n",
    "     #step 4: Pass the output to another Layer Normalization \n",
    "     layer_norm_2 = self.layer_norm2(skip_connection_1)\n",
    "\n",
    "     #Step 5: Adding skip connection 1 outputs to the output of the FeedForward Network (applied on Step 4)\n",
    "     skip_connection_2 = skip_connection_1 + self.feedforward(layer_norm_2)\n",
    "     #print(f'output of MultiHeadAttention and FeedForward Network is {skip_connection_2.shape}')\n",
    "     return skip_connection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDecoder(nn.Module):\n",
    "  def __init__(self,num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = Embeddings(pins_vocab, pin_embedding_dim, actions_vocab, action_embedding_dim, hidden_size)\n",
    "    self.layers = nn.ModuleList([TransformerDecoderLayer(hidden_size, num_attention_heads) for _ in range(num_layers)]) \n",
    "                                \n",
    "  def forward(self, pin_ids, action_ids):\n",
    "    embeddings = self.embedding(pin_ids, action_ids) #in: [batch_size, seq_len] out: [batch_size, seq_len, hidden_size]\n",
    "    for layer in self.layers:\n",
    "      embeddings = layer(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[240, 986, 1000, 675, 995]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]['pin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_decoder = TransferDecoder(num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#need to pull embeddings for each user and then store it in a dictionary\n",
    "user1_pins = torch.tensor(X_train.iloc[0]['pin']).unsqueeze(0) #unserequeeze is used to add a dimension to the tensor for batch size dimension is [1,5] for batch size 1 and sequence length 5\n",
    "user1_actions = torch.tensor(X_train.iloc[0]['action']).unsqueeze(0) #unserequeeze is used to add a dimension to the tensor for batch size dimension is [1,5] for batch size 1 and sequence length 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 5, 12])\n",
      "Softmax for each column across one row tensor([[[0.2023, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1910, 0.2268, 0.1941, 0.1941, 0.1941],\n",
      "         [0.1998, 0.1550, 0.2549, 0.1952, 0.1952],\n",
      "         [0.1958, 0.2675, 0.1451, 0.1901, 0.2015],\n",
      "         [0.1964, 0.1817, 0.2117, 0.1979, 0.2123]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1971, 0.2007, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2055, 0.1762, 0.2061, 0.2061, 0.2061],\n",
      "         [0.1864, 0.2475, 0.1953, 0.1854, 0.1854],\n",
      "         [0.1837, 0.2518, 0.1935, 0.1883, 0.1826],\n",
      "         [0.1739, 0.3043, 0.1906, 0.1816, 0.1495]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2286, 0.1693, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1827, 0.2035, 0.2311, 0.1914, 0.1914],\n",
      "         [0.2396, 0.2119, 0.1833, 0.1379, 0.2272],\n",
      "         [0.1493, 0.1759, 0.2135, 0.3123, 0.1490]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1766, 0.2058, 0.2058, 0.2058, 0.2058],\n",
      "         [0.2059, 0.1890, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2149, 0.1722, 0.2057, 0.2036, 0.2036],\n",
      "         [0.1597, 0.2713, 0.1772, 0.2102, 0.1816],\n",
      "         [0.2075, 0.1555, 0.1960, 0.1786, 0.2624]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1363, 0.2159, 0.2159, 0.2159, 0.2159],\n",
      "         [0.1793, 0.1572, 0.2211, 0.2211, 0.2211],\n",
      "         [0.1769, 0.1448, 0.1916, 0.2434, 0.2434],\n",
      "         [0.1948, 0.1543, 0.2138, 0.1547, 0.2825],\n",
      "         [0.2008, 0.2111, 0.1968, 0.2110, 0.1803]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2348, 0.1913, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2326, 0.1283, 0.2130, 0.2130, 0.2130],\n",
      "         [0.2562, 0.1115, 0.1795, 0.2265, 0.2265],\n",
      "         [0.2571, 0.0545, 0.1324, 0.3517, 0.2043],\n",
      "         [0.1617, 0.3205, 0.2166, 0.1408, 0.1604]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1818, 0.2221, 0.1987, 0.1987, 0.1987],\n",
      "         [0.2288, 0.1896, 0.1607, 0.2105, 0.2105],\n",
      "         [0.1685, 0.1926, 0.2166, 0.2435, 0.1788],\n",
      "         [0.2655, 0.1834, 0.1325, 0.0958, 0.3229]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1836, 0.2041, 0.2041, 0.2041, 0.2041],\n",
      "         [0.2157, 0.1961, 0.1961, 0.1961, 0.1961],\n",
      "         [0.1868, 0.2067, 0.1932, 0.2067, 0.2067],\n",
      "         [0.1986, 0.2015, 0.1996, 0.1987, 0.2015],\n",
      "         [0.2076, 0.2717, 0.2270, 0.2085, 0.0852]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1990, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1942, 0.2203, 0.1952, 0.1952, 0.1952],\n",
      "         [0.1914, 0.2038, 0.2211, 0.1919, 0.1919],\n",
      "         [0.2091, 0.1948, 0.1776, 0.2101, 0.2085],\n",
      "         [0.2610, 0.2045, 0.1489, 0.2655, 0.1201]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3235, 0.1691, 0.1691, 0.1691, 0.1691],\n",
      "         [0.3545, 0.1444, 0.1671, 0.1671, 0.1671],\n",
      "         [0.1650, 0.2204, 0.1941, 0.2103, 0.2103],\n",
      "         [0.2280, 0.1953, 0.2090, 0.1674, 0.2003],\n",
      "         [0.1890, 0.1998, 0.1950, 0.2112, 0.2051]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2025, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2002, 0.1996, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1863, 0.2004, 0.2376, 0.1879, 0.1879],\n",
      "         [0.1677, 0.1921, 0.2639, 0.2058, 0.1704],\n",
      "         [0.1921, 0.1970, 0.2090, 0.1995, 0.2025]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1549, 0.2113, 0.2113, 0.2113, 0.2113],\n",
      "         [0.1710, 0.1962, 0.2109, 0.2109, 0.2109],\n",
      "         [0.2437, 0.1776, 0.2781, 0.1503, 0.1503],\n",
      "         [0.2140, 0.1582, 0.2427, 0.2502, 0.1349],\n",
      "         [0.2043, 0.1888, 0.2112, 0.2128, 0.1828]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1836, 0.2041, 0.2041, 0.2041, 0.2041],\n",
      "         [0.1469, 0.2270, 0.2087, 0.2087, 0.2087],\n",
      "         [0.1007, 0.2324, 0.2713, 0.1978, 0.1978],\n",
      "         [0.0453, 0.1153, 0.1370, 0.6061, 0.0963],\n",
      "         [0.2050, 0.1959, 0.1942, 0.1807, 0.2243]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1819, 0.2045, 0.2045, 0.2045, 0.2045],\n",
      "         [0.1955, 0.1814, 0.2077, 0.2077, 0.2077],\n",
      "         [0.1648, 0.1234, 0.2947, 0.2085, 0.2085],\n",
      "         [0.1996, 0.1975, 0.2038, 0.1977, 0.2013],\n",
      "         [0.1940, 0.1833, 0.2172, 0.1842, 0.2213]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1803, 0.2049, 0.2049, 0.2049, 0.2049],\n",
      "         [0.1769, 0.2398, 0.1944, 0.1944, 0.1944],\n",
      "         [0.3145, 0.0937, 0.1603, 0.2158, 0.2158],\n",
      "         [0.2698, 0.1538, 0.1973, 0.1525, 0.2265],\n",
      "         [0.2863, 0.1027, 0.1617, 0.1010, 0.3483]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2633, 0.1842, 0.1842, 0.1842, 0.1842],\n",
      "         [0.1667, 0.2111, 0.2074, 0.2074, 0.2074],\n",
      "         [0.1506, 0.2215, 0.1974, 0.2152, 0.2152],\n",
      "         [0.1935, 0.2030, 0.2001, 0.2011, 0.2023],\n",
      "         [0.2036, 0.1988, 0.2002, 0.1998, 0.1976]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1949, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2269, 0.2869, 0.1621, 0.1621, 0.1621],\n",
      "         [0.1668, 0.1435, 0.2756, 0.2071, 0.2071],\n",
      "         [0.2247, 0.2555, 0.1466, 0.1861, 0.1870],\n",
      "         [0.1947, 0.1914, 0.2062, 0.1997, 0.2079]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1632, 0.2092, 0.2092, 0.2092, 0.2092],\n",
      "         [0.2225, 0.2322, 0.1818, 0.1818, 0.1818],\n",
      "         [0.2170, 0.2299, 0.2231, 0.1650, 0.1650],\n",
      "         [0.2184, 0.2267, 0.2223, 0.1496, 0.1829],\n",
      "         [0.2327, 0.2451, 0.2386, 0.1370, 0.1466]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1930, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1768, 0.1698, 0.2178, 0.2178, 0.2178],\n",
      "         [0.1788, 0.1717, 0.2094, 0.2200, 0.2200],\n",
      "         [0.1846, 0.1785, 0.2101, 0.2079, 0.2188],\n",
      "         [0.1762, 0.1646, 0.2295, 0.2246, 0.2051]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1931, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1988, 0.1955, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1613, 0.1206, 0.2991, 0.2095, 0.2095],\n",
      "         [0.1710, 0.1299, 0.3064, 0.1738, 0.2189],\n",
      "         [0.2001, 0.2006, 0.1990, 0.2000, 0.2003]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2003, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2099, 0.1236, 0.2221, 0.2221, 0.2221],\n",
      "         [0.1647, 0.3463, 0.1849, 0.1521, 0.1521],\n",
      "         [0.2095, 0.0883, 0.1832, 0.2893, 0.2297],\n",
      "         [0.1487, 0.4396, 0.1760, 0.0992, 0.1364]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2212, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1819, 0.1988, 0.2064, 0.2064, 0.2064],\n",
      "         [0.1820, 0.2062, 0.1770, 0.2174, 0.2174],\n",
      "         [0.1960, 0.1881, 0.1978, 0.2331, 0.1849],\n",
      "         [0.2067, 0.2326, 0.2013, 0.1251, 0.2344]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1693, 0.2077, 0.2077, 0.2077, 0.2077],\n",
      "         [0.2140, 0.3663, 0.1399, 0.1399, 0.1399],\n",
      "         [0.2046, 0.2240, 0.1903, 0.1906, 0.1906],\n",
      "         [0.2249, 0.4353, 0.1321, 0.0742, 0.1334],\n",
      "         [0.1174, 0.0600, 0.2016, 0.3620, 0.2590]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2054, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.2667, 0.2075, 0.1753, 0.1753, 0.1753],\n",
      "         [0.1055, 0.1935, 0.1191, 0.2910, 0.2910],\n",
      "         [0.1724, 0.1967, 0.1770, 0.2389, 0.2150],\n",
      "         [0.0946, 0.1686, 0.1062, 0.3936, 0.2370]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1652, 0.2087, 0.2087, 0.2087, 0.2087],\n",
      "         [0.2203, 0.2177, 0.1873, 0.1873, 0.1873],\n",
      "         [0.1933, 0.1950, 0.1730, 0.2193, 0.2193],\n",
      "         [0.1912, 0.1955, 0.1466, 0.2077, 0.2590],\n",
      "         [0.1943, 0.1906, 0.2452, 0.1807, 0.1891]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2314, 0.1921, 0.1921, 0.1921, 0.1921],\n",
      "         [0.1930, 0.1977, 0.2031, 0.2031, 0.2031],\n",
      "         [0.2303, 0.2148, 0.1573, 0.1988, 0.1988],\n",
      "         [0.1254, 0.1453, 0.2804, 0.2780, 0.1710],\n",
      "         [0.2276, 0.2192, 0.1852, 0.1856, 0.1825]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2196, 0.1951, 0.1951, 0.1951, 0.1951],\n",
      "         [0.2033, 0.2434, 0.1844, 0.1844, 0.1844],\n",
      "         [0.2001, 0.2054, 0.1999, 0.1973, 0.1973],\n",
      "         [0.2015, 0.2158, 0.2008, 0.1878, 0.1941],\n",
      "         [0.1992, 0.2248, 0.1979, 0.1759, 0.2023]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1474, 0.2131, 0.2131, 0.2131, 0.2131],\n",
      "         [0.1562, 0.1844, 0.2198, 0.2198, 0.2198],\n",
      "         [0.1138, 0.1736, 0.1695, 0.2715, 0.2715],\n",
      "         [0.1266, 0.2006, 0.1954, 0.1509, 0.3265],\n",
      "         [0.2684, 0.1890, 0.1928, 0.2348, 0.1150]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2414, 0.1896, 0.1896, 0.1896, 0.1896],\n",
      "         [0.2530, 0.1590, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1395, 0.2475, 0.2304, 0.1913, 0.1913],\n",
      "         [0.2256, 0.1827, 0.1876, 0.2032, 0.2009],\n",
      "         [0.1053, 0.2513, 0.2254, 0.1619, 0.2562]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2036, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2300, 0.1838, 0.1954, 0.1954, 0.1954],\n",
      "         [0.1960, 0.1815, 0.2518, 0.1854, 0.1854],\n",
      "         [0.1369, 0.1147, 0.2430, 0.3849, 0.1204],\n",
      "         [0.1955, 0.1931, 0.2037, 0.2105, 0.1972]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2027, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.1697, 0.2961, 0.1781, 0.1781, 0.1781],\n",
      "         [0.1865, 0.2302, 0.2036, 0.1899, 0.1899],\n",
      "         [0.2130, 0.1673, 0.1926, 0.2185, 0.2086],\n",
      "         [0.1822, 0.2662, 0.2135, 0.1751, 0.1629]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2162, 0.1960, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1970, 0.2226, 0.1935, 0.1935, 0.1935],\n",
      "         [0.1842, 0.2529, 0.2111, 0.1759, 0.1759],\n",
      "         [0.1884, 0.2377, 0.2082, 0.1835, 0.1822],\n",
      "         [0.1962, 0.2081, 0.2012, 0.1949, 0.1996]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2646, 0.1838, 0.1838, 0.1838, 0.1838],\n",
      "         [0.2007, 0.2004, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2227, 0.2140, 0.1746, 0.1943, 0.1943],\n",
      "         [0.2044, 0.2014, 0.1868, 0.2132, 0.1943],\n",
      "         [0.2397, 0.2267, 0.1707, 0.2810, 0.0818]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1554, 0.2112, 0.2112, 0.2112, 0.2112],\n",
      "         [0.2099, 0.1951, 0.1983, 0.1983, 0.1983],\n",
      "         [0.1894, 0.2020, 0.2102, 0.1992, 0.1992],\n",
      "         [0.2834, 0.1912, 0.1500, 0.1666, 0.2087],\n",
      "         [0.1581, 0.1955, 0.2229, 0.2106, 0.2128]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1885, 0.2029, 0.2029, 0.2029, 0.2029],\n",
      "         [0.2081, 0.1805, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1975, 0.2025, 0.2035, 0.1982, 0.1982],\n",
      "         [0.2076, 0.1922, 0.1891, 0.2058, 0.2052],\n",
      "         [0.2023, 0.2001, 0.1996, 0.2020, 0.1960]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1781, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.1530, 0.1870, 0.2200, 0.2200, 0.2200],\n",
      "         [0.2277, 0.2076, 0.1793, 0.1927, 0.1927],\n",
      "         [0.1973, 0.1995, 0.2031, 0.1988, 0.2013],\n",
      "         [0.3248, 0.2240, 0.1243, 0.2547, 0.0722]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1880, 0.2030, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1935, 0.2010, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2738, 0.1753, 0.2166, 0.1671, 0.1671],\n",
      "         [0.1292, 0.2092, 0.1664, 0.2750, 0.2202],\n",
      "         [0.2990, 0.1954, 0.2391, 0.1536, 0.1129]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1843, 0.2039, 0.2039, 0.2039, 0.2039],\n",
      "         [0.2599, 0.2702, 0.1567, 0.1567, 0.1567],\n",
      "         [0.1818, 0.1839, 0.3205, 0.1569, 0.1569],\n",
      "         [0.1787, 0.1801, 0.2636, 0.2161, 0.1615],\n",
      "         [0.2814, 0.2753, 0.0930, 0.1636, 0.1867]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2637, 0.1841, 0.1841, 0.1841, 0.1841],\n",
      "         [0.3790, 0.2297, 0.1305, 0.1305, 0.1305],\n",
      "         [0.2430, 0.2143, 0.1707, 0.1860, 0.1860],\n",
      "         [0.2415, 0.2114, 0.1659, 0.1993, 0.1818],\n",
      "         [0.1559, 0.1782, 0.2273, 0.1890, 0.2496]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1953, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1810, 0.2548, 0.1881, 0.1881, 0.1881],\n",
      "         [0.2009, 0.0551, 0.3961, 0.1740, 0.1740],\n",
      "         [0.2112, 0.1377, 0.2643, 0.1856, 0.2013],\n",
      "         [0.1601, 0.0560, 0.2778, 0.1166, 0.3895]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2121, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1864, 0.2097, 0.2013, 0.2013, 0.2013],\n",
      "         [0.1652, 0.2281, 0.1986, 0.2040, 0.2040],\n",
      "         [0.2245, 0.1890, 0.2035, 0.1823, 0.2006],\n",
      "         [0.2310, 0.1754, 0.1975, 0.1655, 0.2306]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2394, 0.1902, 0.1902, 0.1902, 0.1902],\n",
      "         [0.0982, 0.3770, 0.1749, 0.1749, 0.1749],\n",
      "         [0.2316, 0.1458, 0.2428, 0.1899, 0.1899],\n",
      "         [0.1679, 0.3225, 0.1570, 0.1305, 0.2222],\n",
      "         [0.2196, 0.1283, 0.2320, 0.2703, 0.1498]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2179, 0.1955, 0.1955, 0.1955, 0.1955],\n",
      "         [0.1876, 0.2109, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1945, 0.2045, 0.2008, 0.2001, 0.2001],\n",
      "         [0.1931, 0.2199, 0.2096, 0.1695, 0.2079],\n",
      "         [0.1909, 0.2231, 0.2106, 0.1633, 0.2121]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1971, 0.2007, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1885, 0.2004, 0.2037, 0.2037, 0.2037],\n",
      "         [0.1931, 0.2080, 0.1746, 0.2121, 0.2121],\n",
      "         [0.2029, 0.2214, 0.1801, 0.1691, 0.2266],\n",
      "         [0.2025, 0.2118, 0.1904, 0.1842, 0.2111]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1830, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.1448, 0.1661, 0.2297, 0.2297, 0.2297],\n",
      "         [0.3564, 0.2404, 0.2137, 0.0947, 0.0947],\n",
      "         [0.2308, 0.2106, 0.2049, 0.1841, 0.1696],\n",
      "         [0.2078, 0.2033, 0.2019, 0.1968, 0.1902]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2445, 0.1889, 0.1889, 0.1889, 0.1889],\n",
      "         [0.1848, 0.1809, 0.2114, 0.2114, 0.2114],\n",
      "         [0.2014, 0.2185, 0.3386, 0.1207, 0.1207],\n",
      "         [0.1697, 0.1506, 0.0791, 0.2403, 0.3602],\n",
      "         [0.1958, 0.2052, 0.2643, 0.1708, 0.1639]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1713, 0.2072, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1673, 0.2138, 0.2063, 0.2063, 0.2063],\n",
      "         [0.1735, 0.2227, 0.1745, 0.2147, 0.2147],\n",
      "         [0.2939, 0.1419, 0.2889, 0.1175, 0.1579],\n",
      "         [0.1543, 0.2569, 0.1562, 0.2931, 0.1396]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1848, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1934, 0.1781, 0.2095, 0.2095, 0.2095],\n",
      "         [0.1936, 0.1785, 0.2090, 0.2094, 0.2094],\n",
      "         [0.2060, 0.1834, 0.2297, 0.1507, 0.2303],\n",
      "         [0.2007, 0.1983, 0.2029, 0.1944, 0.2037]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1776, 0.2056, 0.2056, 0.2056, 0.2056],\n",
      "         [0.2807, 0.1607, 0.1862, 0.1862, 0.1862],\n",
      "         [0.2218, 0.1821, 0.2124, 0.1918, 0.1918],\n",
      "         [0.2157, 0.1628, 0.2028, 0.2434, 0.1753],\n",
      "         [0.2203, 0.1704, 0.2083, 0.2460, 0.1551]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1720, 0.2070, 0.2070, 0.2070, 0.2070],\n",
      "         [0.2065, 0.2090, 0.1948, 0.1948, 0.1948],\n",
      "         [0.1943, 0.1896, 0.1789, 0.2186, 0.2186],\n",
      "         [0.2158, 0.2108, 0.1994, 0.1325, 0.2415],\n",
      "         [0.1897, 0.1938, 0.2038, 0.2948, 0.1178]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1170, 0.2208, 0.2208, 0.2208, 0.2208],\n",
      "         [0.1800, 0.1551, 0.2216, 0.2216, 0.2216],\n",
      "         [0.2001, 0.2256, 0.2355, 0.1694, 0.1694],\n",
      "         [0.1966, 0.2126, 0.2186, 0.1958, 0.1764],\n",
      "         [0.1823, 0.2095, 0.2201, 0.1809, 0.2073]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1569, 0.2108, 0.2108, 0.2108, 0.2108],\n",
      "         [0.2066, 0.2005, 0.1976, 0.1976, 0.1976],\n",
      "         [0.1438, 0.1985, 0.1936, 0.2320, 0.2320],\n",
      "         [0.1959, 0.1996, 0.1993, 0.2037, 0.2014],\n",
      "         [0.2066, 0.1975, 0.1982, 0.1882, 0.2095]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1833, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.1966, 0.2038, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1925, 0.2083, 0.1999, 0.1997, 0.1997],\n",
      "         [0.1967, 0.2103, 0.2031, 0.1871, 0.2029],\n",
      "         [0.2009, 0.1972, 0.1991, 0.2037, 0.1992]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1739, 0.2065, 0.2065, 0.2065, 0.2065],\n",
      "         [0.1432, 0.2336, 0.2078, 0.2078, 0.2078],\n",
      "         [0.2064, 0.1971, 0.1978, 0.1993, 0.1993],\n",
      "         [0.1621, 0.2167, 0.2121, 0.2068, 0.2022],\n",
      "         [0.2708, 0.1682, 0.1743, 0.1816, 0.2051]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1755, 0.2061, 0.2061, 0.2061, 0.2061],\n",
      "         [0.1960, 0.1966, 0.2025, 0.2025, 0.2025],\n",
      "         [0.2022, 0.2027, 0.1793, 0.2079, 0.2079],\n",
      "         [0.2017, 0.2006, 0.2564, 0.1505, 0.1908],\n",
      "         [0.2003, 0.2015, 0.1491, 0.2869, 0.1621]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2083, 0.1685, 0.2077, 0.2077, 0.2077],\n",
      "         [0.2039, 0.1680, 0.2214, 0.2034, 0.2034],\n",
      "         [0.1768, 0.2404, 0.1551, 0.2502, 0.1775],\n",
      "         [0.2032, 0.0992, 0.2756, 0.0903, 0.3317]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2275, 0.1931, 0.1931, 0.1931, 0.1931],\n",
      "         [0.2034, 0.2580, 0.1795, 0.1795, 0.1795],\n",
      "         [0.1995, 0.2840, 0.1850, 0.1658, 0.1658],\n",
      "         [0.2064, 0.2412, 0.1996, 0.1626, 0.1902],\n",
      "         [0.2013, 0.2247, 0.1967, 0.1703, 0.2070]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2185, 0.1954, 0.1954, 0.1954, 0.1954],\n",
      "         [0.1985, 0.2007, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2071, 0.1985, 0.1939, 0.2002, 0.2002],\n",
      "         [0.2174, 0.2026, 0.1949, 0.1796, 0.2056],\n",
      "         [0.2074, 0.2012, 0.1980, 0.1913, 0.2021]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2323, 0.1919, 0.1919, 0.1919, 0.1919],\n",
      "         [0.1514, 0.1316, 0.2390, 0.2390, 0.2390],\n",
      "         [0.2385, 0.2823, 0.2039, 0.1377, 0.1377],\n",
      "         [0.2233, 0.2487, 0.2020, 0.1688, 0.1572],\n",
      "         [0.2078, 0.2171, 0.1995, 0.1855, 0.1901]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2461, 0.1885, 0.1885, 0.1885, 0.1885],\n",
      "         [0.2651, 0.2665, 0.1561, 0.1561, 0.1561],\n",
      "         [0.2012, 0.2020, 0.3196, 0.1386, 0.1386],\n",
      "         [0.1919, 0.1926, 0.3161, 0.1711, 0.1283],\n",
      "         [0.2120, 0.2115, 0.1545, 0.2280, 0.1940]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1935, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.3954, 0.2079, 0.1322, 0.1322, 0.1322],\n",
      "         [0.1807, 0.1946, 0.2145, 0.2051, 0.2051],\n",
      "         [0.1909, 0.1972, 0.2057, 0.2045, 0.2017],\n",
      "         [0.1354, 0.1693, 0.2268, 0.2178, 0.2508]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2586, 0.1854, 0.1854, 0.1854, 0.1854],\n",
      "         [0.2092, 0.1978, 0.1977, 0.1977, 0.1977],\n",
      "         [0.2077, 0.1973, 0.2007, 0.1971, 0.1971],\n",
      "         [0.2720, 0.1733, 0.2014, 0.1812, 0.1721],\n",
      "         [0.2603, 0.1693, 0.1954, 0.1767, 0.1984]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1937, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.2617, 0.2152, 0.1744, 0.1744, 0.1744],\n",
      "         [0.1984, 0.2001, 0.1974, 0.2020, 0.2020],\n",
      "         [0.1715, 0.2088, 0.1536, 0.2081, 0.2580],\n",
      "         [0.2030, 0.1901, 0.2107, 0.1903, 0.2060]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2086, 0.1978, 0.1978, 0.1978, 0.1978],\n",
      "         [0.2502, 0.1695, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2154, 0.1759, 0.2319, 0.1884, 0.1884],\n",
      "         [0.2056, 0.1831, 0.2145, 0.2063, 0.1905],\n",
      "         [0.2047, 0.1846, 0.2125, 0.2053, 0.1930]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1991, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2154, 0.2235, 0.1870, 0.1870, 0.1870],\n",
      "         [0.1736, 0.1601, 0.1924, 0.2370, 0.2370],\n",
      "         [0.1851, 0.1762, 0.1970, 0.2181, 0.2237],\n",
      "         [0.1852, 0.1696, 0.2072, 0.2486, 0.1895]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1667, 0.2083, 0.2083, 0.2083, 0.2083],\n",
      "         [0.1970, 0.2002, 0.2009, 0.2009, 0.2009],\n",
      "         [0.1994, 0.2001, 0.1998, 0.2003, 0.2003],\n",
      "         [0.1865, 0.2031, 0.1950, 0.2082, 0.2072],\n",
      "         [0.2577, 0.2118, 0.2326, 0.1999, 0.0980]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1970, 0.1401, 0.2210, 0.2210, 0.2210],\n",
      "         [0.2034, 0.1933, 0.1893, 0.2070, 0.2070],\n",
      "         [0.1730, 0.2391, 0.2723, 0.1606, 0.1551],\n",
      "         [0.1769, 0.2132, 0.2298, 0.1694, 0.2107]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1711, 0.2072, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1995, 0.1992, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2505, 0.2903, 0.1701, 0.1446, 0.1446],\n",
      "         [0.2098, 0.2300, 0.1651, 0.2459, 0.1493],\n",
      "         [0.2129, 0.2323, 0.1694, 0.2476, 0.1379]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2864, 0.1784, 0.1784, 0.1784, 0.1784],\n",
      "         [0.3104, 0.1267, 0.1876, 0.1876, 0.1876],\n",
      "         [0.2371, 0.1517, 0.2422, 0.1845, 0.1845],\n",
      "         [0.1954, 0.2079, 0.1948, 0.1995, 0.2023],\n",
      "         [0.1919, 0.2506, 0.1895, 0.2098, 0.1581]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2047, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1883, 0.2413, 0.1901, 0.1901, 0.1901],\n",
      "         [0.1963, 0.2121, 0.1979, 0.1969, 0.1969],\n",
      "         [0.1672, 0.2810, 0.1768, 0.2043, 0.1707],\n",
      "         [0.1050, 0.4762, 0.1236, 0.1882, 0.1071]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2697, 0.1826, 0.1826, 0.1826, 0.1826],\n",
      "         [0.2546, 0.3057, 0.1466, 0.1466, 0.1466],\n",
      "         [0.2521, 0.2906, 0.1293, 0.1640, 0.1640],\n",
      "         [0.1940, 0.1902, 0.2125, 0.1975, 0.2057],\n",
      "         [0.2452, 0.2737, 0.1465, 0.2217, 0.1129]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1830, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2029, 0.1983, 0.1996, 0.1996, 0.1996],\n",
      "         [0.1957, 0.2104, 0.1813, 0.2062, 0.2062],\n",
      "         [0.2015, 0.1978, 0.2054, 0.1965, 0.1988],\n",
      "         [0.1825, 0.2584, 0.1264, 0.2931, 0.1397]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1836, 0.2041, 0.2041, 0.2041, 0.2041],\n",
      "         [0.1852, 0.1274, 0.2291, 0.2291, 0.2291],\n",
      "         [0.1633, 0.0603, 0.2006, 0.2879, 0.2879],\n",
      "         [0.1860, 0.0479, 0.2462, 0.1174, 0.4025],\n",
      "         [0.2106, 0.1257, 0.2343, 0.1768, 0.2525]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.4488, 0.1378, 0.1378, 0.1378, 0.1378],\n",
      "         [0.3453, 0.1867, 0.1560, 0.1560, 0.1560],\n",
      "         [0.1211, 0.2143, 0.1581, 0.2532, 0.2532],\n",
      "         [0.1303, 0.1973, 0.1581, 0.2916, 0.2227],\n",
      "         [0.0553, 0.1750, 0.0947, 0.5170, 0.1580]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1832, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.1968, 0.1906, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2003, 0.1995, 0.1978, 0.2012, 0.2012],\n",
      "         [0.1979, 0.2016, 0.2107, 0.1961, 0.1937],\n",
      "         [0.1628, 0.1901, 0.2733, 0.1514, 0.2224]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1918, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.4243, 0.1086, 0.1557, 0.1557, 0.1557],\n",
      "         [0.2056, 0.1971, 0.1987, 0.1993, 0.1993],\n",
      "         [0.1953, 0.2026, 0.2011, 0.2004, 0.2006],\n",
      "         [0.0905, 0.2943, 0.2347, 0.2089, 0.1716]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3178, 0.1705, 0.1705, 0.1705, 0.1705],\n",
      "         [0.2378, 0.2279, 0.1781, 0.1781, 0.1781],\n",
      "         [0.2917, 0.2596, 0.1835, 0.1326, 0.1326],\n",
      "         [0.1847, 0.1888, 0.2014, 0.2111, 0.2140],\n",
      "         [0.2836, 0.2542, 0.1835, 0.1448, 0.1339]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1823, 0.2044, 0.2044, 0.2044, 0.2044],\n",
      "         [0.2107, 0.2416, 0.1826, 0.1826, 0.1826],\n",
      "         [0.1951, 0.1870, 0.2100, 0.2039, 0.2039],\n",
      "         [0.2155, 0.2520, 0.1642, 0.1853, 0.1830],\n",
      "         [0.1801, 0.1633, 0.2137, 0.1981, 0.2448]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2045, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1875, 0.1986, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1392, 0.1820, 0.2598, 0.2095, 0.2095],\n",
      "         [0.1649, 0.1836, 0.2116, 0.2458, 0.1941],\n",
      "         [0.1559, 0.1808, 0.2201, 0.2710, 0.1722]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1882, 0.2030, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1731, 0.2279, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1430, 0.2542, 0.2171, 0.1928, 0.1928],\n",
      "         [0.1693, 0.2141, 0.2007, 0.2246, 0.1913],\n",
      "         [0.1438, 0.2148, 0.1924, 0.2332, 0.2159]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2470, 0.1882, 0.1882, 0.1882, 0.1882],\n",
      "         [0.1982, 0.2194, 0.1941, 0.1941, 0.1941],\n",
      "         [0.1878, 0.3888, 0.0997, 0.1619, 0.1619],\n",
      "         [0.1979, 0.2220, 0.1791, 0.2077, 0.1933],\n",
      "         [0.2027, 0.2513, 0.1681, 0.2219, 0.1559]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2005, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2095, 0.1823, 0.2027, 0.2027, 0.2027],\n",
      "         [0.2363, 0.1736, 0.1509, 0.2196, 0.2196],\n",
      "         [0.1791, 0.2103, 0.2262, 0.1983, 0.1861],\n",
      "         [0.2347, 0.1977, 0.1829, 0.2104, 0.1743]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2052, 0.1987, 0.1987, 0.1987, 0.1987],\n",
      "         [0.2212, 0.2373, 0.1805, 0.1805, 0.1805],\n",
      "         [0.2121, 0.2213, 0.1916, 0.1875, 0.1875],\n",
      "         [0.1897, 0.2045, 0.1586, 0.2946, 0.1526],\n",
      "         [0.2032, 0.1973, 0.2180, 0.1709, 0.2105]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2080, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.2254, 0.1455, 0.2097, 0.2097, 0.2097],\n",
      "         [0.2206, 0.1791, 0.1740, 0.2132, 0.2132],\n",
      "         [0.3054, 0.1583, 0.1444, 0.1179, 0.2741],\n",
      "         [0.1979, 0.2001, 0.2004, 0.2011, 0.2006]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1716, 0.2071, 0.2071, 0.2071, 0.2071],\n",
      "         [0.1727, 0.1877, 0.2132, 0.2132, 0.2132],\n",
      "         [0.1767, 0.1897, 0.2100, 0.2118, 0.2118],\n",
      "         [0.1888, 0.2077, 0.2379, 0.1251, 0.2406],\n",
      "         [0.1997, 0.2109, 0.2279, 0.1580, 0.2035]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3537, 0.1616, 0.1616, 0.1616, 0.1616],\n",
      "         [0.4476, 0.1821, 0.1234, 0.1234, 0.1234],\n",
      "         [0.2813, 0.1881, 0.2144, 0.1581, 0.1581],\n",
      "         [0.1649, 0.2116, 0.1952, 0.1926, 0.2357],\n",
      "         [0.1919, 0.2101, 0.2040, 0.2030, 0.1910]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1778, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.1747, 0.1909, 0.2115, 0.2115, 0.2115],\n",
      "         [0.2117, 0.1971, 0.2281, 0.1816, 0.1816],\n",
      "         [0.1997, 0.2047, 0.1946, 0.1904, 0.2107],\n",
      "         [0.2122, 0.1785, 0.2540, 0.2955, 0.0597]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0453, 0.2387, 0.2387, 0.2387, 0.2387],\n",
      "         [0.0745, 0.1732, 0.2508, 0.2508, 0.2508],\n",
      "         [0.0775, 0.1720, 0.2623, 0.2441, 0.2441],\n",
      "         [0.0966, 0.1942, 0.2812, 0.1640, 0.2640],\n",
      "         [0.1692, 0.1966, 0.2129, 0.1896, 0.2317]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0749, 0.2313, 0.2313, 0.2313, 0.2313],\n",
      "         [0.0614, 0.4333, 0.1684, 0.1684, 0.1684],\n",
      "         [0.1267, 0.3056, 0.1683, 0.1997, 0.1997],\n",
      "         [0.1549, 0.2561, 0.1821, 0.2060, 0.2009],\n",
      "         [0.2101, 0.1887, 0.2030, 0.1977, 0.2005]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2353, 0.1912, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2157, 0.3880, 0.1321, 0.1321, 0.1321],\n",
      "         [0.2050, 0.2225, 0.1894, 0.1916, 0.1916],\n",
      "         [0.1941, 0.2845, 0.1336, 0.2468, 0.1410],\n",
      "         [0.1836, 0.1592, 0.2109, 0.1679, 0.2784]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1143, 0.2214, 0.2214, 0.2214, 0.2214],\n",
      "         [0.1528, 0.2160, 0.2104, 0.2104, 0.2104],\n",
      "         [0.2101, 0.2008, 0.1863, 0.2014, 0.2014],\n",
      "         [0.1822, 0.1951, 0.2184, 0.2103, 0.1941],\n",
      "         [0.3433, 0.2416, 0.1353, 0.1645, 0.1153]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1660, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.1282, 0.1287, 0.2477, 0.2477, 0.2477],\n",
      "         [0.1665, 0.1667, 0.2344, 0.2162, 0.2162],\n",
      "         [0.1498, 0.1501, 0.2315, 0.2599, 0.2089],\n",
      "         [0.2586, 0.2582, 0.1755, 0.1584, 0.1493]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1828, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1428, 0.1694, 0.2293, 0.2293, 0.2293],\n",
      "         [0.2331, 0.2090, 0.2135, 0.1722, 0.1722],\n",
      "         [0.2228, 0.2073, 0.2103, 0.1771, 0.1824],\n",
      "         [0.2689, 0.2213, 0.2299, 0.1446, 0.1354]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2131, 0.1967, 0.1967, 0.1967, 0.1967],\n",
      "         [0.1715, 0.2227, 0.2019, 0.2019, 0.2019],\n",
      "         [0.3191, 0.1279, 0.1926, 0.1802, 0.1802],\n",
      "         [0.1795, 0.2123, 0.1969, 0.2119, 0.1994],\n",
      "         [0.2963, 0.1476, 0.2016, 0.1489, 0.2056]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1638, 0.2090, 0.2090, 0.2090, 0.2090],\n",
      "         [0.2011, 0.2007, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1920, 0.1988, 0.1602, 0.2245, 0.2245],\n",
      "         [0.2048, 0.2175, 0.1496, 0.1595, 0.2686],\n",
      "         [0.2282, 0.2415, 0.1698, 0.1804, 0.1801]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1701, 0.2075, 0.2075, 0.2075, 0.2075],\n",
      "         [0.1576, 0.1370, 0.2351, 0.2351, 0.2351],\n",
      "         [0.1935, 0.1873, 0.1938, 0.2127, 0.2127],\n",
      "         [0.1997, 0.1984, 0.1998, 0.1984, 0.2036],\n",
      "         [0.1887, 0.2173, 0.1875, 0.2173, 0.1892]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1450, 0.2137, 0.2137, 0.2137, 0.2137],\n",
      "         [0.1466, 0.2701, 0.1944, 0.1944, 0.1944],\n",
      "         [0.0781, 0.2688, 0.3766, 0.1382, 0.1382],\n",
      "         [0.0736, 0.2603, 0.3673, 0.1669, 0.1319],\n",
      "         [0.1438, 0.2185, 0.2450, 0.1886, 0.2042]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1582, 0.2105, 0.2105, 0.2105, 0.2105],\n",
      "         [0.1902, 0.2118, 0.1993, 0.1993, 0.1993],\n",
      "         [0.2204, 0.1814, 0.1933, 0.2024, 0.2024],\n",
      "         [0.2146, 0.1903, 0.1979, 0.1936, 0.2036],\n",
      "         [0.2211, 0.1470, 0.1678, 0.1558, 0.3083]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1998, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1964, 0.2104, 0.1977, 0.1977, 0.1977],\n",
      "         [0.1725, 0.1943, 0.2844, 0.1744, 0.1744],\n",
      "         [0.2143, 0.2078, 0.1883, 0.1759, 0.2137],\n",
      "         [0.1823, 0.1872, 0.2036, 0.2159, 0.2110]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1652, 0.2087, 0.2087, 0.2087, 0.2087],\n",
      "         [0.1919, 0.1942, 0.2046, 0.2046, 0.2046],\n",
      "         [0.2021, 0.1989, 0.2283, 0.1853, 0.1853],\n",
      "         [0.1912, 0.1845, 0.2508, 0.2161, 0.1575],\n",
      "         [0.2104, 0.2141, 0.1839, 0.1980, 0.1936]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1932, 0.2025, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2305, 0.1853, 0.2038, 0.1901, 0.1901],\n",
      "         [0.4311, 0.1028, 0.1919, 0.1526, 0.1216],\n",
      "         [0.2600, 0.1554, 0.1945, 0.1791, 0.2110]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0763, 0.2309, 0.2309, 0.2309, 0.2309],\n",
      "         [0.1685, 0.1560, 0.2252, 0.2252, 0.2252],\n",
      "         [0.1783, 0.1674, 0.2012, 0.2266, 0.2266],\n",
      "         [0.2382, 0.2932, 0.1607, 0.1992, 0.1089],\n",
      "         [0.1989, 0.1979, 0.2007, 0.1997, 0.2028]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2235, 0.1941, 0.1941, 0.1941, 0.1941],\n",
      "         [0.2310, 0.2205, 0.1828, 0.1828, 0.1828],\n",
      "         [0.1976, 0.1887, 0.3005, 0.1566, 0.1566],\n",
      "         [0.1932, 0.1879, 0.2479, 0.2028, 0.1682],\n",
      "         [0.1950, 0.1940, 0.2036, 0.1966, 0.2108]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1842, 0.2040, 0.2040, 0.2040, 0.2040],\n",
      "         [0.1380, 0.1388, 0.2411, 0.2411, 0.2411],\n",
      "         [0.1729, 0.1732, 0.2502, 0.2018, 0.2018],\n",
      "         [0.1347, 0.1350, 0.2515, 0.3038, 0.1749],\n",
      "         [0.1713, 0.1715, 0.2306, 0.2523, 0.1743]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1154, 0.2211, 0.2211, 0.2211, 0.2211],\n",
      "         [0.1215, 0.1493, 0.2431, 0.2431, 0.2431],\n",
      "         [0.0687, 0.1122, 0.1033, 0.3579, 0.3579],\n",
      "         [0.1078, 0.1527, 0.1440, 0.2479, 0.3476],\n",
      "         [0.1142, 0.1751, 0.1630, 0.3175, 0.2302]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1927, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2093, 0.1789, 0.2039, 0.2039, 0.2039],\n",
      "         [0.2078, 0.1857, 0.1985, 0.2040, 0.2040],\n",
      "         [0.1947, 0.2082, 0.2001, 0.2000, 0.1969],\n",
      "         [0.2095, 0.1721, 0.1934, 0.1939, 0.2311]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1953, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2185, 0.1842, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1566, 0.2208, 0.2450, 0.1888, 0.1888],\n",
      "         [0.1913, 0.2029, 0.2066, 0.2016, 0.1975],\n",
      "         [0.1590, 0.1945, 0.2068, 0.1903, 0.2494]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1967, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1810, 0.1928, 0.2087, 0.2087, 0.2087],\n",
      "         [0.2034, 0.1993, 0.2086, 0.1943, 0.1943],\n",
      "         [0.1998, 0.1996, 0.2000, 0.2012, 0.1995],\n",
      "         [0.1867, 0.1797, 0.1958, 0.2695, 0.1683]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2173, 0.1957, 0.1957, 0.1957, 0.1957],\n",
      "         [0.2023, 0.2294, 0.1894, 0.1894, 0.1894],\n",
      "         [0.1975, 0.2269, 0.2085, 0.1835, 0.1835],\n",
      "         [0.2000, 0.2003, 0.2001, 0.1996, 0.1999],\n",
      "         [0.1909, 0.2320, 0.2061, 0.1315, 0.2396]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2018, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1903, 0.1906, 0.2064, 0.2064, 0.2064],\n",
      "         [0.2077, 0.2075, 0.1920, 0.1964, 0.1964],\n",
      "         [0.1956, 0.1963, 0.2397, 0.1423, 0.2261],\n",
      "         [0.2014, 0.2013, 0.1940, 0.2135, 0.1898]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1872, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1974, 0.1795, 0.2077, 0.2077, 0.2077],\n",
      "         [0.2001, 0.1998, 0.1998, 0.2002, 0.2002],\n",
      "         [0.1986, 0.2052, 0.2069, 0.1942, 0.1951],\n",
      "         [0.1695, 0.2018, 0.2106, 0.1505, 0.2675]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2658, 0.1836, 0.1836, 0.1836, 0.1836],\n",
      "         [0.1838, 0.2157, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2157, 0.1882, 0.1950, 0.2006, 0.2006],\n",
      "         [0.2249, 0.1762, 0.1877, 0.2137, 0.1975],\n",
      "         [0.2313, 0.1834, 0.1947, 0.2203, 0.1704]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1952, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1942, 0.2016, 0.2114, 0.1964, 0.1964],\n",
      "         [0.1967, 0.1999, 0.2040, 0.2018, 0.1977],\n",
      "         [0.1658, 0.1826, 0.2066, 0.1937, 0.2513]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2007, 0.1895, 0.2033, 0.2033, 0.2033],\n",
      "         [0.2014, 0.1946, 0.1982, 0.2029, 0.2029],\n",
      "         [0.2008, 0.1963, 0.1987, 0.2024, 0.2018],\n",
      "         [0.2013, 0.2109, 0.2057, 0.1979, 0.1842]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2043, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1790, 0.2126, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1681, 0.2001, 0.2502, 0.1908, 0.1908],\n",
      "         [0.2269, 0.2053, 0.1806, 0.1761, 0.2110],\n",
      "         [0.1752, 0.1947, 0.2230, 0.2290, 0.1780]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1932, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1996, 0.1807, 0.2065, 0.2065, 0.2065],\n",
      "         [0.2020, 0.1639, 0.2002, 0.2170, 0.2170],\n",
      "         [0.2192, 0.1664, 0.2166, 0.1569, 0.2408],\n",
      "         [0.1999, 0.2004, 0.1999, 0.2005, 0.1993]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2458, 0.1886, 0.1886, 0.1886, 0.1886],\n",
      "         [0.2441, 0.2894, 0.1555, 0.1555, 0.1555],\n",
      "         [0.1774, 0.2019, 0.3688, 0.1259, 0.1259],\n",
      "         [0.1927, 0.1993, 0.2333, 0.1985, 0.1762],\n",
      "         [0.1749, 0.1905, 0.2833, 0.1884, 0.1629]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0702, 0.2324, 0.2324, 0.2324, 0.2324],\n",
      "         [0.1399, 0.2023, 0.2193, 0.2193, 0.2193],\n",
      "         [0.2321, 0.1956, 0.1954, 0.1884, 0.1884],\n",
      "         [0.1400, 0.2177, 0.2182, 0.1843, 0.2397],\n",
      "         [0.2535, 0.1814, 0.1811, 0.2058, 0.1781]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2415, 0.1896, 0.1896, 0.1896, 0.1896],\n",
      "         [0.3846, 0.1787, 0.1455, 0.1455, 0.1455],\n",
      "         [0.3024, 0.1639, 0.2556, 0.1391, 0.1391],\n",
      "         [0.1936, 0.2033, 0.1962, 0.2008, 0.2060],\n",
      "         [0.2487, 0.1484, 0.2158, 0.1692, 0.2179]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2120, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.2034, 0.1977, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2019, 0.2002, 0.1963, 0.2008, 0.2008],\n",
      "         [0.2339, 0.1843, 0.1056, 0.2762, 0.2000],\n",
      "         [0.2448, 0.2030, 0.1310, 0.2791, 0.1421]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1565, 0.2109, 0.2109, 0.2109, 0.2109],\n",
      "         [0.1633, 0.2148, 0.2073, 0.2073, 0.2073],\n",
      "         [0.1806, 0.2083, 0.2021, 0.2045, 0.2045],\n",
      "         [0.1651, 0.2175, 0.2051, 0.2024, 0.2099],\n",
      "         [0.2215, 0.1823, 0.1900, 0.1918, 0.2145]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1078, 0.2231, 0.2231, 0.2231, 0.2231],\n",
      "         [0.1483, 0.2430, 0.2029, 0.2029, 0.2029],\n",
      "         [0.1538, 0.2225, 0.2348, 0.1944, 0.1944],\n",
      "         [0.1432, 0.2243, 0.2394, 0.2027, 0.1904],\n",
      "         [0.1264, 0.2342, 0.2561, 0.2037, 0.1796]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2324, 0.1919, 0.1919, 0.1919, 0.1919],\n",
      "         [0.1955, 0.1998, 0.2016, 0.2016, 0.2016],\n",
      "         [0.2144, 0.2051, 0.1778, 0.2014, 0.2014],\n",
      "         [0.2072, 0.2034, 0.1916, 0.1959, 0.2019],\n",
      "         [0.1831, 0.1892, 0.2102, 0.2022, 0.2152]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1708, 0.2073, 0.2073, 0.2073, 0.2073],\n",
      "         [0.2024, 0.1974, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2101, 0.1916, 0.1961, 0.2011, 0.2011],\n",
      "         [0.1144, 0.2748, 0.2211, 0.2163, 0.1735],\n",
      "         [0.2381, 0.1790, 0.1922, 0.1936, 0.1971]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1912, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.2539, 0.1330, 0.2044, 0.2044, 0.2044],\n",
      "         [0.1850, 0.2141, 0.2123, 0.1943, 0.1943],\n",
      "         [0.1875, 0.2183, 0.2163, 0.1805, 0.1973],\n",
      "         [0.1600, 0.2456, 0.2394, 0.1440, 0.2111]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2450, 0.1888, 0.1888, 0.1888, 0.1888],\n",
      "         [0.3061, 0.2549, 0.1463, 0.1463, 0.1463],\n",
      "         [0.3390, 0.2606, 0.1658, 0.1173, 0.1173],\n",
      "         [0.2648, 0.2333, 0.1877, 0.1553, 0.1589],\n",
      "         [0.2863, 0.2261, 0.1507, 0.1057, 0.2311]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2113, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2018, 0.2026, 0.1985, 0.1985, 0.1985],\n",
      "         [0.2162, 0.2237, 0.1898, 0.1851, 0.1851],\n",
      "         [0.2112, 0.2157, 0.1951, 0.1859, 0.1921],\n",
      "         [0.2001, 0.2002, 0.1997, 0.1995, 0.2005]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2517, 0.1871, 0.1871, 0.1871, 0.1871],\n",
      "         [0.2405, 0.2515, 0.1693, 0.1693, 0.1693],\n",
      "         [0.2270, 0.2363, 0.2056, 0.1655, 0.1655],\n",
      "         [0.2290, 0.2464, 0.1910, 0.2051, 0.1286],\n",
      "         [0.2403, 0.2650, 0.1887, 0.2075, 0.0984]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1844, 0.2039, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1384, 0.1223, 0.2464, 0.2464, 0.2464],\n",
      "         [0.1631, 0.1486, 0.1847, 0.2518, 0.2518],\n",
      "         [0.1893, 0.1780, 0.2056, 0.1747, 0.2524],\n",
      "         [0.1974, 0.1765, 0.2292, 0.1706, 0.2263]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1362, 0.2160, 0.2160, 0.2160, 0.2160],\n",
      "         [0.1649, 0.0962, 0.2463, 0.2463, 0.2463],\n",
      "         [0.1725, 0.1055, 0.2245, 0.2487, 0.2487],\n",
      "         [0.1731, 0.0854, 0.2526, 0.1962, 0.2927],\n",
      "         [0.1997, 0.1934, 0.2032, 0.2009, 0.2028]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2255, 0.1936, 0.1936, 0.1936, 0.1936],\n",
      "         [0.1918, 0.1983, 0.2033, 0.2033, 0.2033],\n",
      "         [0.2065, 0.2000, 0.2030, 0.1953, 0.1953],\n",
      "         [0.1871, 0.2040, 0.1960, 0.1949, 0.2179],\n",
      "         [0.1886, 0.2094, 0.1995, 0.1982, 0.2043]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0872, 0.2282, 0.2282, 0.2282, 0.2282],\n",
      "         [0.1344, 0.1562, 0.2365, 0.2365, 0.2365],\n",
      "         [0.1735, 0.1844, 0.2060, 0.2180, 0.2180],\n",
      "         [0.1960, 0.1985, 0.2032, 0.1966, 0.2056],\n",
      "         [0.1291, 0.1576, 0.2270, 0.1351, 0.3512]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1939, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.2200, 0.1383, 0.2139, 0.2139, 0.2139],\n",
      "         [0.1633, 0.2839, 0.2152, 0.1688, 0.1688],\n",
      "         [0.2001, 0.1999, 0.2000, 0.1999, 0.2001],\n",
      "         [0.1592, 0.2368, 0.1941, 0.2194, 0.1905]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1491, 0.2127, 0.2127, 0.2127, 0.2127],\n",
      "         [0.1343, 0.2918, 0.1913, 0.1913, 0.1913],\n",
      "         [0.1081, 0.2512, 0.3232, 0.1588, 0.1588],\n",
      "         [0.0628, 0.2365, 0.3514, 0.2344, 0.1149],\n",
      "         [0.1267, 0.2071, 0.2398, 0.2064, 0.2201]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2065, 0.1984, 0.1984, 0.1984, 0.1984],\n",
      "         [0.1894, 0.1971, 0.2045, 0.2045, 0.2045],\n",
      "         [0.2119, 0.2035, 0.1921, 0.1962, 0.1962],\n",
      "         [0.1990, 0.1998, 0.2010, 0.1997, 0.2006],\n",
      "         [0.2146, 0.2030, 0.1874, 0.2040, 0.1911]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2535, 0.1866, 0.1866, 0.1866, 0.1866],\n",
      "         [0.2064, 0.2038, 0.1966, 0.1966, 0.1966],\n",
      "         [0.1929, 0.1970, 0.1917, 0.2092, 0.2092],\n",
      "         [0.2360, 0.2108, 0.2443, 0.1565, 0.1524],\n",
      "         [0.1678, 0.1965, 0.1599, 0.2974, 0.1783]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1106, 0.2224, 0.2224, 0.2224, 0.2224],\n",
      "         [0.1259, 0.0820, 0.2640, 0.2640, 0.2640],\n",
      "         [0.1832, 0.1453, 0.1248, 0.2734, 0.2734],\n",
      "         [0.2014, 0.1472, 0.1196, 0.1851, 0.3467],\n",
      "         [0.2518, 0.1778, 0.1414, 0.2293, 0.1996]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2619, 0.1845, 0.1845, 0.1845, 0.1845],\n",
      "         [0.2595, 0.1775, 0.1877, 0.1877, 0.1877],\n",
      "         [0.2940, 0.1659, 0.1795, 0.1804, 0.1804],\n",
      "         [0.2707, 0.1877, 0.1974, 0.1462, 0.1980],\n",
      "         [0.2233, 0.1960, 0.1996, 0.1793, 0.2017]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1546, 0.2114, 0.2114, 0.2114, 0.2114],\n",
      "         [0.1992, 0.2001, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1672, 0.2052, 0.2082, 0.2097, 0.2097],\n",
      "         [0.1957, 0.2003, 0.2006, 0.2026, 0.2008],\n",
      "         [0.0599, 0.2061, 0.2245, 0.3731, 0.1364]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1398, 0.2150, 0.2150, 0.2150, 0.2150],\n",
      "         [0.1416, 0.1677, 0.2302, 0.2302, 0.2302],\n",
      "         [0.3991, 0.2599, 0.1083, 0.1164, 0.1164],\n",
      "         [0.2788, 0.2318, 0.1591, 0.1661, 0.1642],\n",
      "         [0.2072, 0.2034, 0.1959, 0.1967, 0.1968]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2134, 0.1787, 0.2026, 0.2026, 0.2026],\n",
      "         [0.1961, 0.1900, 0.2253, 0.1943, 0.1943],\n",
      "         [0.1427, 0.1157, 0.3596, 0.2477, 0.1342],\n",
      "         [0.1774, 0.1684, 0.2236, 0.2037, 0.2269]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1932, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2021, 0.1991, 0.1996, 0.1996, 0.1996],\n",
      "         [0.1859, 0.2068, 0.2013, 0.2030, 0.2030],\n",
      "         [0.2093, 0.1967, 0.1998, 0.1953, 0.1989],\n",
      "         [0.1365, 0.2138, 0.1910, 0.2251, 0.2336]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1369, 0.2158, 0.2158, 0.2158, 0.2158],\n",
      "         [0.2070, 0.2010, 0.1973, 0.1973, 0.1973],\n",
      "         [0.1564, 0.2142, 0.1077, 0.2608, 0.2608],\n",
      "         [0.1823, 0.2134, 0.1511, 0.2176, 0.2356],\n",
      "         [0.1849, 0.2483, 0.1303, 0.2573, 0.1791]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2766, 0.1809, 0.1809, 0.1809, 0.1809],\n",
      "         [0.2124, 0.2051, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2272, 0.2066, 0.2112, 0.1775, 0.1775],\n",
      "         [0.1966, 0.2014, 0.2003, 0.1924, 0.2093],\n",
      "         [0.2152, 0.1918, 0.1971, 0.2387, 0.1573]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 12])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1_embeddings = embedding_decoder(user1_pins,user1_actions)\n",
    "user1_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8])\n",
      "tensor([1000,  688,  125,  739,  381,  515,  828,  598])\n",
      "torch.Size([8, 5, 12])\n",
      "torch.Size([8, 5, 12])\n",
      "Softmax for each column across one row tensor([[[0.1848, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.2570, 0.1473, 0.1986, 0.1986, 0.1986],\n",
      "         [0.1860, 0.2087, 0.2129, 0.1962, 0.1962],\n",
      "         [0.2075, 0.1960, 0.1940, 0.2004, 0.2021],\n",
      "         [0.2085, 0.1965, 0.1945, 0.2011, 0.1993]],\n",
      "\n",
      "        [[0.1694, 0.2076, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1420, 0.2926, 0.1885, 0.1885, 0.1885],\n",
      "         [0.2541, 0.0973, 0.2997, 0.1745, 0.1745],\n",
      "         [0.2331, 0.1125, 0.2642, 0.2149, 0.1753],\n",
      "         [0.1745, 0.2875, 0.1601, 0.1844, 0.1935]],\n",
      "\n",
      "        [[0.2049, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1978, 0.2022, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2143, 0.1655, 0.2437, 0.1882, 0.1882],\n",
      "         [0.2330, 0.0952, 0.3633, 0.1598, 0.1487],\n",
      "         [0.1698, 0.3160, 0.1248, 0.2206, 0.1688]],\n",
      "\n",
      "        [[0.1471, 0.2132, 0.2132, 0.2132, 0.2132],\n",
      "         [0.2909, 0.1471, 0.1873, 0.1873, 0.1873],\n",
      "         [0.1486, 0.2376, 0.2115, 0.2012, 0.2012],\n",
      "         [0.2506, 0.1651, 0.1830, 0.2098, 0.1914],\n",
      "         [0.2090, 0.1901, 0.1946, 0.2008, 0.2055]],\n",
      "\n",
      "        [[0.1955, 0.2011, 0.2011, 0.2011, 0.2011],\n",
      "         [0.1739, 0.2434, 0.1942, 0.1942, 0.1942],\n",
      "         [0.1991, 0.1842, 0.2286, 0.1941, 0.1941],\n",
      "         [0.1853, 0.1710, 0.2136, 0.2497, 0.1805],\n",
      "         [0.1968, 0.1938, 0.2023, 0.2085, 0.1986]],\n",
      "\n",
      "        [[0.2082, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.2321, 0.2021, 0.1886, 0.1886, 0.1886],\n",
      "         [0.1500, 0.1818, 0.2679, 0.2002, 0.2002],\n",
      "         [0.1499, 0.1759, 0.2428, 0.2409, 0.1905],\n",
      "         [0.2668, 0.2309, 0.1725, 0.1737, 0.1561]],\n",
      "\n",
      "        [[0.1819, 0.2045, 0.2045, 0.2045, 0.2045],\n",
      "         [0.1617, 0.1938, 0.2149, 0.2149, 0.2149],\n",
      "         [0.1993, 0.2007, 0.1971, 0.2015, 0.2015],\n",
      "         [0.1980, 0.2026, 0.1912, 0.2030, 0.2052],\n",
      "         [0.2050, 0.1740, 0.2642, 0.1711, 0.1856]],\n",
      "\n",
      "        [[0.2170, 0.1957, 0.1957, 0.1957, 0.1957],\n",
      "         [0.1863, 0.2007, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1355, 0.1824, 0.2899, 0.1961, 0.1961],\n",
      "         [0.2596, 0.2089, 0.1490, 0.1842, 0.1982],\n",
      "         [0.2196, 0.2064, 0.1874, 0.1991, 0.1876]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1848, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.2067, 0.1326, 0.2203, 0.2203, 0.2203],\n",
      "         [0.2177, 0.1640, 0.1648, 0.2267, 0.2267],\n",
      "         [0.1488, 0.2978, 0.2943, 0.1244, 0.1347],\n",
      "         [0.1928, 0.2133, 0.2129, 0.1878, 0.1932]],\n",
      "\n",
      "        [[0.2179, 0.1955, 0.1955, 0.1955, 0.1955],\n",
      "         [0.2998, 0.0481, 0.2173, 0.2173, 0.2173],\n",
      "         [0.1092, 0.5159, 0.0880, 0.1435, 0.1435],\n",
      "         [0.1003, 0.5365, 0.0795, 0.1491, 0.1347],\n",
      "         [0.1827, 0.2324, 0.1766, 0.1934, 0.2149]],\n",
      "\n",
      "        [[0.1496, 0.2126, 0.2126, 0.2126, 0.2126],\n",
      "         [0.2079, 0.2066, 0.1952, 0.1952, 0.1952],\n",
      "         [0.2602, 0.2464, 0.1914, 0.1510, 0.1510],\n",
      "         [0.2542, 0.2440, 0.2016, 0.1316, 0.1686],\n",
      "         [0.2106, 0.2081, 0.1970, 0.1741, 0.2102]],\n",
      "\n",
      "        [[0.1310, 0.2172, 0.2172, 0.2172, 0.2172],\n",
      "         [0.2230, 0.1879, 0.1964, 0.1964, 0.1964],\n",
      "         [0.2830, 0.1730, 0.1512, 0.1964, 0.1964],\n",
      "         [0.1412, 0.2133, 0.2387, 0.2151, 0.1917],\n",
      "         [0.3136, 0.2001, 0.1770, 0.1982, 0.1111]],\n",
      "\n",
      "        [[0.2458, 0.1885, 0.1885, 0.1885, 0.1885],\n",
      "         [0.1972, 0.1785, 0.2081, 0.2081, 0.2081],\n",
      "         [0.1799, 0.6085, 0.0254, 0.0931, 0.0931],\n",
      "         [0.2091, 0.5754, 0.0412, 0.0532, 0.1210],\n",
      "         [0.2036, 0.3261, 0.0955, 0.1076, 0.2672]],\n",
      "\n",
      "        [[0.2040, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2063, 0.1990, 0.1982, 0.1982, 0.1982],\n",
      "         [0.1456, 0.2251, 0.1573, 0.2360, 0.2360],\n",
      "         [0.1649, 0.2039, 0.1712, 0.2515, 0.2086],\n",
      "         [0.2069, 0.1996, 0.2056, 0.1926, 0.1952]],\n",
      "\n",
      "        [[0.1159, 0.2210, 0.2210, 0.2210, 0.2210],\n",
      "         [0.1396, 0.1801, 0.2267, 0.2267, 0.2267],\n",
      "         [0.1044, 0.1489, 0.3365, 0.2051, 0.2051],\n",
      "         [0.2102, 0.2039, 0.1901, 0.1974, 0.1984],\n",
      "         [0.1772, 0.1919, 0.2305, 0.2090, 0.1914]],\n",
      "\n",
      "        [[0.1482, 0.2130, 0.2130, 0.2130, 0.2130],\n",
      "         [0.2301, 0.2550, 0.1716, 0.1716, 0.1716],\n",
      "         [0.2983, 0.3999, 0.0422, 0.1298, 0.1298],\n",
      "         [0.2832, 0.3338, 0.0945, 0.1108, 0.1775],\n",
      "         [0.3188, 0.4156, 0.0542, 0.0701, 0.1412]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2231, 0.1942, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2179, 0.2057, 0.1921, 0.1921, 0.1921],\n",
      "         [0.1944, 0.1983, 0.2011, 0.2031, 0.2031],\n",
      "         [0.2307, 0.2193, 0.2116, 0.1319, 0.2065],\n",
      "         [0.1733, 0.1836, 0.1912, 0.3283, 0.1236]],\n",
      "\n",
      "        [[0.1975, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1966, 0.1507, 0.2175, 0.2175, 0.2175],\n",
      "         [0.2062, 0.2374, 0.1656, 0.1954, 0.1954],\n",
      "         [0.2065, 0.1856, 0.2436, 0.1492, 0.2150],\n",
      "         [0.1969, 0.2010, 0.1906, 0.2098, 0.2016]],\n",
      "\n",
      "        [[0.2192, 0.1952, 0.1952, 0.1952, 0.1952],\n",
      "         [0.1816, 0.1534, 0.2217, 0.2217, 0.2217],\n",
      "         [0.2024, 0.2231, 0.2138, 0.1804, 0.1804],\n",
      "         [0.2000, 0.2006, 0.2003, 0.1998, 0.1993],\n",
      "         [0.1822, 0.2251, 0.2052, 0.1729, 0.2145]],\n",
      "\n",
      "        [[0.2434, 0.1891, 0.1891, 0.1891, 0.1891],\n",
      "         [0.1610, 0.1667, 0.2241, 0.2241, 0.2241],\n",
      "         [0.1800, 0.1842, 0.1891, 0.2233, 0.2233],\n",
      "         [0.2148, 0.2107, 0.2061, 0.1892, 0.1793],\n",
      "         [0.2130, 0.2102, 0.2070, 0.1952, 0.1746]],\n",
      "\n",
      "        [[0.1422, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1618, 0.1529, 0.2284, 0.2284, 0.2284],\n",
      "         [0.1968, 0.1954, 0.1970, 0.2055, 0.2055],\n",
      "         [0.2055, 0.2171, 0.2038, 0.2268, 0.1468],\n",
      "         [0.1972, 0.1959, 0.1974, 0.1949, 0.2146]],\n",
      "\n",
      "        [[0.1945, 0.2014, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2086, 0.2244, 0.1890, 0.1890, 0.1890],\n",
      "         [0.2008, 0.2021, 0.1990, 0.1991, 0.1991],\n",
      "         [0.2321, 0.2554, 0.2028, 0.1059, 0.2039],\n",
      "         [0.1607, 0.1459, 0.1841, 0.3544, 0.1548]],\n",
      "\n",
      "        [[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2052, 0.1816, 0.2044, 0.2044, 0.2044],\n",
      "         [0.2007, 0.1988, 0.1992, 0.2006, 0.2006],\n",
      "         [0.1682, 0.2573, 0.2335, 0.1703, 0.1707],\n",
      "         [0.1819, 0.2309, 0.2186, 0.1832, 0.1854]],\n",
      "\n",
      "        [[0.1613, 0.2097, 0.2097, 0.2097, 0.2097],\n",
      "         [0.1779, 0.1243, 0.2326, 0.2326, 0.2326],\n",
      "         [0.2300, 0.3143, 0.0916, 0.1821, 0.1821],\n",
      "         [0.2250, 0.2904, 0.1061, 0.1927, 0.1859],\n",
      "         [0.2272, 0.3026, 0.0977, 0.1909, 0.1817]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1421, 0.2145, 0.2145, 0.2145, 0.2145],\n",
      "         [0.2492, 0.1341, 0.2056, 0.2056, 0.2056],\n",
      "         [0.2898, 0.1152, 0.1596, 0.2177, 0.2177],\n",
      "         [0.1414, 0.2678, 0.2137, 0.2048, 0.1724],\n",
      "         [0.2170, 0.1783, 0.1911, 0.1936, 0.2200]],\n",
      "\n",
      "        [[0.2359, 0.1910, 0.1910, 0.1910, 0.1910],\n",
      "         [0.2006, 0.2127, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2011, 0.2060, 0.1951, 0.1989, 0.1989],\n",
      "         [0.1997, 0.2092, 0.1886, 0.2068, 0.1957],\n",
      "         [0.1941, 0.1458, 0.2773, 0.1566, 0.2263]],\n",
      "\n",
      "        [[0.2526, 0.1868, 0.1868, 0.1868, 0.1868],\n",
      "         [0.2367, 0.1896, 0.1913, 0.1913, 0.1913],\n",
      "         [0.1281, 0.2576, 0.1131, 0.2506, 0.2506],\n",
      "         [0.1652, 0.2383, 0.1547, 0.2069, 0.2349],\n",
      "         [0.1736, 0.2343, 0.1645, 0.2087, 0.2188]],\n",
      "\n",
      "        [[0.2046, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1900, 0.2081, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1811, 0.2211, 0.1897, 0.2040, 0.2040],\n",
      "         [0.2113, 0.1955, 0.2076, 0.1839, 0.2017],\n",
      "         [0.2730, 0.2199, 0.2597, 0.1855, 0.0619]],\n",
      "\n",
      "        [[0.2119, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.2050, 0.2125, 0.1942, 0.1942, 0.1942],\n",
      "         [0.1984, 0.1964, 0.2025, 0.2014, 0.2014],\n",
      "         [0.1785, 0.1517, 0.2491, 0.1926, 0.2281],\n",
      "         [0.1755, 0.1260, 0.3458, 0.2048, 0.1478]],\n",
      "\n",
      "        [[0.0860, 0.2285, 0.2285, 0.2285, 0.2285],\n",
      "         [0.1031, 0.2661, 0.2103, 0.2103, 0.2103],\n",
      "         [0.1830, 0.2105, 0.1999, 0.2033, 0.2033],\n",
      "         [0.3063, 0.1662, 0.2082, 0.1258, 0.1935],\n",
      "         [0.2291, 0.1878, 0.2021, 0.1716, 0.2095]],\n",
      "\n",
      "        [[0.2206, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2145, 0.2515, 0.1780, 0.1780, 0.1780],\n",
      "         [0.1956, 0.1441, 0.1009, 0.2797, 0.2797],\n",
      "         [0.1948, 0.2084, 0.2255, 0.1913, 0.1800],\n",
      "         [0.2049, 0.1783, 0.1516, 0.2126, 0.2525]],\n",
      "\n",
      "        [[0.2302, 0.1925, 0.1925, 0.1925, 0.1925],\n",
      "         [0.1996, 0.1344, 0.2220, 0.2220, 0.2220],\n",
      "         [0.1961, 0.1722, 0.2254, 0.2031, 0.2031],\n",
      "         [0.2034, 0.2584, 0.1575, 0.1900, 0.1907],\n",
      "         [0.1582, 0.0750, 0.3518, 0.1957, 0.2194]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1535, 0.2116, 0.2116, 0.2116, 0.2116],\n",
      "         [0.1796, 0.1610, 0.2198, 0.2198, 0.2198],\n",
      "         [0.1830, 0.1689, 0.2236, 0.2123, 0.2123],\n",
      "         [0.1966, 0.1874, 0.2217, 0.1794, 0.2149],\n",
      "         [0.1996, 0.1991, 0.2009, 0.1986, 0.2018]],\n",
      "\n",
      "        [[0.1370, 0.2157, 0.2157, 0.2157, 0.2157],\n",
      "         [0.1770, 0.0995, 0.2412, 0.2412, 0.2412],\n",
      "         [0.1708, 0.1098, 0.2862, 0.2166, 0.2166],\n",
      "         [0.1936, 0.1747, 0.2182, 0.2090, 0.2045],\n",
      "         [0.2054, 0.2535, 0.1606, 0.1754, 0.2051]],\n",
      "\n",
      "        [[0.1074, 0.2232, 0.2232, 0.2232, 0.2232],\n",
      "         [0.1799, 0.2017, 0.2061, 0.2061, 0.2061],\n",
      "         [0.2677, 0.2009, 0.1509, 0.1902, 0.1902],\n",
      "         [0.3171, 0.2036, 0.1309, 0.1613, 0.1871],\n",
      "         [0.2025, 0.2000, 0.1975, 0.1987, 0.2014]],\n",
      "\n",
      "        [[0.1993, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1935, 0.1442, 0.2208, 0.2208, 0.2208],\n",
      "         [0.2230, 0.1153, 0.0626, 0.2995, 0.2995],\n",
      "         [0.2223, 0.1681, 0.1298, 0.2278, 0.2519],\n",
      "         [0.1915, 0.2091, 0.2269, 0.1900, 0.1825]],\n",
      "\n",
      "        [[0.1876, 0.2031, 0.2031, 0.2031, 0.2031],\n",
      "         [0.1610, 0.1366, 0.2341, 0.2341, 0.2341],\n",
      "         [0.2075, 0.2130, 0.1887, 0.1954, 0.1954],\n",
      "         [0.2051, 0.2089, 0.1920, 0.1972, 0.1967],\n",
      "         [0.2746, 0.3447, 0.1211, 0.1678, 0.0917]],\n",
      "\n",
      "        [[0.1645, 0.2089, 0.2089, 0.2089, 0.2089],\n",
      "         [0.1781, 0.2088, 0.2043, 0.2043, 0.2043],\n",
      "         [0.2412, 0.2062, 0.1311, 0.2107, 0.2107],\n",
      "         [0.1906, 0.1993, 0.2266, 0.1853, 0.1981],\n",
      "         [0.2054, 0.1996, 0.1838, 0.2092, 0.2020]],\n",
      "\n",
      "        [[0.1945, 0.2014, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2004, 0.2004, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1887, 0.1897, 0.1917, 0.2149, 0.2149],\n",
      "         [0.2351, 0.2326, 0.2280, 0.1220, 0.1823],\n",
      "         [0.1856, 0.1865, 0.1883, 0.2535, 0.1862]],\n",
      "\n",
      "        [[0.0838, 0.2291, 0.2291, 0.2291, 0.2291],\n",
      "         [0.2373, 0.2033, 0.1865, 0.1865, 0.1865],\n",
      "         [0.2369, 0.2097, 0.1617, 0.1958, 0.1958],\n",
      "         [0.1638, 0.1880, 0.2520, 0.1931, 0.2031],\n",
      "         [0.3002, 0.2198, 0.1132, 0.2067, 0.1601]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2107, 0.1973, 0.1973, 0.1973, 0.1973],\n",
      "         [0.1817, 0.2363, 0.1940, 0.1940, 0.1940],\n",
      "         [0.2266, 0.1898, 0.1500, 0.2168, 0.2168],\n",
      "         [0.3286, 0.1738, 0.0745, 0.1429, 0.2802],\n",
      "         [0.1271, 0.1923, 0.3333, 0.2183, 0.1291]],\n",
      "\n",
      "        [[0.2735, 0.1816, 0.1816, 0.1816, 0.1816],\n",
      "         [0.3171, 0.1222, 0.1869, 0.1869, 0.1869],\n",
      "         [0.3176, 0.1279, 0.1710, 0.1918, 0.1918],\n",
      "         [0.6840, 0.0382, 0.0959, 0.0438, 0.1380],\n",
      "         [0.1645, 0.2164, 0.1983, 0.2136, 0.2072]],\n",
      "\n",
      "        [[0.2034, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2407, 0.0653, 0.2313, 0.2313, 0.2313],\n",
      "         [0.1950, 0.2072, 0.2070, 0.1954, 0.1954],\n",
      "         [0.3034, 0.1531, 0.1553, 0.0911, 0.2971],\n",
      "         [0.2525, 0.1920, 0.1931, 0.1560, 0.2065]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2874, 0.0680, 0.2149, 0.2149, 0.2149],\n",
      "         [0.2135, 0.1389, 0.2559, 0.1958, 0.1958],\n",
      "         [0.2418, 0.1186, 0.3264, 0.1036, 0.2095],\n",
      "         [0.1688, 0.2154, 0.1524, 0.2256, 0.2377]],\n",
      "\n",
      "        [[0.1941, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1938, 0.1573, 0.2163, 0.2163, 0.2163],\n",
      "         [0.1904, 0.1664, 0.2346, 0.2043, 0.2043],\n",
      "         [0.1996, 0.1989, 0.2009, 0.2006, 0.2001],\n",
      "         [0.1701, 0.2249, 0.1101, 0.1210, 0.3739]],\n",
      "\n",
      "        [[0.2528, 0.1868, 0.1868, 0.1868, 0.1868],\n",
      "         [0.2033, 0.2113, 0.1951, 0.1951, 0.1951],\n",
      "         [0.2373, 0.3145, 0.0971, 0.1756, 0.1756],\n",
      "         [0.2670, 0.3407, 0.1231, 0.0637, 0.2056],\n",
      "         [0.1634, 0.1478, 0.2246, 0.2944, 0.1697]],\n",
      "\n",
      "        [[0.2056, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.2209, 0.1865, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1865, 0.1967, 0.2305, 0.1931, 0.1931],\n",
      "         [0.2573, 0.2268, 0.1554, 0.1237, 0.2368],\n",
      "         [0.2548, 0.2354, 0.1857, 0.1609, 0.1632]],\n",
      "\n",
      "        [[0.1534, 0.2116, 0.2116, 0.2116, 0.2116],\n",
      "         [0.1875, 0.0657, 0.2490, 0.2490, 0.2490],\n",
      "         [0.1919, 0.2678, 0.1897, 0.1753, 0.1753],\n",
      "         [0.2114, 0.1007, 0.2168, 0.2127, 0.2584],\n",
      "         [0.1983, 0.2038, 0.1982, 0.1983, 0.2014]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1636, 0.2427, 0.1979, 0.1979, 0.1979],\n",
      "         [0.2048, 0.1967, 0.1968, 0.2009, 0.2009],\n",
      "         [0.1691, 0.2034, 0.2032, 0.2394, 0.1849],\n",
      "         [0.2565, 0.1649, 0.1651, 0.1115, 0.3021]],\n",
      "\n",
      "        [[0.2353, 0.1912, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2272, 0.2017, 0.1904, 0.1904, 0.1904],\n",
      "         [0.1865, 0.2051, 0.1787, 0.2148, 0.2148],\n",
      "         [0.2008, 0.1729, 0.2149, 0.2507, 0.1608],\n",
      "         [0.1794, 0.2247, 0.1620, 0.1284, 0.3055]],\n",
      "\n",
      "        [[0.1689, 0.2078, 0.2078, 0.2078, 0.2078],\n",
      "         [0.2557, 0.1687, 0.1919, 0.1919, 0.1919],\n",
      "         [0.1781, 0.2204, 0.1888, 0.2064, 0.2064],\n",
      "         [0.1824, 0.2169, 0.1912, 0.2039, 0.2056],\n",
      "         [0.0931, 0.3242, 0.1309, 0.2077, 0.2440]],\n",
      "\n",
      "        [[0.2026, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1948, 0.2143, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1926, 0.1994, 0.2211, 0.1934, 0.1934],\n",
      "         [0.2264, 0.2083, 0.1624, 0.1787, 0.2242],\n",
      "         [0.2145, 0.2049, 0.1788, 0.1885, 0.2132]],\n",
      "\n",
      "        [[0.2293, 0.1927, 0.1927, 0.1927, 0.1927],\n",
      "         [0.2190, 0.2116, 0.1898, 0.1898, 0.1898],\n",
      "         [0.2204, 0.2112, 0.2001, 0.1842, 0.1842],\n",
      "         [0.1807, 0.1921, 0.2075, 0.1860, 0.2336],\n",
      "         [0.1916, 0.1958, 0.2014, 0.1936, 0.2176]],\n",
      "\n",
      "        [[0.2243, 0.1939, 0.1939, 0.1939, 0.1939],\n",
      "         [0.1518, 0.1989, 0.2164, 0.2164, 0.2164],\n",
      "         [0.1910, 0.2004, 0.2016, 0.2035, 0.2035],\n",
      "         [0.1517, 0.1784, 0.1817, 0.3005, 0.1877],\n",
      "         [0.2546, 0.1850, 0.1784, 0.0663, 0.3157]],\n",
      "\n",
      "        [[0.2078, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.2129, 0.2156, 0.1905, 0.1905, 0.1905],\n",
      "         [0.1960, 0.1965, 0.2245, 0.1915, 0.1915],\n",
      "         [0.2138, 0.2128, 0.1669, 0.1833, 0.2232],\n",
      "         [0.2057, 0.2046, 0.1555, 0.1729, 0.2614]],\n",
      "\n",
      "        [[0.2268, 0.1933, 0.1933, 0.1933, 0.1933],\n",
      "         [0.2166, 0.2540, 0.1765, 0.1765, 0.1765],\n",
      "         [0.1858, 0.1577, 0.1973, 0.2296, 0.2296],\n",
      "         [0.1962, 0.1801, 0.2024, 0.2022, 0.2191],\n",
      "         [0.1780, 0.1443, 0.1922, 0.1916, 0.2939]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1424, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1335, 0.3079, 0.1862, 0.1862, 0.1862],\n",
      "         [0.1192, 0.3505, 0.1642, 0.1831, 0.1831],\n",
      "         [0.3347, 0.0825, 0.2208, 0.1704, 0.1916],\n",
      "         [0.0390, 0.6752, 0.0909, 0.1541, 0.0407]],\n",
      "\n",
      "        [[0.2758, 0.1810, 0.1810, 0.1810, 0.1810],\n",
      "         [0.1899, 0.2076, 0.2008, 0.2008, 0.2008],\n",
      "         [0.2438, 0.1674, 0.2033, 0.1927, 0.1927],\n",
      "         [0.1792, 0.2170, 0.1966, 0.2052, 0.2020],\n",
      "         [0.3980, 0.1306, 0.2323, 0.1806, 0.0585]],\n",
      "\n",
      "        [[0.2184, 0.1954, 0.1954, 0.1954, 0.1954],\n",
      "         [0.1816, 0.2471, 0.1904, 0.1904, 0.1904],\n",
      "         [0.2128, 0.1599, 0.2201, 0.2036, 0.2036],\n",
      "         [0.1435, 0.3232, 0.1304, 0.2402, 0.1626],\n",
      "         [0.2480, 0.1318, 0.2673, 0.1661, 0.1869]],\n",
      "\n",
      "        [[0.2643, 0.1839, 0.1839, 0.1839, 0.1839],\n",
      "         [0.1321, 0.2610, 0.2023, 0.2023, 0.2023],\n",
      "         [0.2789, 0.1662, 0.1515, 0.2017, 0.2017],\n",
      "         [0.1784, 0.2155, 0.2230, 0.1824, 0.2008],\n",
      "         [0.2694, 0.1425, 0.1271, 0.2502, 0.2108]],\n",
      "\n",
      "        [[0.1758, 0.2060, 0.2060, 0.2060, 0.2060],\n",
      "         [0.1839, 0.1915, 0.2082, 0.2082, 0.2082],\n",
      "         [0.1996, 0.1998, 0.2004, 0.2001, 0.2001],\n",
      "         [0.1636, 0.1715, 0.2053, 0.2705, 0.1890],\n",
      "         [0.1514, 0.1627, 0.2139, 0.3255, 0.1465]],\n",
      "\n",
      "        [[0.1937, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.1836, 0.2318, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1977, 0.2109, 0.1894, 0.2010, 0.2010],\n",
      "         [0.2090, 0.1223, 0.2979, 0.1886, 0.1821],\n",
      "         [0.1642, 0.4568, 0.0835, 0.1997, 0.0959]],\n",
      "\n",
      "        [[0.1960, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1195, 0.1389, 0.2472, 0.2472, 0.2472],\n",
      "         [0.3221, 0.2671, 0.1508, 0.1300, 0.1300],\n",
      "         [0.1634, 0.1775, 0.2287, 0.1862, 0.2442],\n",
      "         [0.1423, 0.1585, 0.2198, 0.1685, 0.3109]],\n",
      "\n",
      "        [[0.2011, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2072, 0.1884, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1947, 0.2196, 0.1823, 0.2017, 0.2017],\n",
      "         [0.2152, 0.1996, 0.2242, 0.1506, 0.2104],\n",
      "         [0.1773, 0.1935, 0.1690, 0.2686, 0.1915]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1978, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1827, 0.1664, 0.2170, 0.2170, 0.2170],\n",
      "         [0.1911, 0.1787, 0.1977, 0.2162, 0.2162],\n",
      "         [0.2415, 0.3227, 0.2084, 0.0859, 0.1415],\n",
      "         [0.1645, 0.1261, 0.1883, 0.4243, 0.0968]],\n",
      "\n",
      "        [[0.2020, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1739, 0.2124, 0.2045, 0.2045, 0.2045],\n",
      "         [0.2284, 0.1387, 0.3281, 0.1524, 0.1524],\n",
      "         [0.2368, 0.1242, 0.3783, 0.1204, 0.1403],\n",
      "         [0.1530, 0.2714, 0.1009, 0.2790, 0.1958]],\n",
      "\n",
      "        [[0.1847, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1832, 0.1641, 0.2175, 0.2175, 0.2175],\n",
      "         [0.2046, 0.2073, 0.1872, 0.2004, 0.2004],\n",
      "         [0.1250, 0.1033, 0.4511, 0.1522, 0.1684],\n",
      "         [0.1978, 0.1965, 0.2072, 0.1992, 0.1992]],\n",
      "\n",
      "        [[0.2562, 0.1859, 0.1859, 0.1859, 0.1859],\n",
      "         [0.3630, 0.1424, 0.1649, 0.1649, 0.1649],\n",
      "         [0.1432, 0.2381, 0.1790, 0.2199, 0.2199],\n",
      "         [0.2845, 0.1323, 0.2033, 0.2307, 0.1492],\n",
      "         [0.1903, 0.2129, 0.1999, 0.1963, 0.2005]],\n",
      "\n",
      "        [[0.2049, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1912, 0.2092, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1998, 0.1880, 0.2243, 0.1939, 0.1939],\n",
      "         [0.1871, 0.1665, 0.2332, 0.2365, 0.1767],\n",
      "         [0.2177, 0.2434, 0.1764, 0.1741, 0.1882]],\n",
      "\n",
      "        [[0.1952, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2006, 0.1981, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1819, 0.2616, 0.1823, 0.1871, 0.1871],\n",
      "         [0.1685, 0.2531, 0.1689, 0.2355, 0.1740],\n",
      "         [0.2435, 0.1596, 0.2429, 0.1720, 0.1819]],\n",
      "\n",
      "        [[0.2414, 0.1896, 0.1896, 0.1896, 0.1896],\n",
      "         [0.2390, 0.1518, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1860, 0.2529, 0.1456, 0.2077, 0.2077],\n",
      "         [0.2117, 0.1148, 0.3449, 0.1588, 0.1698],\n",
      "         [0.2045, 0.1741, 0.2325, 0.1896, 0.1993]],\n",
      "\n",
      "        [[0.1987, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2047, 0.1719, 0.2078, 0.2078, 0.2078],\n",
      "         [0.1740, 0.1565, 0.3182, 0.1756, 0.1756],\n",
      "         [0.1450, 0.1186, 0.4555, 0.1334, 0.1475],\n",
      "         [0.1575, 0.1389, 0.3225, 0.1495, 0.2315]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2028, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.1880, 0.2597, 0.1841, 0.1841, 0.1841],\n",
      "         [0.1956, 0.1821, 0.2294, 0.1965, 0.1965],\n",
      "         [0.1995, 0.1979, 0.2032, 0.1998, 0.1996],\n",
      "         [0.2216, 0.2510, 0.1678, 0.2160, 0.1436]],\n",
      "\n",
      "        [[0.2641, 0.1840, 0.1840, 0.1840, 0.1840],\n",
      "         [0.2943, 0.1123, 0.1978, 0.1978, 0.1978],\n",
      "         [0.1764, 0.2439, 0.1765, 0.2016, 0.2016],\n",
      "         [0.1856, 0.2135, 0.1857, 0.2186, 0.1966],\n",
      "         [0.2334, 0.1902, 0.2333, 0.1837, 0.1594]],\n",
      "\n",
      "        [[0.2227, 0.1943, 0.1943, 0.1943, 0.1943],\n",
      "         [0.2713, 0.1267, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1102, 0.2963, 0.2675, 0.1630, 0.1630],\n",
      "         [0.1050, 0.3123, 0.2791, 0.1418, 0.1618],\n",
      "         [0.1951, 0.2050, 0.2039, 0.1977, 0.1983]],\n",
      "\n",
      "        [[0.2503, 0.1874, 0.1874, 0.1874, 0.1874],\n",
      "         [0.2731, 0.2149, 0.1707, 0.1707, 0.1707],\n",
      "         [0.3306, 0.2341, 0.0995, 0.1679, 0.1679],\n",
      "         [0.1464, 0.1789, 0.2941, 0.1637, 0.2170],\n",
      "         [0.1972, 0.1993, 0.2046, 0.1983, 0.2006]],\n",
      "\n",
      "        [[0.1043, 0.2239, 0.2239, 0.2239, 0.2239],\n",
      "         [0.1184, 0.0939, 0.2626, 0.2626, 0.2626],\n",
      "         [0.2200, 0.2359, 0.1978, 0.1731, 0.1731],\n",
      "         [0.2416, 0.2719, 0.2019, 0.1234, 0.1612],\n",
      "         [0.1747, 0.1585, 0.2025, 0.3040, 0.1604]],\n",
      "\n",
      "        [[0.3120, 0.1720, 0.1720, 0.1720, 0.1720],\n",
      "         [0.3896, 0.1108, 0.1665, 0.1665, 0.1665],\n",
      "         [0.1563, 0.2103, 0.2514, 0.1910, 0.1910],\n",
      "         [0.1582, 0.2113, 0.2515, 0.1866, 0.1924],\n",
      "         [0.2507, 0.1926, 0.1645, 0.2157, 0.1765]],\n",
      "\n",
      "        [[0.2738, 0.1816, 0.1816, 0.1816, 0.1816],\n",
      "         [0.2130, 0.1937, 0.1977, 0.1977, 0.1977],\n",
      "         [0.1386, 0.2002, 0.2913, 0.1849, 0.1849],\n",
      "         [0.1522, 0.2128, 0.2995, 0.1375, 0.1980],\n",
      "         [0.0874, 0.2054, 0.4905, 0.0675, 0.1492]],\n",
      "\n",
      "        [[0.2387, 0.1903, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2162, 0.1800, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1837, 0.2166, 0.2077, 0.1960, 0.1960],\n",
      "         [0.1997, 0.2024, 0.2017, 0.1954, 0.2007],\n",
      "         [0.1999, 0.1963, 0.1972, 0.2057, 0.2010]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1733, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.1541, 0.3166, 0.1764, 0.1764, 0.1764],\n",
      "         [0.1893, 0.2324, 0.1848, 0.1968, 0.1968],\n",
      "         [0.2364, 0.0590, 0.2788, 0.2435, 0.1823],\n",
      "         [0.1532, 0.3513, 0.1387, 0.1505, 0.2063]],\n",
      "\n",
      "        [[0.3593, 0.1602, 0.1602, 0.1602, 0.1602],\n",
      "         [0.1591, 0.2355, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2821, 0.1397, 0.2096, 0.1842, 0.1842],\n",
      "         [0.1616, 0.2362, 0.1897, 0.2089, 0.2035],\n",
      "         [0.0691, 0.2573, 0.1204, 0.1682, 0.3850]],\n",
      "\n",
      "        [[0.2027, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.2020, 0.2183, 0.1932, 0.1932, 0.1932],\n",
      "         [0.2028, 0.2305, 0.1895, 0.1886, 0.1886],\n",
      "         [0.1963, 0.2740, 0.1643, 0.2030, 0.1624],\n",
      "         [0.1496, 0.2224, 0.1211, 0.1557, 0.3512]],\n",
      "\n",
      "        [[0.1881, 0.2030, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1916, 0.2113, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2063, 0.1943, 0.1963, 0.2016, 0.2016],\n",
      "         [0.1688, 0.2187, 0.2093, 0.2167, 0.1865],\n",
      "         [0.2106, 0.1948, 0.1974, 0.1953, 0.2020]],\n",
      "\n",
      "        [[0.2014, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1904, 0.2513, 0.1861, 0.1861, 0.1861],\n",
      "         [0.2000, 0.2008, 0.1992, 0.2000, 0.2000],\n",
      "         [0.2042, 0.2508, 0.1633, 0.1809, 0.2008],\n",
      "         [0.1831, 0.1375, 0.2500, 0.2169, 0.2125]],\n",
      "\n",
      "        [[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1266, 0.5090, 0.1214, 0.1214, 0.1214],\n",
      "         [0.1833, 0.2609, 0.1931, 0.1814, 0.1814],\n",
      "         [0.2123, 0.1203, 0.1952, 0.2562, 0.2160],\n",
      "         [0.2050, 0.1203, 0.1894, 0.2444, 0.2408]],\n",
      "\n",
      "        [[0.2677, 0.1831, 0.1831, 0.1831, 0.1831],\n",
      "         [0.1664, 0.1800, 0.2179, 0.2179, 0.2179],\n",
      "         [0.1811, 0.1897, 0.2050, 0.2121, 0.2121],\n",
      "         [0.1135, 0.1448, 0.2183, 0.2622, 0.2612],\n",
      "         [0.1075, 0.1382, 0.2115, 0.2558, 0.2869]],\n",
      "\n",
      "        [[0.1956, 0.2011, 0.2011, 0.2011, 0.2011],\n",
      "         [0.2064, 0.2047, 0.1963, 0.1963, 0.1963],\n",
      "         [0.1853, 0.1866, 0.2412, 0.1934, 0.1934],\n",
      "         [0.2481, 0.2440, 0.1335, 0.1500, 0.2243],\n",
      "         [0.2290, 0.2261, 0.1420, 0.1554, 0.2475]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1728, 0.2068, 0.2068, 0.2068, 0.2068],\n",
      "         [0.2248, 0.1684, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1732, 0.2560, 0.1712, 0.1998, 0.1998],\n",
      "         [0.1768, 0.1021, 0.1797, 0.3967, 0.1447],\n",
      "         [0.1887, 0.2480, 0.1872, 0.1262, 0.2499]],\n",
      "\n",
      "        [[0.2141, 0.1965, 0.1965, 0.1965, 0.1965],\n",
      "         [0.1989, 0.1446, 0.2188, 0.2188, 0.2188],\n",
      "         [0.1801, 0.2550, 0.2402, 0.1623, 0.1623],\n",
      "         [0.1957, 0.2040, 0.2026, 0.2044, 0.1933],\n",
      "         [0.2324, 0.1873, 0.1943, 0.1855, 0.2005]],\n",
      "\n",
      "        [[0.2270, 0.1932, 0.1932, 0.1932, 0.1932],\n",
      "         [0.1167, 0.1188, 0.2549, 0.2549, 0.2549],\n",
      "         [0.2668, 0.2637, 0.1527, 0.1584, 0.1584],\n",
      "         [0.1873, 0.1879, 0.2169, 0.1931, 0.2148],\n",
      "         [0.2093, 0.2078, 0.1502, 0.1954, 0.2373]],\n",
      "\n",
      "        [[0.3441, 0.1640, 0.1640, 0.1640, 0.1640],\n",
      "         [0.1827, 0.1649, 0.2175, 0.2175, 0.2175],\n",
      "         [0.2097, 0.2342, 0.2080, 0.1740, 0.1740],\n",
      "         [0.1795, 0.2395, 0.1757, 0.2951, 0.1102],\n",
      "         [0.1906, 0.1478, 0.1942, 0.1230, 0.3444]],\n",
      "\n",
      "        [[0.1950, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2028, 0.1928, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1540, 0.3339, 0.1703, 0.1709, 0.1709],\n",
      "         [0.1242, 0.3919, 0.1442, 0.1947, 0.1450],\n",
      "         [0.1870, 0.1026, 0.1730, 0.1479, 0.3894]],\n",
      "\n",
      "        [[0.2557, 0.1861, 0.1861, 0.1861, 0.1861],\n",
      "         [0.2175, 0.1910, 0.1972, 0.1972, 0.1972],\n",
      "         [0.0856, 0.2826, 0.2096, 0.2111, 0.2111],\n",
      "         [0.2253, 0.1849, 0.1943, 0.2014, 0.1941],\n",
      "         [0.1699, 0.2261, 0.2105, 0.1998, 0.1938]],\n",
      "\n",
      "        [[0.2081, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.1937, 0.2370, 0.1898, 0.1898, 0.1898],\n",
      "         [0.2023, 0.2413, 0.1591, 0.1987, 0.1987],\n",
      "         [0.1249, 0.2032, 0.0645, 0.4885, 0.1189],\n",
      "         [0.2247, 0.1779, 0.3088, 0.1167, 0.1718]],\n",
      "\n",
      "        [[0.1813, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1716, 0.1997, 0.2096, 0.2096, 0.2096],\n",
      "         [0.2867, 0.1967, 0.1681, 0.1743, 0.1743],\n",
      "         [0.2068, 0.1082, 0.0826, 0.5145, 0.0879],\n",
      "         [0.2032, 0.1444, 0.1253, 0.3282, 0.1989]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1218, 0.2196, 0.2196, 0.2196, 0.2196],\n",
      "         [0.3756, 0.1249, 0.1665, 0.1665, 0.1665],\n",
      "         [0.0992, 0.2522, 0.2533, 0.1976, 0.1976],\n",
      "         [0.0140, 0.1390, 0.1404, 0.6302, 0.0763],\n",
      "         [0.2704, 0.1559, 0.1556, 0.1085, 0.3096]],\n",
      "\n",
      "        [[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1975, 0.2284, 0.1914, 0.1914, 0.1914],\n",
      "         [0.1885, 0.4157, 0.0781, 0.1588, 0.1588],\n",
      "         [0.1603, 0.4129, 0.0559, 0.2404, 0.1306],\n",
      "         [0.1994, 0.1913, 0.2089, 0.1959, 0.2045]],\n",
      "\n",
      "        [[0.2547, 0.1863, 0.1863, 0.1863, 0.1863],\n",
      "         [0.5153, 0.0846, 0.1334, 0.1334, 0.1334],\n",
      "         [0.3459, 0.1309, 0.1888, 0.1672, 0.1672],\n",
      "         [0.2618, 0.1613, 0.1936, 0.2010, 0.1823],\n",
      "         [0.1786, 0.2225, 0.2048, 0.2014, 0.1927]],\n",
      "\n",
      "        [[0.2372, 0.1907, 0.1907, 0.1907, 0.1907],\n",
      "         [0.1151, 0.2344, 0.2168, 0.2168, 0.2168],\n",
      "         [0.0643, 0.1509, 0.5101, 0.1374, 0.1374],\n",
      "         [0.0483, 0.1189, 0.4298, 0.2952, 0.1077],\n",
      "         [0.3729, 0.2095, 0.0920, 0.1170, 0.2086]],\n",
      "\n",
      "        [[0.2388, 0.1903, 0.1903, 0.1903, 0.1903],\n",
      "         [0.1971, 0.3694, 0.1445, 0.1445, 0.1445],\n",
      "         [0.2034, 0.4458, 0.0749, 0.1380, 0.1380],\n",
      "         [0.2042, 0.2375, 0.1684, 0.2004, 0.1895],\n",
      "         [0.1696, 0.1322, 0.2329, 0.1748, 0.2905]],\n",
      "\n",
      "        [[0.1508, 0.2123, 0.2123, 0.2123, 0.2123],\n",
      "         [0.2028, 0.2031, 0.1980, 0.1980, 0.1980],\n",
      "         [0.1747, 0.1697, 0.1280, 0.2638, 0.2638],\n",
      "         [0.1043, 0.0989, 0.0591, 0.5170, 0.2207],\n",
      "         [0.2113, 0.2200, 0.3253, 0.0626, 0.1808]],\n",
      "\n",
      "        [[0.2011, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1856, 0.2532, 0.1871, 0.1871, 0.1871],\n",
      "         [0.2022, 0.1963, 0.1975, 0.2020, 0.2020],\n",
      "         [0.1091, 0.2098, 0.1822, 0.3878, 0.1111],\n",
      "         [0.2402, 0.1787, 0.1905, 0.1354, 0.2552]],\n",
      "\n",
      "        [[0.2414, 0.1897, 0.1897, 0.1897, 0.1897],\n",
      "         [0.2213, 0.2294, 0.1831, 0.1831, 0.1831],\n",
      "         [0.2351, 0.2424, 0.1220, 0.2003, 0.2003],\n",
      "         [0.2072, 0.2098, 0.1580, 0.2311, 0.1939],\n",
      "         [0.2084, 0.2102, 0.1734, 0.2244, 0.1836]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2036, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1942, 0.1694, 0.2121, 0.2121, 0.2121],\n",
      "         [0.2007, 0.2037, 0.1980, 0.1988, 0.1988],\n",
      "         [0.2048, 0.1696, 0.2427, 0.1516, 0.2313],\n",
      "         [0.2016, 0.2217, 0.1850, 0.2346, 0.1572]],\n",
      "\n",
      "        [[0.1812, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1996, 0.1843, 0.2054, 0.2054, 0.2054],\n",
      "         [0.1808, 0.1282, 0.2815, 0.2047, 0.2047],\n",
      "         [0.1978, 0.1828, 0.2190, 0.1968, 0.2035],\n",
      "         [0.1994, 0.1806, 0.2265, 0.1981, 0.1953]],\n",
      "\n",
      "        [[0.1385, 0.2154, 0.2154, 0.2154, 0.2154],\n",
      "         [0.2111, 0.2196, 0.1898, 0.1898, 0.1898],\n",
      "         [0.1807, 0.1730, 0.2397, 0.2033, 0.2033],\n",
      "         [0.1892, 0.1852, 0.2176, 0.2074, 0.2006],\n",
      "         [0.1120, 0.0930, 0.3787, 0.2497, 0.1666]],\n",
      "\n",
      "        [[0.2388, 0.1903, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2117, 0.1841, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2268, 0.1810, 0.1735, 0.2094, 0.2094],\n",
      "         [0.2479, 0.1371, 0.1227, 0.2914, 0.2009],\n",
      "         [0.2142, 0.1842, 0.1791, 0.2233, 0.1992]],\n",
      "\n",
      "        [[0.1659, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.1559, 0.1348, 0.2364, 0.2364, 0.2364],\n",
      "         [0.2022, 0.2034, 0.1969, 0.1988, 0.1988],\n",
      "         [0.0702, 0.0499, 0.3224, 0.3717, 0.1858],\n",
      "         [0.2347, 0.2557, 0.1600, 0.1544, 0.1952]],\n",
      "\n",
      "        [[0.1794, 0.2052, 0.2052, 0.2052, 0.2052],\n",
      "         [0.2050, 0.2107, 0.1948, 0.1948, 0.1948],\n",
      "         [0.2163, 0.2324, 0.1731, 0.1891, 0.1891],\n",
      "         [0.1908, 0.1826, 0.2187, 0.2007, 0.2071],\n",
      "         [0.1919, 0.1844, 0.2174, 0.2010, 0.2052]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2019, 0.1978, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2010, 0.1984, 0.2008, 0.1999, 0.1999],\n",
      "         [0.2073, 0.1532, 0.2025, 0.2549, 0.1821],\n",
      "         [0.2024, 0.2149, 0.2034, 0.1943, 0.1850]],\n",
      "\n",
      "        [[0.1567, 0.2108, 0.2108, 0.2108, 0.2108],\n",
      "         [0.1874, 0.1917, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1690, 0.1768, 0.2425, 0.2058, 0.2058],\n",
      "         [0.0961, 0.1131, 0.3564, 0.2377, 0.1966],\n",
      "         [0.0886, 0.1046, 0.3370, 0.2230, 0.2467]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1969, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.2111, 0.1824, 0.2022, 0.2022, 0.2022],\n",
      "         [0.2370, 0.1459, 0.2064, 0.2053, 0.2053],\n",
      "         [0.1754, 0.2225, 0.1877, 0.2263, 0.1882],\n",
      "         [0.2756, 0.1157, 0.2153, 0.1089, 0.2844]],\n",
      "\n",
      "        [[0.1985, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1966, 0.1766, 0.2089, 0.2089, 0.2089],\n",
      "         [0.1968, 0.1765, 0.2081, 0.2093, 0.2093],\n",
      "         [0.2030, 0.2659, 0.1766, 0.1802, 0.1742],\n",
      "         [0.1847, 0.1208, 0.2299, 0.2227, 0.2418]],\n",
      "\n",
      "        [[0.1942, 0.2014, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2130, 0.1906, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1953, 0.2044, 0.1986, 0.2009, 0.2009],\n",
      "         [0.2108, 0.1917, 0.2036, 0.1953, 0.1987],\n",
      "         [0.1959, 0.2121, 0.2017, 0.2087, 0.1816]],\n",
      "\n",
      "        [[0.1916, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.1755, 0.2410, 0.1945, 0.1945, 0.1945],\n",
      "         [0.2876, 0.1593, 0.0784, 0.2374, 0.2374],\n",
      "         [0.2471, 0.2008, 0.1565, 0.1647, 0.2310],\n",
      "         [0.1857, 0.2014, 0.2219, 0.2175, 0.1734]],\n",
      "\n",
      "        [[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.2138, 0.1386, 0.2159, 0.2159, 0.2159],\n",
      "         [0.1940, 0.2261, 0.1932, 0.1934, 0.1934],\n",
      "         [0.2096, 0.1529, 0.2113, 0.2152, 0.2110],\n",
      "         [0.1851, 0.3597, 0.1819, 0.1750, 0.0983]],\n",
      "\n",
      "        [[0.2065, 0.1984, 0.1984, 0.1984, 0.1984],\n",
      "         [0.1986, 0.2360, 0.1884, 0.1884, 0.1884],\n",
      "         [0.2239, 0.2944, 0.0697, 0.2060, 0.2060],\n",
      "         [0.2024, 0.2110, 0.1696, 0.2171, 0.1999],\n",
      "         [0.1699, 0.1437, 0.3469, 0.1281, 0.2113]],\n",
      "\n",
      "        [[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1781, 0.2865, 0.1785, 0.1785, 0.1785],\n",
      "         [0.2116, 0.1681, 0.1975, 0.2114, 0.2114],\n",
      "         [0.2173, 0.1928, 0.2097, 0.1629, 0.2172],\n",
      "         [0.1703, 0.2147, 0.1826, 0.2976, 0.1349]],\n",
      "\n",
      "        [[0.2253, 0.1937, 0.1937, 0.1937, 0.1937],\n",
      "         [0.2988, 0.2035, 0.1659, 0.1659, 0.1659],\n",
      "         [0.2170, 0.2030, 0.1883, 0.1959, 0.1959],\n",
      "         [0.2300, 0.1958, 0.1635, 0.2308, 0.1798],\n",
      "         [0.2084, 0.1989, 0.1887, 0.2087, 0.1954]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.2048, 0.1881, 0.2024, 0.2024, 0.2024],\n",
      "         [0.1722, 0.3446, 0.1044, 0.1894, 0.1894],\n",
      "         [0.2010, 0.2293, 0.1828, 0.1821, 0.2047],\n",
      "         [0.2000, 0.2016, 0.1989, 0.1988, 0.2007]],\n",
      "\n",
      "        [[0.1760, 0.2060, 0.2060, 0.2060, 0.2060],\n",
      "         [0.2045, 0.1982, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2187, 0.2005, 0.1745, 0.2032, 0.2032],\n",
      "         [0.2420, 0.2071, 0.1614, 0.1776, 0.2120],\n",
      "         [0.2239, 0.2024, 0.1722, 0.1832, 0.2183]],\n",
      "\n",
      "        [[0.1863, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1882, 0.1558, 0.2187, 0.2187, 0.2187],\n",
      "         [0.1453, 0.0732, 0.2797, 0.2509, 0.2509],\n",
      "         [0.1852, 0.1040, 0.3214, 0.0960, 0.2933],\n",
      "         [0.2128, 0.1754, 0.2559, 0.1707, 0.1853]],\n",
      "\n",
      "        [[0.2103, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.1929, 0.1767, 0.2101, 0.2101, 0.2101],\n",
      "         [0.2034, 0.2105, 0.1926, 0.1967, 0.1967],\n",
      "         [0.1828, 0.1316, 0.3082, 0.1255, 0.2519],\n",
      "         [0.1953, 0.1243, 0.4003, 0.1164, 0.1638]],\n",
      "\n",
      "        [[0.2263, 0.1934, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2228, 0.2023, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2009, 0.2000, 0.2000, 0.1996, 0.1996],\n",
      "         [0.2065, 0.1991, 0.1989, 0.2004, 0.1951],\n",
      "         [0.2736, 0.1875, 0.1849, 0.1999, 0.1542]],\n",
      "\n",
      "        [[0.3793, 0.1552, 0.1552, 0.1552, 0.1552],\n",
      "         [0.3634, 0.1489, 0.1626, 0.1626, 0.1626],\n",
      "         [0.1156, 0.2538, 0.1608, 0.2349, 0.2349],\n",
      "         [0.0755, 0.2557, 0.1260, 0.3161, 0.2268],\n",
      "         [0.2326, 0.1815, 0.2096, 0.1738, 0.2024]],\n",
      "\n",
      "        [[0.2103, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.1918, 0.1634, 0.2149, 0.2149, 0.2149],\n",
      "         [0.1021, 0.0504, 0.5099, 0.1688, 0.1688],\n",
      "         [0.1660, 0.1120, 0.4068, 0.0957, 0.2196],\n",
      "         [0.1729, 0.1056, 0.5315, 0.0868, 0.1031]],\n",
      "\n",
      "        [[0.2066, 0.1983, 0.1983, 0.1983, 0.1983],\n",
      "         [0.1809, 0.1996, 0.2065, 0.2065, 0.2065],\n",
      "         [0.1973, 0.2080, 0.1710, 0.2118, 0.2118],\n",
      "         [0.2054, 0.2080, 0.1985, 0.1792, 0.2089],\n",
      "         [0.2298, 0.2413, 0.2012, 0.1356, 0.1922]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2279, 0.1930, 0.1930, 0.1930, 0.1930],\n",
      "         [0.1786, 0.1606, 0.2203, 0.2203, 0.2203],\n",
      "         [0.1517, 0.1250, 0.2786, 0.2224, 0.2224],\n",
      "         [0.2319, 0.2645, 0.1532, 0.1718, 0.1786],\n",
      "         [0.1699, 0.1541, 0.2307, 0.2120, 0.2334]],\n",
      "\n",
      "        [[0.1660, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.2222, 0.3356, 0.1474, 0.1474, 0.1474],\n",
      "         [0.1270, 0.0765, 0.3755, 0.2104, 0.2104],\n",
      "         [0.1740, 0.1442, 0.2600, 0.2122, 0.2097],\n",
      "         [0.2327, 0.3013, 0.1339, 0.1771, 0.1550]],\n",
      "\n",
      "        [[0.1852, 0.2037, 0.2037, 0.2037, 0.2037],\n",
      "         [0.1925, 0.2743, 0.1777, 0.1777, 0.1777],\n",
      "         [0.1588, 0.1265, 0.3803, 0.1672, 0.1672],\n",
      "         [0.2439, 0.3110, 0.0960, 0.1183, 0.2309],\n",
      "         [0.1924, 0.1880, 0.2102, 0.2061, 0.2032]],\n",
      "\n",
      "        [[0.1707, 0.2073, 0.2073, 0.2073, 0.2073],\n",
      "         [0.1209, 0.2991, 0.1933, 0.1933, 0.1933],\n",
      "         [0.2618, 0.1596, 0.1734, 0.2026, 0.2026],\n",
      "         [0.2432, 0.1533, 0.1655, 0.2465, 0.1915],\n",
      "         [0.2536, 0.1357, 0.1506, 0.2583, 0.2018]],\n",
      "\n",
      "        [[0.2012, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2010, 0.2063, 0.1976, 0.1976, 0.1976],\n",
      "         [0.2126, 0.2421, 0.1547, 0.1952, 0.1952],\n",
      "         [0.1387, 0.1063, 0.2663, 0.3235, 0.1653],\n",
      "         [0.1867, 0.1757, 0.2165, 0.2263, 0.1947]],\n",
      "\n",
      "        [[0.2383, 0.1904, 0.1904, 0.1904, 0.1904],\n",
      "         [0.2076, 0.2214, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2018, 0.2039, 0.1965, 0.1989, 0.1989],\n",
      "         [0.2022, 0.2048, 0.1959, 0.1984, 0.1988],\n",
      "         [0.2162, 0.2321, 0.1812, 0.1943, 0.1762]],\n",
      "\n",
      "        [[0.2024, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1939, 0.1793, 0.2089, 0.2089, 0.2089],\n",
      "         [0.1747, 0.2003, 0.3184, 0.1533, 0.1533],\n",
      "         [0.1879, 0.1951, 0.2214, 0.2142, 0.1813],\n",
      "         [0.3054, 0.2628, 0.1582, 0.1807, 0.0930]],\n",
      "\n",
      "        [[0.2739, 0.1815, 0.1815, 0.1815, 0.1815],\n",
      "         [0.2596, 0.2048, 0.1785, 0.1785, 0.1785],\n",
      "         [0.0890, 0.1491, 0.3595, 0.2012, 0.2012],\n",
      "         [0.2845, 0.2218, 0.1450, 0.1568, 0.1919],\n",
      "         [0.2054, 0.2022, 0.1969, 0.1979, 0.1977]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1778, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.1365, 0.1209, 0.2475, 0.2475, 0.2475],\n",
      "         [0.1288, 0.1172, 0.3457, 0.2041, 0.2041],\n",
      "         [0.2526, 0.2749, 0.1042, 0.2013, 0.1671],\n",
      "         [0.1773, 0.1730, 0.2284, 0.1892, 0.2321]],\n",
      "\n",
      "        [[0.1186, 0.2204, 0.2204, 0.2204, 0.2204],\n",
      "         [0.2217, 0.1917, 0.1955, 0.1955, 0.1955],\n",
      "         [0.1927, 0.2058, 0.1935, 0.2040, 0.2040],\n",
      "         [0.2193, 0.1844, 0.2170, 0.1904, 0.1888],\n",
      "         [0.3169, 0.1516, 0.3030, 0.1737, 0.0547]],\n",
      "\n",
      "        [[0.2045, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1931, 0.1847, 0.2074, 0.2074, 0.2074],\n",
      "         [0.2010, 0.2065, 0.2076, 0.1925, 0.1925],\n",
      "         [0.2017, 0.1958, 0.1946, 0.1963, 0.2115],\n",
      "         [0.2302, 0.2089, 0.2048, 0.2108, 0.1453]],\n",
      "\n",
      "        [[0.2316, 0.1921, 0.1921, 0.1921, 0.1921],\n",
      "         [0.2059, 0.2392, 0.1850, 0.1850, 0.1850],\n",
      "         [0.2093, 0.2528, 0.1723, 0.1828, 0.1828],\n",
      "         [0.2013, 0.2382, 0.1694, 0.2125, 0.1785],\n",
      "         [0.1679, 0.1263, 0.2251, 0.1532, 0.3275]],\n",
      "\n",
      "        [[0.1726, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1897, 0.1460, 0.2214, 0.2214, 0.2214],\n",
      "         [0.2033, 0.2117, 0.1882, 0.1984, 0.1984],\n",
      "         [0.1985, 0.1967, 0.2017, 0.2036, 0.1995],\n",
      "         [0.1913, 0.1754, 0.2252, 0.2463, 0.1619]],\n",
      "\n",
      "        [[0.2147, 0.1963, 0.1963, 0.1963, 0.1963],\n",
      "         [0.1416, 0.1597, 0.2329, 0.2329, 0.2329],\n",
      "         [0.1683, 0.1768, 0.2420, 0.2064, 0.2064],\n",
      "         [0.2049, 0.2015, 0.1810, 0.2215, 0.1911],\n",
      "         [0.2037, 0.2027, 0.1963, 0.2086, 0.1887]],\n",
      "\n",
      "        [[0.1509, 0.2123, 0.2123, 0.2123, 0.2123],\n",
      "         [0.1198, 0.2545, 0.2086, 0.2086, 0.2086],\n",
      "         [0.1329, 0.2185, 0.2655, 0.1916, 0.1916],\n",
      "         [0.1913, 0.2035, 0.2085, 0.1964, 0.2002],\n",
      "         [0.1267, 0.2169, 0.2677, 0.1589, 0.2298]],\n",
      "\n",
      "        [[0.2240, 0.1940, 0.1940, 0.1940, 0.1940],\n",
      "         [0.2574, 0.1996, 0.1810, 0.1810, 0.1810],\n",
      "         [0.2263, 0.2013, 0.1877, 0.1924, 0.1924],\n",
      "         [0.2438, 0.2017, 0.1802, 0.1868, 0.1876],\n",
      "         [0.3274, 0.1964, 0.1448, 0.1596, 0.1718]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1400, 0.2150, 0.2150, 0.2150, 0.2150],\n",
      "         [0.1824, 0.2048, 0.2043, 0.2043, 0.2043],\n",
      "         [0.2017, 0.1997, 0.1993, 0.1997, 0.1997],\n",
      "         [0.1905, 0.2040, 0.2067, 0.1951, 0.2037],\n",
      "         [0.1973, 0.2016, 0.2024, 0.1988, 0.1999]],\n",
      "\n",
      "        [[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2062, 0.1743, 0.2065, 0.2065, 0.2065],\n",
      "         [0.1999, 0.2004, 0.1998, 0.1999, 0.1999],\n",
      "         [0.2109, 0.1338, 0.2318, 0.2119, 0.2116],\n",
      "         [0.2188, 0.1571, 0.2343, 0.2195, 0.1703]],\n",
      "\n",
      "        [[0.2112, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2008, 0.1657, 0.2112, 0.2112, 0.2112],\n",
      "         [0.1900, 0.1396, 0.2585, 0.2059, 0.2059],\n",
      "         [0.2010, 0.1840, 0.2196, 0.1897, 0.2057],\n",
      "         [0.1836, 0.2449, 0.1378, 0.2219, 0.2118]],\n",
      "\n",
      "        [[0.1789, 0.2053, 0.2053, 0.2053, 0.2053],\n",
      "         [0.1892, 0.1671, 0.2145, 0.2145, 0.2145],\n",
      "         [0.2026, 0.2189, 0.2039, 0.1873, 0.1873],\n",
      "         [0.2034, 0.2477, 0.2067, 0.1756, 0.1667],\n",
      "         [0.1999, 0.1983, 0.1998, 0.2011, 0.2009]],\n",
      "\n",
      "        [[0.1313, 0.2172, 0.2172, 0.2172, 0.2172],\n",
      "         [0.1961, 0.1970, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1260, 0.1381, 0.2614, 0.2372, 0.2372],\n",
      "         [0.1361, 0.1459, 0.2369, 0.2609, 0.2201],\n",
      "         [0.1619, 0.1698, 0.2355, 0.2514, 0.1814]],\n",
      "\n",
      "        [[0.2531, 0.1867, 0.1867, 0.1867, 0.1867],\n",
      "         [0.1579, 0.2055, 0.2122, 0.2122, 0.2122],\n",
      "         [0.1380, 0.2118, 0.2037, 0.2232, 0.2232],\n",
      "         [0.1095, 0.2294, 0.2146, 0.1953, 0.2512],\n",
      "         [0.2225, 0.1876, 0.1906, 0.1947, 0.2045]],\n",
      "\n",
      "        [[0.1963, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2002, 0.2000, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1638, 0.1820, 0.2847, 0.1847, 0.1847],\n",
      "         [0.2315, 0.2027, 0.1151, 0.2519, 0.1988],\n",
      "         [0.1926, 0.2033, 0.2555, 0.1862, 0.1624]],\n",
      "\n",
      "        [[0.1732, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.1417, 0.1649, 0.2312, 0.2312, 0.2312],\n",
      "         [0.2233, 0.2116, 0.1899, 0.1876, 0.1876],\n",
      "         [0.2709, 0.2219, 0.1487, 0.2162, 0.1422],\n",
      "         [0.2071, 0.2017, 0.1914, 0.2010, 0.1988]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1936, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.3398, 0.1176, 0.1809, 0.1809, 0.1809],\n",
      "         [0.0716, 0.3322, 0.2396, 0.1783, 0.1783],\n",
      "         [0.1369, 0.2524, 0.2216, 0.1922, 0.1969],\n",
      "         [0.1899, 0.2115, 0.2067, 0.2016, 0.1904]],\n",
      "\n",
      "        [[0.1799, 0.2050, 0.2050, 0.2050, 0.2050],\n",
      "         [0.2175, 0.1704, 0.2040, 0.2040, 0.2040],\n",
      "         [0.2437, 0.0867, 0.2978, 0.1859, 0.1859],\n",
      "         [0.2482, 0.0606, 0.3263, 0.1934, 0.1716],\n",
      "         [0.2395, 0.1373, 0.2667, 0.2170, 0.1395]],\n",
      "\n",
      "        [[0.2211, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.2243, 0.1432, 0.2108, 0.2108, 0.2108],\n",
      "         [0.2038, 0.1115, 0.3096, 0.1875, 0.1875],\n",
      "         [0.2124, 0.1441, 0.2780, 0.1643, 0.2013],\n",
      "         [0.2117, 0.1436, 0.2770, 0.1637, 0.2040]],\n",
      "\n",
      "        [[0.1857, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2188, 0.1562, 0.2084, 0.2084, 0.2084],\n",
      "         [0.2887, 0.1071, 0.1038, 0.2502, 0.2502],\n",
      "         [0.2371, 0.1050, 0.1024, 0.3446, 0.2108],\n",
      "         [0.2295, 0.1387, 0.1365, 0.2892, 0.2060]],\n",
      "\n",
      "        [[0.1390, 0.2153, 0.2153, 0.2153, 0.2153],\n",
      "         [0.1268, 0.1640, 0.2364, 0.2364, 0.2364],\n",
      "         [0.2225, 0.2094, 0.1836, 0.1922, 0.1922],\n",
      "         [0.2433, 0.2223, 0.1827, 0.1561, 0.1956],\n",
      "         [0.1477, 0.1698, 0.2299, 0.2931, 0.1595]],\n",
      "\n",
      "        [[0.2091, 0.1977, 0.1977, 0.1977, 0.1977],\n",
      "         [0.2518, 0.1883, 0.1866, 0.1866, 0.1866],\n",
      "         [0.1772, 0.2147, 0.1763, 0.2159, 0.2159],\n",
      "         [0.1779, 0.2139, 0.1770, 0.2161, 0.2151],\n",
      "         [0.2035, 0.1937, 0.2038, 0.1932, 0.2059]],\n",
      "\n",
      "        [[0.1618, 0.2095, 0.2095, 0.2095, 0.2095],\n",
      "         [0.1943, 0.1938, 0.2040, 0.2040, 0.2040],\n",
      "         [0.2131, 0.2149, 0.2151, 0.1785, 0.1785],\n",
      "         [0.2050, 0.2078, 0.2081, 0.2257, 0.1534],\n",
      "         [0.1991, 0.1990, 0.1989, 0.1983, 0.2048]],\n",
      "\n",
      "        [[0.1863, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1729, 0.1761, 0.2170, 0.2170, 0.2170],\n",
      "         [0.1653, 0.1692, 0.2214, 0.2221, 0.2221],\n",
      "         [0.2006, 0.2005, 0.1997, 0.1995, 0.1997],\n",
      "         [0.2039, 0.2035, 0.1984, 0.1975, 0.1966]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1961, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.2032, 0.1979, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2232, 0.1580, 0.2662, 0.1763, 0.1763],\n",
      "         [0.1809, 0.2550, 0.1519, 0.1835, 0.2287],\n",
      "         [0.2195, 0.1236, 0.2941, 0.2145, 0.1483]],\n",
      "\n",
      "        [[0.2520, 0.1870, 0.1870, 0.1870, 0.1870],\n",
      "         [0.4388, 0.0636, 0.1659, 0.1659, 0.1659],\n",
      "         [0.0417, 0.5405, 0.1144, 0.1517, 0.1517],\n",
      "         [0.2402, 0.1634, 0.2064, 0.1921, 0.1978],\n",
      "         [0.1501, 0.2402, 0.1806, 0.1971, 0.2320]],\n",
      "\n",
      "        [[0.1857, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2415, 0.1072, 0.2171, 0.2171, 0.2171],\n",
      "         [0.1074, 0.4670, 0.1652, 0.1302, 0.1302],\n",
      "         [0.1687, 0.2308, 0.1849, 0.2400, 0.1757],\n",
      "         [0.0872, 0.2834, 0.1232, 0.3283, 0.1779]],\n",
      "\n",
      "        [[0.2289, 0.1928, 0.1928, 0.1928, 0.1928],\n",
      "         [0.1982, 0.1168, 0.2284, 0.2284, 0.2284],\n",
      "         [0.1995, 0.1552, 0.2183, 0.2134, 0.2134],\n",
      "         [0.1537, 0.3763, 0.1115, 0.2377, 0.1209],\n",
      "         [0.1228, 0.3046, 0.0887, 0.1912, 0.2927]],\n",
      "\n",
      "        [[0.1653, 0.2087, 0.2087, 0.2087, 0.2087],\n",
      "         [0.1762, 0.1604, 0.2212, 0.2212, 0.2212],\n",
      "         [0.2002, 0.2003, 0.1997, 0.2000, 0.2000],\n",
      "         [0.2632, 0.2986, 0.1269, 0.1175, 0.1938],\n",
      "         [0.2045, 0.2286, 0.1073, 0.1002, 0.3594]],\n",
      "\n",
      "        [[0.2006, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1736, 0.2611, 0.1884, 0.1884, 0.1884],\n",
      "         [0.2091, 0.1733, 0.2150, 0.2013, 0.2013],\n",
      "         [0.1948, 0.2505, 0.1876, 0.1623, 0.2049],\n",
      "         [0.1936, 0.1029, 0.2128, 0.3059, 0.1848]],\n",
      "\n",
      "        [[0.1902, 0.2025, 0.2025, 0.2025, 0.2025],\n",
      "         [0.2168, 0.1970, 0.1954, 0.1954, 0.1954],\n",
      "         [0.1866, 0.2109, 0.1766, 0.2130, 0.2130],\n",
      "         [0.1640, 0.1914, 0.1529, 0.2980, 0.1938],\n",
      "         [0.1822, 0.1895, 0.1790, 0.2119, 0.2373]],\n",
      "\n",
      "        [[0.1448, 0.2138, 0.2138, 0.2138, 0.2138],\n",
      "         [0.1792, 0.1360, 0.2282, 0.2282, 0.2282],\n",
      "         [0.2124, 0.4692, 0.1065, 0.1059, 0.1059],\n",
      "         [0.2063, 0.2852, 0.1556, 0.1976, 0.1553],\n",
      "         [0.1757, 0.3458, 0.0974, 0.1604, 0.2207]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1842, 0.2617, 0.1847, 0.1847, 0.1847],\n",
      "         [0.1947, 0.2094, 0.2062, 0.1948, 0.1948],\n",
      "         [0.1919, 0.1502, 0.1584, 0.3081, 0.1915],\n",
      "         [0.1741, 0.2325, 0.2183, 0.0995, 0.2756]],\n",
      "\n",
      "        [[0.2169, 0.1958, 0.1958, 0.1958, 0.1958],\n",
      "         [0.1824, 0.1579, 0.2199, 0.2199, 0.2199],\n",
      "         [0.2022, 0.2044, 0.1946, 0.1994, 0.1994],\n",
      "         [0.2083, 0.2581, 0.0989, 0.2768, 0.1579],\n",
      "         [0.1934, 0.1601, 0.3728, 0.1505, 0.1233]],\n",
      "\n",
      "        [[0.2548, 0.1863, 0.1863, 0.1863, 0.1863],\n",
      "         [0.2983, 0.1971, 0.1682, 0.1682, 0.1682],\n",
      "         [0.1762, 0.1963, 0.2182, 0.2046, 0.2046],\n",
      "         [0.3990, 0.2126, 0.1149, 0.1064, 0.1671],\n",
      "         [0.1265, 0.1838, 0.2648, 0.2770, 0.1479]],\n",
      "\n",
      "        [[0.2017, 0.1996, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2070, 0.2044, 0.1962, 0.1962, 0.1962],\n",
      "         [0.2085, 0.2115, 0.1371, 0.2215, 0.2215],\n",
      "         [0.1979, 0.2010, 0.1262, 0.2637, 0.2112],\n",
      "         [0.2001, 0.2000, 0.2020, 0.1988, 0.1991]],\n",
      "\n",
      "        [[0.4201, 0.1450, 0.1450, 0.1450, 0.1450],\n",
      "         [0.1182, 0.1270, 0.2516, 0.2516, 0.2516],\n",
      "         [0.3138, 0.2898, 0.1252, 0.1356, 0.1356],\n",
      "         [0.2356, 0.2292, 0.1711, 0.1881, 0.1759],\n",
      "         [0.2593, 0.2479, 0.1543, 0.1799, 0.1587]],\n",
      "\n",
      "        [[0.2039, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2125, 0.1694, 0.2060, 0.2060, 0.2060],\n",
      "         [0.2611, 0.1224, 0.1454, 0.2355, 0.2355],\n",
      "         [0.2084, 0.1837, 0.1891, 0.2140, 0.2049],\n",
      "         [0.1843, 0.2439, 0.2288, 0.1738, 0.1692]],\n",
      "\n",
      "        [[0.1390, 0.2152, 0.2152, 0.2152, 0.2152],\n",
      "         [0.1408, 0.1381, 0.2404, 0.2404, 0.2404],\n",
      "         [0.2025, 0.2026, 0.1976, 0.1986, 0.1986],\n",
      "         [0.2046, 0.2051, 0.1884, 0.2103, 0.1917],\n",
      "         [0.1999, 0.1976, 0.3013, 0.1745, 0.1266]],\n",
      "\n",
      "        [[0.1965, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2454, 0.2232, 0.1771, 0.1771, 0.1771],\n",
      "         [0.2366, 0.2234, 0.1520, 0.1940, 0.1940],\n",
      "         [0.2393, 0.2274, 0.1619, 0.1705, 0.2008],\n",
      "         [0.1600, 0.1687, 0.2405, 0.2277, 0.2031]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1812, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.2368, 0.2237, 0.1798, 0.1798, 0.1798],\n",
      "         [0.2358, 0.2283, 0.1322, 0.2018, 0.2018],\n",
      "         [0.2373, 0.2157, 0.0427, 0.3545, 0.1497],\n",
      "         [0.1003, 0.1089, 0.4388, 0.0710, 0.2809]],\n",
      "\n",
      "        [[0.2039, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1981, 0.2244, 0.1925, 0.1925, 0.1925],\n",
      "         [0.1969, 0.2127, 0.2035, 0.1935, 0.1935],\n",
      "         [0.1697, 0.2315, 0.1937, 0.2469, 0.1581],\n",
      "         [0.2092, 0.1754, 0.1941, 0.1691, 0.2522]],\n",
      "\n",
      "        [[0.1979, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1880, 0.2579, 0.1847, 0.1847, 0.1847],\n",
      "         [0.1956, 0.2995, 0.1230, 0.1909, 0.1909],\n",
      "         [0.1888, 0.2701, 0.1279, 0.2282, 0.1850],\n",
      "         [0.2023, 0.1260, 0.3389, 0.1575, 0.1753]],\n",
      "\n",
      "        [[0.1341, 0.2165, 0.2165, 0.2165, 0.2165],\n",
      "         [0.2985, 0.2862, 0.1385, 0.1385, 0.1385],\n",
      "         [0.2107, 0.2098, 0.1904, 0.1945, 0.1945],\n",
      "         [0.2791, 0.2696, 0.1259, 0.1765, 0.1489],\n",
      "         [0.1487, 0.1521, 0.2524, 0.2016, 0.2452]],\n",
      "\n",
      "        [[0.2684, 0.1829, 0.1829, 0.1829, 0.1829],\n",
      "         [0.2740, 0.2106, 0.1718, 0.1718, 0.1718],\n",
      "         [0.3675, 0.2152, 0.1330, 0.1421, 0.1421],\n",
      "         [0.2352, 0.2041, 0.1797, 0.1981, 0.1829],\n",
      "         [0.1705, 0.2005, 0.2320, 0.2075, 0.1895]],\n",
      "\n",
      "        [[0.1419, 0.2145, 0.2145, 0.2145, 0.2145],\n",
      "         [0.3512, 0.1882, 0.1536, 0.1536, 0.1536],\n",
      "         [0.2775, 0.1950, 0.1799, 0.1738, 0.1738],\n",
      "         [0.4353, 0.1312, 0.0999, 0.2448, 0.0888],\n",
      "         [0.0568, 0.1555, 0.1956, 0.0921, 0.4999]],\n",
      "\n",
      "        [[0.2953, 0.1762, 0.1762, 0.1762, 0.1762],\n",
      "         [0.5217, 0.1095, 0.1229, 0.1229, 0.1229],\n",
      "         [0.3991, 0.1635, 0.0880, 0.1747, 0.1747],\n",
      "         [0.2996, 0.1836, 0.1307, 0.1957, 0.1904],\n",
      "         [0.2684, 0.1961, 0.1577, 0.2043, 0.1735]],\n",
      "\n",
      "        [[0.3155, 0.1711, 0.1711, 0.1711, 0.1711],\n",
      "         [0.3240, 0.3676, 0.1028, 0.1028, 0.1028],\n",
      "         [0.1773, 0.1730, 0.2067, 0.2215, 0.2215],\n",
      "         [0.2111, 0.2144, 0.1914, 0.2001, 0.1831],\n",
      "         [0.1861, 0.1789, 0.2380, 0.2128, 0.1841]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2013, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2620, 0.2093, 0.1762, 0.1762, 0.1762],\n",
      "         [0.1650, 0.2005, 0.1691, 0.2327, 0.2327],\n",
      "         [0.2003, 0.2000, 0.2002, 0.1998, 0.1997],\n",
      "         [0.1444, 0.1822, 0.1487, 0.2090, 0.3157]],\n",
      "\n",
      "        [[0.2082, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.2332, 0.1760, 0.1969, 0.1969, 0.1969],\n",
      "         [0.1765, 0.2663, 0.1054, 0.2260, 0.2260],\n",
      "         [0.2033, 0.2285, 0.1756, 0.1746, 0.2181],\n",
      "         [0.1964, 0.2331, 0.1585, 0.1572, 0.2548]],\n",
      "\n",
      "        [[0.1964, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2112, 0.1765, 0.2041, 0.2041, 0.2041],\n",
      "         [0.3041, 0.0937, 0.1155, 0.2434, 0.2434],\n",
      "         [0.2437, 0.1654, 0.1772, 0.1872, 0.2265],\n",
      "         [0.3163, 0.1223, 0.1448, 0.1655, 0.2511]],\n",
      "\n",
      "        [[0.1334, 0.2166, 0.2166, 0.2166, 0.2166],\n",
      "         [0.2227, 0.2245, 0.1842, 0.1842, 0.1842],\n",
      "         [0.2301, 0.2316, 0.1472, 0.1955, 0.1955],\n",
      "         [0.0853, 0.0828, 0.5660, 0.0959, 0.1700],\n",
      "         [0.1959, 0.1956, 0.2160, 0.1971, 0.1954]],\n",
      "\n",
      "        [[0.1635, 0.2091, 0.2091, 0.2091, 0.2091],\n",
      "         [0.1829, 0.1718, 0.2151, 0.2151, 0.2151],\n",
      "         [0.2532, 0.2888, 0.0977, 0.1802, 0.1802],\n",
      "         [0.2834, 0.3264, 0.1022, 0.0911, 0.1969],\n",
      "         [0.2108, 0.2137, 0.1912, 0.1891, 0.1951]],\n",
      "\n",
      "        [[0.1759, 0.2060, 0.2060, 0.2060, 0.2060],\n",
      "         [0.2390, 0.2070, 0.1847, 0.1847, 0.1847],\n",
      "         [0.1808, 0.2058, 0.1573, 0.2280, 0.2280],\n",
      "         [0.2012, 0.1914, 0.2124, 0.2111, 0.1839],\n",
      "         [0.1723, 0.2159, 0.1353, 0.1391, 0.3374]],\n",
      "\n",
      "        [[0.2119, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1878, 0.1851, 0.2090, 0.2090, 0.2090],\n",
      "         [0.1885, 0.1862, 0.2131, 0.2061, 0.2061],\n",
      "         [0.1504, 0.1417, 0.2757, 0.1984, 0.2338],\n",
      "         [0.1734, 0.1648, 0.2915, 0.2199, 0.1505]],\n",
      "\n",
      "        [[0.2322, 0.1919, 0.1919, 0.1919, 0.1919],\n",
      "         [0.2126, 0.2418, 0.1818, 0.1818, 0.1818],\n",
      "         [0.1745, 0.1394, 0.2277, 0.2292, 0.2292],\n",
      "         [0.1854, 0.1610, 0.2192, 0.2141, 0.2202],\n",
      "         [0.1947, 0.1521, 0.2611, 0.2506, 0.1416]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1972, 0.2007, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1842, 0.1942, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1740, 0.1988, 0.1590, 0.2341, 0.2341],\n",
      "         [0.1934, 0.2056, 0.1855, 0.1939, 0.2216],\n",
      "         [0.2080, 0.1905, 0.2209, 0.2073, 0.1733]],\n",
      "\n",
      "        [[0.1725, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.2044, 0.2002, 0.1985, 0.1985, 0.1985],\n",
      "         [0.1978, 0.2128, 0.1503, 0.2195, 0.2195],\n",
      "         [0.2149, 0.2385, 0.1454, 0.1518, 0.2493],\n",
      "         [0.1688, 0.1513, 0.2542, 0.2431, 0.1826]],\n",
      "\n",
      "        [[0.1893, 0.2027, 0.2027, 0.2027, 0.2027],\n",
      "         [0.1829, 0.1751, 0.2140, 0.2140, 0.2140],\n",
      "         [0.1879, 0.1773, 0.1715, 0.2317, 0.2317],\n",
      "         [0.1858, 0.1751, 0.1693, 0.2395, 0.2303],\n",
      "         [0.2003, 0.2076, 0.2120, 0.1716, 0.2086]],\n",
      "\n",
      "        [[0.2503, 0.1874, 0.1874, 0.1874, 0.1874],\n",
      "         [0.1919, 0.1934, 0.2049, 0.2049, 0.2049],\n",
      "         [0.2058, 0.2043, 0.2012, 0.1943, 0.1943],\n",
      "         [0.1967, 0.1991, 0.2047, 0.1814, 0.2180],\n",
      "         [0.2002, 0.2009, 0.2026, 0.1957, 0.2006]],\n",
      "\n",
      "        [[0.2525, 0.1869, 0.1869, 0.1869, 0.1869],\n",
      "         [0.2283, 0.2394, 0.1774, 0.1774, 0.1774],\n",
      "         [0.1722, 0.1616, 0.1837, 0.2413, 0.2413],\n",
      "         [0.1961, 0.1914, 0.2010, 0.1886, 0.2229],\n",
      "         [0.1871, 0.2021, 0.1730, 0.2118, 0.2259]],\n",
      "\n",
      "        [[0.1907, 0.2023, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1556, 0.1986, 0.2153, 0.2153, 0.2153],\n",
      "         [0.1563, 0.2109, 0.1673, 0.2327, 0.2327],\n",
      "         [0.1785, 0.2151, 0.1862, 0.1913, 0.2288],\n",
      "         [0.2092, 0.1919, 0.2051, 0.2026, 0.1911]],\n",
      "\n",
      "        [[0.2025, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2037, 0.2146, 0.1939, 0.1939, 0.1939],\n",
      "         [0.2019, 0.2036, 0.1938, 0.2003, 0.2003],\n",
      "         [0.2158, 0.2222, 0.1868, 0.1653, 0.2099],\n",
      "         [0.1911, 0.1993, 0.1549, 0.1297, 0.3250]],\n",
      "\n",
      "        [[0.2029, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.2139, 0.2176, 0.1895, 0.1895, 0.1895],\n",
      "         [0.2117, 0.2192, 0.2367, 0.1662, 0.1662],\n",
      "         [0.1988, 0.1980, 0.1964, 0.2027, 0.2041],\n",
      "         [0.1867, 0.1955, 0.2166, 0.1473, 0.2539]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1978, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1941, 0.1997, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2054, 0.1976, 0.2083, 0.1943, 0.1943],\n",
      "         [0.1872, 0.1987, 0.1832, 0.2270, 0.2039],\n",
      "         [0.2049, 0.2007, 0.2065, 0.1917, 0.1961]],\n",
      "\n",
      "        [[0.2101, 0.1975, 0.1975, 0.1975, 0.1975],\n",
      "         [0.2323, 0.1616, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1647, 0.3004, 0.1199, 0.2075, 0.2075],\n",
      "         [0.2040, 0.1892, 0.2123, 0.1963, 0.1982],\n",
      "         [0.2067, 0.1745, 0.2260, 0.1895, 0.2033]],\n",
      "\n",
      "        [[0.2120, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.2129, 0.1953, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2099, 0.1980, 0.1935, 0.1993, 0.1993],\n",
      "         [0.2488, 0.1863, 0.1660, 0.2064, 0.1925],\n",
      "         [0.3154, 0.1881, 0.1531, 0.2258, 0.1176]],\n",
      "\n",
      "        [[0.1462, 0.2134, 0.2134, 0.2134, 0.2134],\n",
      "         [0.1739, 0.2201, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2829, 0.1491, 0.1915, 0.1882, 0.1882],\n",
      "         [0.1110, 0.3454, 0.2215, 0.0937, 0.2285],\n",
      "         [0.1458, 0.2281, 0.1914, 0.1364, 0.2984]],\n",
      "\n",
      "        [[0.1792, 0.2052, 0.2052, 0.2052, 0.2052],\n",
      "         [0.1767, 0.2179, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1949, 0.1996, 0.2098, 0.1979, 0.1979],\n",
      "         [0.2493, 0.2135, 0.1541, 0.1572, 0.2260],\n",
      "         [0.2141, 0.2032, 0.1819, 0.1832, 0.2176]],\n",
      "\n",
      "        [[0.1429, 0.2143, 0.2143, 0.2143, 0.2143],\n",
      "         [0.2184, 0.2002, 0.1938, 0.1938, 0.1938],\n",
      "         [0.1715, 0.1920, 0.2362, 0.2002, 0.2002],\n",
      "         [0.2410, 0.2151, 0.1745, 0.1632, 0.2062],\n",
      "         [0.1742, 0.1906, 0.2248, 0.2370, 0.1734]],\n",
      "\n",
      "        [[0.1994, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1983, 0.2077, 0.1980, 0.1980, 0.1980],\n",
      "         [0.2027, 0.2690, 0.1262, 0.2011, 0.2011],\n",
      "         [0.1966, 0.1364, 0.3634, 0.1048, 0.1988],\n",
      "         [0.1979, 0.1814, 0.2292, 0.1703, 0.2212]],\n",
      "\n",
      "        [[0.1950, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1976, 0.1900, 0.2041, 0.2041, 0.2041],\n",
      "         [0.2278, 0.3098, 0.1092, 0.1766, 0.1766],\n",
      "         [0.2554, 0.3572, 0.1145, 0.0797, 0.1934],\n",
      "         [0.2918, 0.4159, 0.1250, 0.0852, 0.0821]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2378, 0.1906, 0.1906, 0.1906, 0.1906],\n",
      "         [0.1491, 0.2449, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2400, 0.1863, 0.1626, 0.2056, 0.2056],\n",
      "         [0.2168, 0.1898, 0.1767, 0.2168, 0.1999],\n",
      "         [0.2311, 0.1789, 0.1559, 0.2311, 0.2030]],\n",
      "\n",
      "        [[0.1993, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2031, 0.1907, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2004, 0.2008, 0.1979, 0.2004, 0.2004],\n",
      "         [0.1663, 0.1509, 0.2822, 0.2354, 0.1651],\n",
      "         [0.1645, 0.1477, 0.2960, 0.2420, 0.1499]],\n",
      "\n",
      "        [[0.1864, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1738, 0.2770, 0.1831, 0.1831, 0.1831],\n",
      "         [0.2020, 0.1952, 0.2004, 0.2012, 0.2012],\n",
      "         [0.1425, 0.2571, 0.1626, 0.2857, 0.1522],\n",
      "         [0.1705, 0.2273, 0.1818, 0.2393, 0.1811]],\n",
      "\n",
      "        [[0.2676, 0.1831, 0.1831, 0.1831, 0.1831],\n",
      "         [0.2911, 0.2635, 0.1485, 0.1485, 0.1485],\n",
      "         [0.2818, 0.2579, 0.1509, 0.1547, 0.1547],\n",
      "         [0.1932, 0.1952, 0.2072, 0.1977, 0.2066],\n",
      "         [0.2982, 0.2618, 0.1194, 0.2209, 0.0997]],\n",
      "\n",
      "        [[0.4099, 0.1475, 0.1475, 0.1475, 0.1475],\n",
      "         [0.2852, 0.1835, 0.1771, 0.1771, 0.1771],\n",
      "         [0.1657, 0.2173, 0.1726, 0.2222, 0.2222],\n",
      "         [0.1761, 0.2245, 0.1827, 0.1878, 0.2289],\n",
      "         [0.2535, 0.0975, 0.2194, 0.1969, 0.2327]],\n",
      "\n",
      "        [[0.2547, 0.1863, 0.1863, 0.1863, 0.1863],\n",
      "         [0.1684, 0.1875, 0.2147, 0.2147, 0.2147],\n",
      "         [0.2747, 0.1968, 0.2695, 0.1295, 0.1295],\n",
      "         [0.2267, 0.1912, 0.2245, 0.2033, 0.1543],\n",
      "         [0.2212, 0.1984, 0.2198, 0.2064, 0.1542]],\n",
      "\n",
      "        [[0.2257, 0.1936, 0.1936, 0.1936, 0.1936],\n",
      "         [0.1735, 0.2078, 0.2062, 0.2062, 0.2062],\n",
      "         [0.2496, 0.1902, 0.1753, 0.1925, 0.1925],\n",
      "         [0.1800, 0.2046, 0.2127, 0.1993, 0.2035],\n",
      "         [0.2423, 0.1916, 0.1785, 0.2010, 0.1866]],\n",
      "\n",
      "        [[0.2387, 0.1903, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2374, 0.3508, 0.1373, 0.1373, 0.1373],\n",
      "         [0.1878, 0.2361, 0.3036, 0.1362, 0.1362],\n",
      "         [0.1939, 0.2058, 0.2197, 0.2024, 0.1783],\n",
      "         [0.1730, 0.1955, 0.2237, 0.1890, 0.2187]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2321, 0.1920, 0.1920, 0.1920, 0.1920],\n",
      "         [0.1103, 0.1274, 0.2541, 0.2541, 0.2541],\n",
      "         [0.1587, 0.1678, 0.2357, 0.2189, 0.2189],\n",
      "         [0.0651, 0.0848, 0.4285, 0.1207, 0.3010],\n",
      "         [0.3445, 0.2875, 0.0949, 0.2257, 0.0473]],\n",
      "\n",
      "        [[0.0635, 0.2341, 0.2341, 0.2341, 0.2341],\n",
      "         [0.1472, 0.1723, 0.2268, 0.2268, 0.2268],\n",
      "         [0.1077, 0.1428, 0.2822, 0.2336, 0.2336],\n",
      "         [0.0946, 0.1348, 0.3166, 0.2043, 0.2497],\n",
      "         [0.2681, 0.2263, 0.1502, 0.1853, 0.1702]],\n",
      "\n",
      "        [[0.1611, 0.2097, 0.2097, 0.2097, 0.2097],\n",
      "         [0.2034, 0.2112, 0.1951, 0.1951, 0.1951],\n",
      "         [0.1818, 0.1658, 0.2500, 0.2012, 0.2012],\n",
      "         [0.2053, 0.2088, 0.1938, 0.1905, 0.2016],\n",
      "         [0.1642, 0.1478, 0.2359, 0.2629, 0.1891]],\n",
      "\n",
      "        [[0.2023, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2065, 0.1617, 0.2106, 0.2106, 0.2106],\n",
      "         [0.2374, 0.1289, 0.1348, 0.2495, 0.2495],\n",
      "         [0.2192, 0.1136, 0.1191, 0.3170, 0.2312],\n",
      "         [0.1930, 0.2323, 0.2292, 0.1739, 0.1715]],\n",
      "\n",
      "        [[0.1821, 0.2045, 0.2045, 0.2045, 0.2045],\n",
      "         [0.1634, 0.1427, 0.2313, 0.2313, 0.2313],\n",
      "         [0.1884, 0.1773, 0.1942, 0.2200, 0.2200],\n",
      "         [0.1846, 0.1566, 0.2006, 0.1766, 0.2815],\n",
      "         [0.2177, 0.2591, 0.1994, 0.2282, 0.0955]],\n",
      "\n",
      "        [[0.1848, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1913, 0.1913, 0.2058, 0.2058, 0.2058],\n",
      "         [0.2457, 0.2460, 0.1529, 0.1777, 0.1777],\n",
      "         [0.1072, 0.1069, 0.3691, 0.1675, 0.2493],\n",
      "         [0.2506, 0.2509, 0.1398, 0.2030, 0.1557]],\n",
      "\n",
      "        [[0.2123, 0.1969, 0.1969, 0.1969, 0.1969],\n",
      "         [0.2253, 0.1754, 0.1998, 0.1998, 0.1998],\n",
      "         [0.2026, 0.1607, 0.2743, 0.1812, 0.1812],\n",
      "         [0.1740, 0.1037, 0.3419, 0.2448, 0.1357],\n",
      "         [0.2036, 0.2177, 0.1865, 0.1947, 0.1975]],\n",
      "\n",
      "        [[0.1035, 0.2241, 0.2241, 0.2241, 0.2241],\n",
      "         [0.1467, 0.2061, 0.2157, 0.2157, 0.2157],\n",
      "         [0.2829, 0.2030, 0.1258, 0.1942, 0.1942],\n",
      "         [0.0987, 0.1639, 0.3403, 0.2219, 0.1754],\n",
      "         [0.1852, 0.1962, 0.2132, 0.2031, 0.2023]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2116, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.2354, 0.1915, 0.1910, 0.1910, 0.1910],\n",
      "         [0.2244, 0.2058, 0.1587, 0.2056, 0.2056],\n",
      "         [0.2294, 0.2029, 0.1402, 0.2249, 0.2026],\n",
      "         [0.1181, 0.1522, 0.3266, 0.1230, 0.2801]],\n",
      "\n",
      "        [[0.3380, 0.1655, 0.1655, 0.1655, 0.1655],\n",
      "         [0.2785, 0.1786, 0.1810, 0.1810, 0.1810],\n",
      "         [0.1492, 0.2027, 0.2464, 0.2009, 0.2009],\n",
      "         [0.4707, 0.1602, 0.0804, 0.1234, 0.1653],\n",
      "         [0.1296, 0.1927, 0.2484, 0.2122, 0.2171]],\n",
      "\n",
      "        [[0.3890, 0.1528, 0.1528, 0.1528, 0.1528],\n",
      "         [0.3340, 0.1134, 0.1842, 0.1842, 0.1842],\n",
      "         [0.1858, 0.2082, 0.2103, 0.1978, 0.1978],\n",
      "         [0.2615, 0.1796, 0.1739, 0.1724, 0.2126],\n",
      "         [0.0730, 0.2332, 0.2575, 0.2648, 0.1715]],\n",
      "\n",
      "        [[0.2024, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1669, 0.1944, 0.2129, 0.2129, 0.2129],\n",
      "         [0.1950, 0.1978, 0.2082, 0.1995, 0.1995],\n",
      "         [0.2223, 0.1806, 0.0867, 0.3508, 0.1596],\n",
      "         [0.1745, 0.2026, 0.3430, 0.1259, 0.1540]],\n",
      "\n",
      "        [[0.3404, 0.1649, 0.1649, 0.1649, 0.1649],\n",
      "         [0.2568, 0.2027, 0.1802, 0.1802, 0.1802],\n",
      "         [0.1965, 0.1992, 0.2031, 0.2006, 0.2006],\n",
      "         [0.1744, 0.1958, 0.2308, 0.1914, 0.2075],\n",
      "         [0.3039, 0.2206, 0.1402, 0.2350, 0.1002]],\n",
      "\n",
      "        [[0.1971, 0.2007, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1737, 0.1979, 0.2095, 0.2095, 0.2095],\n",
      "         [0.1979, 0.2208, 0.1180, 0.2316, 0.2316],\n",
      "         [0.1920, 0.2172, 0.1075, 0.2542, 0.2291],\n",
      "         [0.1850, 0.1690, 0.2830, 0.1506, 0.2124]],\n",
      "\n",
      "        [[0.1820, 0.2045, 0.2045, 0.2045, 0.2045],\n",
      "         [0.2032, 0.1140, 0.2276, 0.2276, 0.2276],\n",
      "         [0.2029, 0.1198, 0.2272, 0.2250, 0.2250],\n",
      "         [0.1724, 0.2481, 0.1595, 0.2594, 0.1605],\n",
      "         [0.1781, 0.2098, 0.1719, 0.2141, 0.2262]],\n",
      "\n",
      "        [[0.2445, 0.1889, 0.1889, 0.1889, 0.1889],\n",
      "         [0.2821, 0.1508, 0.1890, 0.1890, 0.1890],\n",
      "         [0.1653, 0.2013, 0.2584, 0.1875, 0.1875],\n",
      "         [0.1362, 0.1832, 0.2667, 0.2494, 0.1646],\n",
      "         [0.1271, 0.1692, 0.2431, 0.2278, 0.2328]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2031, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1990, 0.2011, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2214, 0.2578, 0.0456, 0.2376, 0.2376],\n",
      "         [0.0618, 0.0484, 0.7794, 0.0552, 0.0552],\n",
      "         [0.2227, 0.2397, 0.1036, 0.2303, 0.2037]],\n",
      "\n",
      "        [[0.1890, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1163, 0.3878, 0.1653, 0.1653, 0.1653],\n",
      "         [0.2016, 0.1962, 0.2022, 0.2000, 0.2000],\n",
      "         [0.1473, 0.2940, 0.1360, 0.2426, 0.1802],\n",
      "         [0.1180, 0.2520, 0.1081, 0.2041, 0.3178]],\n",
      "\n",
      "        [[0.2358, 0.1910, 0.1910, 0.1910, 0.1910],\n",
      "         [0.3348, 0.2268, 0.1461, 0.1461, 0.1461],\n",
      "         [0.1580, 0.1914, 0.1752, 0.2377, 0.2377],\n",
      "         [0.2386, 0.2054, 0.2201, 0.1626, 0.1733],\n",
      "         [0.2615, 0.1878, 0.2188, 0.1123, 0.2195]],\n",
      "\n",
      "        [[0.0971, 0.2257, 0.2257, 0.2257, 0.2257],\n",
      "         [0.1052, 0.2028, 0.2307, 0.2307, 0.2307],\n",
      "         [0.0768, 0.1478, 0.4392, 0.1681, 0.1681],\n",
      "         [0.1083, 0.1705, 0.3620, 0.1729, 0.1863],\n",
      "         [0.3466, 0.2278, 0.1134, 0.2249, 0.0872]],\n",
      "\n",
      "        [[0.2323, 0.1919, 0.1919, 0.1919, 0.1919],\n",
      "         [0.1419, 0.5057, 0.1175, 0.1175, 0.1175],\n",
      "         [0.1979, 0.2853, 0.1419, 0.1874, 0.1874],\n",
      "         [0.2073, 0.2843, 0.1555, 0.1552, 0.1977],\n",
      "         [0.1948, 0.3151, 0.1258, 0.1255, 0.2388]],\n",
      "\n",
      "        [[0.1270, 0.2183, 0.2183, 0.2183, 0.2183],\n",
      "         [0.1872, 0.1734, 0.2131, 0.2131, 0.2131],\n",
      "         [0.2150, 0.2351, 0.1805, 0.1847, 0.1847],\n",
      "         [0.1755, 0.1552, 0.2232, 0.2298, 0.2162],\n",
      "         [0.2099, 0.2168, 0.1969, 0.1954, 0.1810]],\n",
      "\n",
      "        [[0.1254, 0.2187, 0.2187, 0.2187, 0.2187],\n",
      "         [0.1771, 0.2070, 0.2053, 0.2053, 0.2053],\n",
      "         [0.4140, 0.1586, 0.0934, 0.1670, 0.1670],\n",
      "         [0.1225, 0.2068, 0.2762, 0.1935, 0.2011],\n",
      "         [0.4874, 0.1469, 0.0757, 0.1710, 0.1190]],\n",
      "\n",
      "        [[0.1962, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.1948, 0.2075, 0.1993, 0.1993, 0.1993],\n",
      "         [0.2123, 0.2168, 0.1432, 0.2139, 0.2139],\n",
      "         [0.2696, 0.2882, 0.0777, 0.0883, 0.2762],\n",
      "         [0.3235, 0.3447, 0.0992, 0.1119, 0.1208]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2266, 0.1934, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2020, 0.3763, 0.1406, 0.1406, 0.1406],\n",
      "         [0.2003, 0.2056, 0.1994, 0.1973, 0.1973],\n",
      "         [0.1966, 0.1708, 0.2015, 0.2176, 0.2135],\n",
      "         [0.2075, 0.2669, 0.1987, 0.1730, 0.1539]],\n",
      "\n",
      "        [[0.2001, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1966, 0.2138, 0.1965, 0.1965, 0.1965],\n",
      "         [0.1896, 0.2036, 0.2278, 0.1895, 0.1895],\n",
      "         [0.2037, 0.2001, 0.1946, 0.1978, 0.2037],\n",
      "         [0.2025, 0.1987, 0.1928, 0.1962, 0.2098]],\n",
      "\n",
      "        [[0.3018, 0.1745, 0.1745, 0.1745, 0.1745],\n",
      "         [0.2705, 0.1668, 0.1876, 0.1876, 0.1876],\n",
      "         [0.1181, 0.2314, 0.2573, 0.1966, 0.1966],\n",
      "         [0.2825, 0.1616, 0.1480, 0.2228, 0.1851],\n",
      "         [0.2864, 0.1204, 0.1050, 0.1982, 0.2900]],\n",
      "\n",
      "        [[0.2078, 0.1981, 0.1981, 0.1981, 0.1981],\n",
      "         [0.2011, 0.1985, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2031, 0.1945, 0.2026, 0.1999, 0.1999],\n",
      "         [0.2125, 0.1886, 0.2112, 0.1844, 0.2033],\n",
      "         [0.2019, 0.1981, 0.2017, 0.1974, 0.2008]],\n",
      "\n",
      "        [[0.2018, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1991, 0.2007, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1692, 0.2096, 0.2336, 0.1937, 0.1937],\n",
      "         [0.1675, 0.2112, 0.2375, 0.1898, 0.1939],\n",
      "         [0.1989, 0.2002, 0.2009, 0.1996, 0.2005]],\n",
      "\n",
      "        [[0.2134, 0.1967, 0.1967, 0.1967, 0.1967],\n",
      "         [0.1690, 0.2799, 0.1837, 0.1837, 0.1837],\n",
      "         [0.2088, 0.1835, 0.1991, 0.2044, 0.2044],\n",
      "         [0.1318, 0.2430, 0.1652, 0.3141, 0.1459],\n",
      "         [0.2087, 0.1595, 0.1890, 0.1425, 0.3004]],\n",
      "\n",
      "        [[0.1980, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2055, 0.2050, 0.1965, 0.1965, 0.1965],\n",
      "         [0.1744, 0.1758, 0.2458, 0.2020, 0.2020],\n",
      "         [0.2162, 0.2151, 0.1747, 0.1967, 0.1973],\n",
      "         [0.2004, 0.2003, 0.1939, 0.1975, 0.2078]],\n",
      "\n",
      "        [[0.2329, 0.1918, 0.1918, 0.1918, 0.1918],\n",
      "         [0.2165, 0.2151, 0.1895, 0.1895, 0.1895],\n",
      "         [0.2030, 0.2027, 0.2002, 0.1971, 0.1971],\n",
      "         [0.1420, 0.1454, 0.1770, 0.3084, 0.2271],\n",
      "         [0.1981, 0.1983, 0.1997, 0.2038, 0.2000]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2761, 0.1810, 0.1810, 0.1810, 0.1810],\n",
      "         [0.1755, 0.2158, 0.2029, 0.2029, 0.2029],\n",
      "         [0.2442, 0.1624, 0.2265, 0.1834, 0.1834],\n",
      "         [0.2122, 0.1882, 0.2075, 0.1971, 0.1950],\n",
      "         [0.1995, 0.2017, 0.1999, 0.2008, 0.1982]],\n",
      "\n",
      "        [[0.1723, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1771, 0.2323, 0.1969, 0.1969, 0.1969],\n",
      "         [0.1478, 0.2523, 0.2359, 0.1820, 0.1820],\n",
      "         [0.1150, 0.2938, 0.2611, 0.1644, 0.1657],\n",
      "         [0.1895, 0.2072, 0.2049, 0.1961, 0.2022]],\n",
      "\n",
      "        [[0.2790, 0.1802, 0.1802, 0.1802, 0.1802],\n",
      "         [0.2565, 0.1957, 0.1826, 0.1826, 0.1826],\n",
      "         [0.2064, 0.2005, 0.1952, 0.1990, 0.1990],\n",
      "         [0.2591, 0.2005, 0.1588, 0.1936, 0.1879],\n",
      "         [0.1945, 0.1999, 0.2050, 0.2006, 0.2000]],\n",
      "\n",
      "        [[0.2040, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1462, 0.3041, 0.1832, 0.1832, 0.1832],\n",
      "         [0.1481, 0.2242, 0.2910, 0.1683, 0.1683],\n",
      "         [0.1936, 0.2012, 0.2061, 0.2032, 0.1959],\n",
      "         [0.1155, 0.2023, 0.2879, 0.2331, 0.1612]],\n",
      "\n",
      "        [[0.2133, 0.1967, 0.1967, 0.1967, 0.1967],\n",
      "         [0.2001, 0.1982, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2041, 0.2350, 0.1676, 0.1967, 0.1967],\n",
      "         [0.2076, 0.2245, 0.1861, 0.1784, 0.2034],\n",
      "         [0.2120, 0.2623, 0.1575, 0.1406, 0.2275]],\n",
      "\n",
      "        [[0.2280, 0.1930, 0.1930, 0.1930, 0.1930],\n",
      "         [0.2128, 0.1858, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2845, 0.1298, 0.1834, 0.2012, 0.2012],\n",
      "         [0.2439, 0.1489, 0.1851, 0.2259, 0.1962],\n",
      "         [0.2004, 0.1994, 0.1999, 0.2003, 0.2000]],\n",
      "\n",
      "        [[0.1718, 0.2071, 0.2071, 0.2071, 0.2071],\n",
      "         [0.1748, 0.1647, 0.2202, 0.2202, 0.2202],\n",
      "         [0.1962, 0.1943, 0.2025, 0.2035, 0.2035],\n",
      "         [0.1830, 0.1778, 0.2016, 0.2328, 0.2047],\n",
      "         [0.1233, 0.1069, 0.1992, 0.4063, 0.1643]],\n",
      "\n",
      "        [[0.2593, 0.1852, 0.1852, 0.1852, 0.1852],\n",
      "         [0.2528, 0.2245, 0.1742, 0.1742, 0.1742],\n",
      "         [0.2078, 0.2006, 0.2193, 0.1862, 0.1862],\n",
      "         [0.1974, 0.2009, 0.1921, 0.2009, 0.2086],\n",
      "         [0.1973, 0.2027, 0.1893, 0.2028, 0.2080]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2295, 0.1926, 0.1926, 0.1926, 0.1926],\n",
      "         [0.2033, 0.2003, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1786, 0.1989, 0.2024, 0.2101, 0.2101],\n",
      "         [0.1993, 0.1955, 0.1949, 0.2168, 0.1936],\n",
      "         [0.1926, 0.1973, 0.1981, 0.1730, 0.2390]],\n",
      "\n",
      "        [[0.4264, 0.1434, 0.1434, 0.1434, 0.1434],\n",
      "         [0.1889, 0.2008, 0.2034, 0.2034, 0.2034],\n",
      "         [0.2110, 0.1972, 0.2031, 0.1944, 0.1944],\n",
      "         [0.1923, 0.2052, 0.1995, 0.1950, 0.2081],\n",
      "         [0.2126, 0.1985, 0.2045, 0.2095, 0.1749]],\n",
      "\n",
      "        [[0.2343, 0.1914, 0.1914, 0.1914, 0.1914],\n",
      "         [0.2936, 0.1588, 0.1825, 0.1825, 0.1825],\n",
      "         [0.1015, 0.2339, 0.2775, 0.1935, 0.1935],\n",
      "         [0.1324, 0.2127, 0.2344, 0.2294, 0.1910],\n",
      "         [0.3666, 0.1869, 0.1629, 0.1679, 0.1156]],\n",
      "\n",
      "        [[0.0475, 0.2381, 0.2381, 0.2381, 0.2381],\n",
      "         [0.1104, 0.2540, 0.2119, 0.2119, 0.2119],\n",
      "         [0.2231, 0.1944, 0.1818, 0.2003, 0.2003],\n",
      "         [0.2935, 0.1749, 0.1362, 0.1996, 0.1958],\n",
      "         [0.3191, 0.1489, 0.1030, 0.1808, 0.2483]],\n",
      "\n",
      "        [[0.2287, 0.1928, 0.1928, 0.1928, 0.1928],\n",
      "         [0.1959, 0.1951, 0.2030, 0.2030, 0.2030],\n",
      "         [0.2324, 0.2387, 0.1631, 0.1829, 0.1829],\n",
      "         [0.2781, 0.2901, 0.1591, 0.0822, 0.1906],\n",
      "         [0.2065, 0.2069, 0.2010, 0.1948, 0.1908]],\n",
      "\n",
      "        [[0.1642, 0.2089, 0.2089, 0.2089, 0.2089],\n",
      "         [0.1650, 0.2701, 0.1883, 0.1883, 0.1883],\n",
      "         [0.1955, 0.2071, 0.2003, 0.1985, 0.1985],\n",
      "         [0.2323, 0.1827, 0.2102, 0.1571, 0.2178],\n",
      "         [0.1727, 0.2420, 0.1988, 0.2991, 0.0874]],\n",
      "\n",
      "        [[0.1960, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.2006, 0.1986, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2430, 0.1676, 0.1306, 0.2294, 0.2294],\n",
      "         [0.2239, 0.1964, 0.1798, 0.1805, 0.2194],\n",
      "         [0.2018, 0.1947, 0.1900, 0.1902, 0.2233]],\n",
      "\n",
      "        [[0.2769, 0.1808, 0.1808, 0.1808, 0.1808],\n",
      "         [0.1838, 0.1980, 0.2061, 0.2061, 0.2061],\n",
      "         [0.1837, 0.1974, 0.2084, 0.2052, 0.2052],\n",
      "         [0.2453, 0.2099, 0.1867, 0.1652, 0.1930],\n",
      "         [0.3337, 0.2415, 0.1893, 0.1470, 0.0885]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1659, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.2003, 0.2000, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1710, 0.2031, 0.2082, 0.2089, 0.2089],\n",
      "         [0.2570, 0.2038, 0.1970, 0.1461, 0.1962],\n",
      "         [0.1311, 0.1618, 0.1668, 0.2188, 0.3214]],\n",
      "\n",
      "        [[0.1990, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2037, 0.1958, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1624, 0.2115, 0.2613, 0.1824, 0.1824],\n",
      "         [0.2077, 0.1914, 0.1793, 0.2212, 0.2004],\n",
      "         [0.2077, 0.1930, 0.1819, 0.2198, 0.1976]],\n",
      "\n",
      "        [[0.1876, 0.2031, 0.2031, 0.2031, 0.2031],\n",
      "         [0.2025, 0.2039, 0.1979, 0.1979, 0.1979],\n",
      "         [0.2001, 0.2002, 0.2000, 0.1998, 0.1998],\n",
      "         [0.1980, 0.1955, 0.2029, 0.1971, 0.2064],\n",
      "         [0.2123, 0.2211, 0.1964, 0.2156, 0.1546]],\n",
      "\n",
      "        [[0.2228, 0.1943, 0.1943, 0.1943, 0.1943],\n",
      "         [0.1895, 0.1978, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2083, 0.2044, 0.1840, 0.2016, 0.2016],\n",
      "         [0.2038, 0.2027, 0.1966, 0.1951, 0.2019],\n",
      "         [0.1140, 0.1321, 0.3001, 0.3697, 0.0841]],\n",
      "\n",
      "        [[0.2735, 0.1816, 0.1816, 0.1816, 0.1816],\n",
      "         [0.3191, 0.1418, 0.1797, 0.1797, 0.1797],\n",
      "         [0.2325, 0.1799, 0.1998, 0.1939, 0.1939],\n",
      "         [0.2459, 0.1625, 0.1924, 0.2158, 0.1834],\n",
      "         [0.1856, 0.2246, 0.2078, 0.1972, 0.1847]],\n",
      "\n",
      "        [[0.2046, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.2047, 0.1931, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2603, 0.1729, 0.1122, 0.2273, 0.2273],\n",
      "         [0.2065, 0.1997, 0.1928, 0.1967, 0.2043],\n",
      "         [0.2204, 0.1957, 0.1726, 0.1852, 0.2262]],\n",
      "\n",
      "        [[0.2133, 0.1967, 0.1967, 0.1967, 0.1967],\n",
      "         [0.2428, 0.2092, 0.1827, 0.1827, 0.1827],\n",
      "         [0.2052, 0.2005, 0.2017, 0.1963, 0.1963],\n",
      "         [0.1769, 0.1884, 0.1855, 0.2497, 0.1995],\n",
      "         [0.0964, 0.1310, 0.1214, 0.5160, 0.1351]],\n",
      "\n",
      "        [[0.2001, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1946, 0.2180, 0.1958, 0.1958, 0.1958],\n",
      "         [0.1987, 0.1393, 0.2721, 0.1949, 0.1949],\n",
      "         [0.2066, 0.2514, 0.1737, 0.1595, 0.2088],\n",
      "         [0.2088, 0.2510, 0.1773, 0.1637, 0.1992]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2493, 0.1877, 0.1877, 0.1877, 0.1877],\n",
      "         [0.2674, 0.1188, 0.2046, 0.2046, 0.2046],\n",
      "         [0.2163, 0.1702, 0.2138, 0.1999, 0.1999],\n",
      "         [0.1577, 0.2690, 0.1619, 0.2234, 0.1881],\n",
      "         [0.2078, 0.1672, 0.2056, 0.1803, 0.2391]],\n",
      "\n",
      "        [[0.0778, 0.2305, 0.2305, 0.2305, 0.2305],\n",
      "         [0.1965, 0.2007, 0.2009, 0.2009, 0.2009],\n",
      "         [0.1576, 0.2003, 0.2377, 0.2022, 0.2022],\n",
      "         [0.3009, 0.1987, 0.1477, 0.1573, 0.1954],\n",
      "         [0.3344, 0.2141, 0.1557, 0.1666, 0.1293]],\n",
      "\n",
      "        [[0.1639, 0.2090, 0.2090, 0.2090, 0.2090],\n",
      "         [0.2382, 0.1468, 0.2050, 0.2050, 0.2050],\n",
      "         [0.3051, 0.1131, 0.1332, 0.2243, 0.2243],\n",
      "         [0.3763, 0.1143, 0.1390, 0.1102, 0.2601],\n",
      "         [0.2499, 0.1806, 0.1905, 0.1788, 0.2001]],\n",
      "\n",
      "        [[0.1455, 0.2136, 0.2136, 0.2136, 0.2136],\n",
      "         [0.2007, 0.2001, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2321, 0.2069, 0.1781, 0.1915, 0.1915],\n",
      "         [0.1709, 0.1946, 0.2306, 0.1914, 0.2125],\n",
      "         [0.2006, 0.1999, 0.1990, 0.2000, 0.2004]],\n",
      "\n",
      "        [[0.1513, 0.2122, 0.2122, 0.2122, 0.2122],\n",
      "         [0.1668, 0.2075, 0.2086, 0.2086, 0.2086],\n",
      "         [0.1931, 0.2051, 0.1912, 0.2053, 0.2053],\n",
      "         [0.1981, 0.2018, 0.1975, 0.2007, 0.2019],\n",
      "         [0.1954, 0.2886, 0.1832, 0.2582, 0.0747]],\n",
      "\n",
      "        [[0.2572, 0.1857, 0.1857, 0.1857, 0.1857],\n",
      "         [0.2582, 0.1755, 0.1888, 0.1888, 0.1888],\n",
      "         [0.1949, 0.2129, 0.1734, 0.2094, 0.2094],\n",
      "         [0.1798, 0.2399, 0.1230, 0.2300, 0.2272],\n",
      "         [0.1979, 0.1637, 0.2543, 0.1682, 0.2159]],\n",
      "\n",
      "        [[0.2001, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2003, 0.1997, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1898, 0.2079, 0.2046, 0.1988, 0.1988],\n",
      "         [0.2098, 0.1994, 0.2012, 0.1852, 0.2044],\n",
      "         [0.1996, 0.1956, 0.1963, 0.1900, 0.2186]],\n",
      "\n",
      "        [[0.1926, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2413, 0.1255, 0.2111, 0.2111, 0.2111],\n",
      "         [0.2355, 0.1795, 0.1395, 0.2228, 0.2228],\n",
      "         [0.2115, 0.1966, 0.1836, 0.2000, 0.2084],\n",
      "         [0.3223, 0.1970, 0.1246, 0.2210, 0.1351]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2030, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.2162, 0.0783, 0.2352, 0.2352, 0.2352],\n",
      "         [0.1855, 0.1126, 0.3150, 0.1934, 0.1934],\n",
      "         [0.2047, 0.1254, 0.3444, 0.1123, 0.2132],\n",
      "         [0.1587, 0.3359, 0.0716, 0.3974, 0.0365]],\n",
      "\n",
      "        [[0.1518, 0.2120, 0.2120, 0.2120, 0.2120],\n",
      "         [0.1227, 0.1905, 0.2289, 0.2289, 0.2289],\n",
      "         [0.2082, 0.2016, 0.1923, 0.1989, 0.1989],\n",
      "         [0.1267, 0.1797, 0.3011, 0.1845, 0.2080],\n",
      "         [0.3154, 0.2086, 0.1133, 0.2022, 0.1604]],\n",
      "\n",
      "        [[0.1874, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1607, 0.2092, 0.2100, 0.2100, 0.2100],\n",
      "         [0.1734, 0.2006, 0.2238, 0.2011, 0.2011],\n",
      "         [0.1412, 0.1939, 0.2458, 0.2242, 0.1949],\n",
      "         [0.2107, 0.1997, 0.1918, 0.1948, 0.2029]],\n",
      "\n",
      "        [[0.2005, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1638, 0.1878, 0.2161, 0.2161, 0.2161],\n",
      "         [0.2320, 0.2070, 0.1926, 0.1842, 0.1842],\n",
      "         [0.1498, 0.1737, 0.1908, 0.2837, 0.2020],\n",
      "         [0.1409, 0.1655, 0.1834, 0.2830, 0.2273]],\n",
      "\n",
      "        [[0.2422, 0.1894, 0.1894, 0.1894, 0.1894],\n",
      "         [0.1686, 0.1874, 0.2147, 0.2147, 0.2147],\n",
      "         [0.2217, 0.2069, 0.1929, 0.1893, 0.1893],\n",
      "         [0.2176, 0.2011, 0.1856, 0.2141, 0.1816],\n",
      "         [0.2113, 0.2018, 0.1925, 0.2094, 0.1850]],\n",
      "\n",
      "        [[0.1923, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1820, 0.1548, 0.2210, 0.2210, 0.2210],\n",
      "         [0.1944, 0.1883, 0.2134, 0.2019, 0.2019],\n",
      "         [0.1770, 0.1463, 0.3102, 0.1441, 0.2224],\n",
      "         [0.2133, 0.2436, 0.1440, 0.2463, 0.1528]],\n",
      "\n",
      "        [[0.1000, 0.2250, 0.2250, 0.2250, 0.2250],\n",
      "         [0.0833, 0.1454, 0.2571, 0.2571, 0.2571],\n",
      "         [0.1618, 0.1878, 0.2131, 0.2187, 0.2187],\n",
      "         [0.0857, 0.1362, 0.2020, 0.3574, 0.2188],\n",
      "         [0.1249, 0.1584, 0.1938, 0.2596, 0.2633]],\n",
      "\n",
      "        [[0.1532, 0.2117, 0.2117, 0.2117, 0.2117],\n",
      "         [0.1530, 0.1679, 0.2264, 0.2264, 0.2264],\n",
      "         [0.2640, 0.2446, 0.1087, 0.1914, 0.1914],\n",
      "         [0.1296, 0.1412, 0.3501, 0.1932, 0.1859],\n",
      "         [0.2062, 0.2047, 0.1899, 0.1995, 0.1997]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2040, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1619, 0.1501, 0.2293, 0.2293, 0.2293],\n",
      "         [0.2141, 0.2206, 0.1922, 0.1865, 0.1865],\n",
      "         [0.1772, 0.1688, 0.2114, 0.2206, 0.2220],\n",
      "         [0.2631, 0.2871, 0.1918, 0.1777, 0.0803]],\n",
      "\n",
      "        [[0.1222, 0.2195, 0.2195, 0.2195, 0.2195],\n",
      "         [0.1279, 0.2377, 0.2115, 0.2115, 0.2115],\n",
      "         [0.3592, 0.1134, 0.2454, 0.1410, 0.1410],\n",
      "         [0.1938, 0.2052, 0.1975, 0.2005, 0.2030],\n",
      "         [0.1894, 0.2027, 0.1937, 0.1972, 0.2170]],\n",
      "\n",
      "        [[0.2012, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1972, 0.1997, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1453, 0.1904, 0.2246, 0.2198, 0.2198],\n",
      "         [0.1568, 0.1956, 0.2239, 0.2038, 0.2200],\n",
      "         [0.1884, 0.2102, 0.2248, 0.2146, 0.1620]],\n",
      "\n",
      "        [[0.1669, 0.2083, 0.2083, 0.2083, 0.2083],\n",
      "         [0.1765, 0.2214, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2139, 0.1812, 0.2155, 0.1947, 0.1947],\n",
      "         [0.1030, 0.3613, 0.0974, 0.2286, 0.2098],\n",
      "         [0.1645, 0.2653, 0.1610, 0.2228, 0.1864]],\n",
      "\n",
      "        [[0.2326, 0.1918, 0.1918, 0.1918, 0.1918],\n",
      "         [0.2337, 0.2612, 0.1684, 0.1684, 0.1684],\n",
      "         [0.1800, 0.1717, 0.2344, 0.2070, 0.2070],\n",
      "         [0.1563, 0.1444, 0.2431, 0.2589, 0.1974],\n",
      "         [0.1647, 0.1515, 0.2617, 0.2795, 0.1426]],\n",
      "\n",
      "        [[0.1981, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2243, 0.1531, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1723, 0.2463, 0.2109, 0.1853, 0.1853],\n",
      "         [0.1944, 0.2020, 0.1987, 0.2090, 0.1959],\n",
      "         [0.1979, 0.2032, 0.2009, 0.2079, 0.1901]],\n",
      "\n",
      "        [[0.2038, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1939, 0.1892, 0.2056, 0.2056, 0.2056],\n",
      "         [0.1999, 0.1999, 0.2001, 0.2000, 0.2000],\n",
      "         [0.2107, 0.2185, 0.1584, 0.2193, 0.1932],\n",
      "         [0.1972, 0.2022, 0.1611, 0.2028, 0.2367]],\n",
      "\n",
      "        [[0.1832, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2069, 0.1982, 0.1983, 0.1983, 0.1983],\n",
      "         [0.3192, 0.1975, 0.0858, 0.1987, 0.1987],\n",
      "         [0.2603, 0.2125, 0.1495, 0.1646, 0.2131],\n",
      "         [0.3349, 0.2359, 0.1283, 0.1515, 0.1494]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1981, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.3289, 0.0775, 0.1978, 0.1978, 0.1978],\n",
      "         [0.1905, 0.2232, 0.1835, 0.2014, 0.2014],\n",
      "         [0.2252, 0.0443, 0.3323, 0.2711, 0.1271],\n",
      "         [0.1602, 0.3709, 0.1310, 0.1455, 0.1923]],\n",
      "\n",
      "        [[0.1593, 0.2102, 0.2102, 0.2102, 0.2102],\n",
      "         [0.1579, 0.2386, 0.2012, 0.2012, 0.2012],\n",
      "         [0.0855, 0.2226, 0.3922, 0.1499, 0.1499],\n",
      "         [0.0971, 0.1857, 0.2726, 0.3026, 0.1420],\n",
      "         [0.4501, 0.2007, 0.1245, 0.1093, 0.1155]],\n",
      "\n",
      "        [[0.1767, 0.2058, 0.2058, 0.2058, 0.2058],\n",
      "         [0.1989, 0.1882, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1877, 0.2647, 0.2297, 0.1589, 0.1589],\n",
      "         [0.2008, 0.1976, 0.1989, 0.2002, 0.2024],\n",
      "         [0.2644, 0.1405, 0.1823, 0.2352, 0.1776]],\n",
      "\n",
      "        [[0.1926, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1805, 0.3241, 0.1651, 0.1651, 0.1651],\n",
      "         [0.1648, 0.2777, 0.2531, 0.1522, 0.1522],\n",
      "         [0.1126, 0.2363, 0.2072, 0.3433, 0.1006],\n",
      "         [0.2664, 0.1749, 0.1885, 0.1416, 0.2286]],\n",
      "\n",
      "        [[0.2150, 0.1962, 0.1962, 0.1962, 0.1962],\n",
      "         [0.2133, 0.2764, 0.1701, 0.1701, 0.1701],\n",
      "         [0.2053, 0.2824, 0.2016, 0.1553, 0.1553],\n",
      "         [0.1999, 0.2332, 0.1981, 0.1942, 0.1746],\n",
      "         [0.2087, 0.1690, 0.2112, 0.2171, 0.1941]],\n",
      "\n",
      "        [[0.1800, 0.2050, 0.2050, 0.2050, 0.2050],\n",
      "         [0.2999, 0.0989, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1910, 0.2209, 0.1853, 0.2014, 0.2014],\n",
      "         [0.2393, 0.0292, 0.3688, 0.2514, 0.1113],\n",
      "         [0.0906, 0.6443, 0.0605, 0.0865, 0.1179]],\n",
      "\n",
      "        [[0.1601, 0.2100, 0.2100, 0.2100, 0.2100],\n",
      "         [0.1722, 0.2009, 0.2090, 0.2090, 0.2090],\n",
      "         [0.1260, 0.2005, 0.2221, 0.2257, 0.2257],\n",
      "         [0.1141, 0.1805, 0.1996, 0.3030, 0.2029],\n",
      "         [0.2811, 0.2082, 0.1950, 0.1484, 0.1673]],\n",
      "\n",
      "        [[0.2780, 0.1805, 0.1805, 0.1805, 0.1805],\n",
      "         [0.2400, 0.2662, 0.1646, 0.1646, 0.1646],\n",
      "         [0.2176, 0.2342, 0.2148, 0.1667, 0.1667],\n",
      "         [0.2059, 0.2115, 0.2049, 0.1907, 0.1870],\n",
      "         [0.1988, 0.2052, 0.1977, 0.1815, 0.2167]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2231, 0.1942, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2604, 0.1529, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2359, 0.1800, 0.1763, 0.2039, 0.2039],\n",
      "         [0.4052, 0.0793, 0.0700, 0.2769, 0.1686],\n",
      "         [0.1921, 0.2038, 0.2047, 0.1948, 0.2046]],\n",
      "\n",
      "        [[0.1780, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.3108, 0.1862, 0.1677, 0.1677, 0.1677],\n",
      "         [0.2154, 0.1992, 0.1933, 0.1960, 0.1960],\n",
      "         [0.2095, 0.1964, 0.1915, 0.2089, 0.1938],\n",
      "         [0.2312, 0.1982, 0.1867, 0.2296, 0.1543]],\n",
      "\n",
      "        [[0.2301, 0.1925, 0.1925, 0.1925, 0.1925],\n",
      "         [0.1979, 0.2920, 0.1700, 0.1700, 0.1700],\n",
      "         [0.1770, 0.1385, 0.2948, 0.1948, 0.1948],\n",
      "         [0.2093, 0.2791, 0.1151, 0.2094, 0.1871],\n",
      "         [0.2234, 0.3466, 0.0896, 0.2235, 0.1169]],\n",
      "\n",
      "        [[0.2134, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.1596, 0.4397, 0.1336, 0.1336, 0.1336],\n",
      "         [0.1570, 0.4481, 0.1336, 0.1306, 0.1306],\n",
      "         [0.1684, 0.4704, 0.1437, 0.0768, 0.1407],\n",
      "         [0.2083, 0.3244, 0.1945, 0.1484, 0.1243]],\n",
      "\n",
      "        [[0.2569, 0.1858, 0.1858, 0.1858, 0.1858],\n",
      "         [0.3890, 0.1319, 0.1597, 0.1597, 0.1597],\n",
      "         [0.1183, 0.2611, 0.1664, 0.2271, 0.2271],\n",
      "         [0.1272, 0.2530, 0.1711, 0.2246, 0.2241],\n",
      "         [0.2525, 0.1751, 0.2157, 0.1866, 0.1701]],\n",
      "\n",
      "        [[0.4040, 0.1490, 0.1490, 0.1490, 0.1490],\n",
      "         [0.1726, 0.1863, 0.2137, 0.2137, 0.2137],\n",
      "         [0.1785, 0.1922, 0.1904, 0.2194, 0.2194],\n",
      "         [0.2232, 0.2032, 0.2057, 0.1964, 0.1716],\n",
      "         [0.2136, 0.2052, 0.2063, 0.2023, 0.1727]],\n",
      "\n",
      "        [[0.2168, 0.1958, 0.1958, 0.1958, 0.1958],\n",
      "         [0.2028, 0.1998, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1491, 0.1797, 0.2976, 0.1868, 0.1868],\n",
      "         [0.4078, 0.2333, 0.0516, 0.0995, 0.2079],\n",
      "         [0.2121, 0.2052, 0.1877, 0.1951, 0.1999]],\n",
      "\n",
      "        [[0.3986, 0.1503, 0.1503, 0.1503, 0.1503],\n",
      "         [0.3043, 0.2437, 0.1507, 0.1507, 0.1507],\n",
      "         [0.2588, 0.2285, 0.1637, 0.1745, 0.1745],\n",
      "         [0.3790, 0.2654, 0.1020, 0.1311, 0.1225],\n",
      "         [0.3035, 0.2457, 0.1394, 0.1618, 0.1496]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1966, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1423, 0.3316, 0.1754, 0.1754, 0.1754],\n",
      "         [0.3008, 0.0641, 0.2248, 0.2052, 0.2052],\n",
      "         [0.1598, 0.2472, 0.1735, 0.2415, 0.1780],\n",
      "         [0.2410, 0.0747, 0.1933, 0.0796, 0.4114]],\n",
      "\n",
      "        [[0.2396, 0.1901, 0.1901, 0.1901, 0.1901],\n",
      "         [0.2006, 0.2004, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1812, 0.1885, 0.2011, 0.2146, 0.2146],\n",
      "         [0.1967, 0.2034, 0.2151, 0.1573, 0.2275],\n",
      "         [0.1458, 0.1574, 0.1786, 0.0880, 0.4301]],\n",
      "\n",
      "        [[0.2128, 0.1968, 0.1968, 0.1968, 0.1968],\n",
      "         [0.2101, 0.1802, 0.2032, 0.2032, 0.2032],\n",
      "         [0.2681, 0.1290, 0.1450, 0.2289, 0.2289],\n",
      "         [0.1694, 0.2056, 0.1993, 0.2490, 0.1767],\n",
      "         [0.2665, 0.1619, 0.1753, 0.0987, 0.2976]],\n",
      "\n",
      "        [[0.2577, 0.1856, 0.1856, 0.1856, 0.1856],\n",
      "         [0.2332, 0.2380, 0.1763, 0.1763, 0.1763],\n",
      "         [0.1442, 0.1394, 0.2552, 0.2306, 0.2306],\n",
      "         [0.1594, 0.1550, 0.2566, 0.1932, 0.2358],\n",
      "         [0.1504, 0.1443, 0.3008, 0.1989, 0.2056]],\n",
      "\n",
      "        [[0.2115, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.1219, 0.2208, 0.2191, 0.2191, 0.2191],\n",
      "         [0.2363, 0.1751, 0.2369, 0.1758, 0.1758],\n",
      "         [0.2189, 0.1748, 0.2193, 0.2118, 0.1753],\n",
      "         [0.1646, 0.2990, 0.1637, 0.1797, 0.1929]],\n",
      "\n",
      "        [[0.2056, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.1454, 0.4512, 0.1345, 0.1345, 0.1345],\n",
      "         [0.2219, 0.1505, 0.1717, 0.2280, 0.2280],\n",
      "         [0.1819, 0.2142, 0.2026, 0.2213, 0.1799],\n",
      "         [0.2290, 0.1167, 0.1468, 0.1020, 0.4055]],\n",
      "\n",
      "        [[0.7184, 0.0704, 0.0704, 0.0704, 0.0704],\n",
      "         [0.3659, 0.3195, 0.1049, 0.1049, 0.1049],\n",
      "         [0.0828, 0.0956, 0.1959, 0.3128, 0.3128],\n",
      "         [0.2091, 0.2070, 0.1969, 0.1963, 0.1906],\n",
      "         [0.2362, 0.2249, 0.1763, 0.1737, 0.1889]],\n",
      "\n",
      "        [[0.2449, 0.1888, 0.1888, 0.1888, 0.1888],\n",
      "         [0.2048, 0.2368, 0.1861, 0.1861, 0.1861],\n",
      "         [0.1936, 0.1505, 0.1987, 0.2286, 0.2286],\n",
      "         [0.1679, 0.3173, 0.1572, 0.2474, 0.1102],\n",
      "         [0.2017, 0.1964, 0.2023, 0.1984, 0.2012]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2495, 0.1876, 0.1876, 0.1876, 0.1876],\n",
      "         [0.1876, 0.1898, 0.2075, 0.2075, 0.2075],\n",
      "         [0.1749, 0.1818, 0.1509, 0.2462, 0.2462],\n",
      "         [0.1680, 0.1725, 0.1516, 0.2947, 0.2132],\n",
      "         [0.1977, 0.1984, 0.1948, 0.2139, 0.1952]],\n",
      "\n",
      "        [[0.1934, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1938, 0.2159, 0.1968, 0.1968, 0.1968],\n",
      "         [0.1942, 0.1130, 0.3328, 0.1800, 0.1800],\n",
      "         [0.1899, 0.1403, 0.2565, 0.2315, 0.1819],\n",
      "         [0.1955, 0.2154, 0.1776, 0.1836, 0.2279]],\n",
      "\n",
      "        [[0.0504, 0.2374, 0.2374, 0.2374, 0.2374],\n",
      "         [0.2781, 0.1534, 0.1895, 0.1895, 0.1895],\n",
      "         [0.1154, 0.2601, 0.2345, 0.1950, 0.1950],\n",
      "         [0.0944, 0.2478, 0.2191, 0.2628, 0.1759],\n",
      "         [0.1312, 0.2365, 0.2194, 0.2452, 0.1678]],\n",
      "\n",
      "        [[0.1969, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.2081, 0.2080, 0.1946, 0.1946, 0.1946],\n",
      "         [0.2714, 0.2707, 0.0976, 0.1801, 0.1801],\n",
      "         [0.2224, 0.2219, 0.0915, 0.3084, 0.1558],\n",
      "         [0.2404, 0.2400, 0.1266, 0.3043, 0.0887]],\n",
      "\n",
      "        [[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2093, 0.1726, 0.2060, 0.2060, 0.2060],\n",
      "         [0.2074, 0.2740, 0.0942, 0.2122, 0.2122],\n",
      "         [0.2031, 0.2090, 0.1874, 0.1969, 0.2036],\n",
      "         [0.1712, 0.1361, 0.3281, 0.2204, 0.1441]],\n",
      "\n",
      "        [[0.1915, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.1932, 0.1387, 0.2227, 0.2227, 0.2227],\n",
      "         [0.1938, 0.2424, 0.2115, 0.1761, 0.1761],\n",
      "         [0.1815, 0.2676, 0.2112, 0.1860, 0.1537],\n",
      "         [0.1939, 0.1647, 0.1819, 0.1919, 0.2676]],\n",
      "\n",
      "        [[0.2598, 0.1851, 0.1851, 0.1851, 0.1851],\n",
      "         [0.1847, 0.1734, 0.2140, 0.2140, 0.2140],\n",
      "         [0.2687, 0.3650, 0.1022, 0.1321, 0.1321],\n",
      "         [0.2719, 0.4501, 0.0555, 0.1379, 0.0846],\n",
      "         [0.1947, 0.1897, 0.2109, 0.2015, 0.2032]],\n",
      "\n",
      "        [[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1982, 0.2345, 0.1891, 0.1891, 0.1891],\n",
      "         [0.1673, 0.2179, 0.3040, 0.1554, 0.1554],\n",
      "         [0.1643, 0.1922, 0.2342, 0.2520, 0.1573],\n",
      "         [0.1724, 0.1890, 0.2122, 0.2215, 0.2048]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2082, 0.1979, 0.1979, 0.1979, 0.1979],\n",
      "         [0.1849, 0.2251, 0.1967, 0.1967, 0.1967],\n",
      "         [0.2033, 0.1961, 0.1987, 0.2010, 0.2010],\n",
      "         [0.1580, 0.2780, 0.2263, 0.1489, 0.1888],\n",
      "         [0.2014, 0.1994, 0.2001, 0.2016, 0.1976]],\n",
      "\n",
      "        [[0.2919, 0.1770, 0.1770, 0.1770, 0.1770],\n",
      "         [0.0718, 0.3231, 0.2017, 0.2017, 0.2017],\n",
      "         [0.3231, 0.1363, 0.1834, 0.1786, 0.1786],\n",
      "         [0.2200, 0.1835, 0.1953, 0.2068, 0.1943],\n",
      "         [0.1425, 0.2380, 0.1995, 0.1698, 0.2503]],\n",
      "\n",
      "        [[0.1913, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.2270, 0.1821, 0.1969, 0.1969, 0.1969],\n",
      "         [0.1761, 0.2269, 0.1822, 0.2074, 0.2074],\n",
      "         [0.2166, 0.1568, 0.2075, 0.2431, 0.1759],\n",
      "         [0.2034, 0.1858, 0.2009, 0.2101, 0.1999]],\n",
      "\n",
      "        [[0.1979, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2700, 0.1907, 0.1797, 0.1797, 0.1797],\n",
      "         [0.2870, 0.1669, 0.2418, 0.1522, 0.1522],\n",
      "         [0.2243, 0.1854, 0.2112, 0.1996, 0.1795],\n",
      "         [0.2023, 0.1912, 0.1987, 0.1954, 0.2124]],\n",
      "\n",
      "        [[0.2205, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2590, 0.2237, 0.1724, 0.1724, 0.1724],\n",
      "         [0.1474, 0.1630, 0.2996, 0.1950, 0.1950],\n",
      "         [0.1158, 0.1341, 0.3249, 0.2512, 0.1740],\n",
      "         [0.2043, 0.2015, 0.1854, 0.1899, 0.2189]],\n",
      "\n",
      "        [[0.1985, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.2093, 0.2054, 0.1951, 0.1951, 0.1951],\n",
      "         [0.2246, 0.2147, 0.1805, 0.1901, 0.1901],\n",
      "         [0.1851, 0.1895, 0.2072, 0.2164, 0.2017],\n",
      "         [0.2047, 0.2036, 0.1995, 0.1976, 0.1945]],\n",
      "\n",
      "        [[0.1863, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.2215, 0.1863, 0.1974, 0.1974, 0.1974],\n",
      "         [0.2147, 0.1801, 0.2231, 0.1910, 0.1910],\n",
      "         [0.2385, 0.1259, 0.2743, 0.2056, 0.1558],\n",
      "         [0.2093, 0.1725, 0.2183, 0.2001, 0.1999]],\n",
      "\n",
      "        [[0.2076, 0.1981, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1821, 0.2945, 0.1745, 0.1745, 0.1745],\n",
      "         [0.2080, 0.1905, 0.1822, 0.2096, 0.2096],\n",
      "         [0.1337, 0.2236, 0.2904, 0.2245, 0.1277],\n",
      "         [0.1883, 0.2012, 0.2081, 0.2013, 0.2011]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2045, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1983, 0.1926, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1931, 0.1752, 0.2144, 0.2087, 0.2087],\n",
      "         [0.1993, 0.1975, 0.2013, 0.2011, 0.2008],\n",
      "         [0.1990, 0.1828, 0.2180, 0.2162, 0.1841]],\n",
      "\n",
      "        [[0.2681, 0.1830, 0.1830, 0.1830, 0.1830],\n",
      "         [0.1583, 0.1914, 0.2168, 0.2168, 0.2168],\n",
      "         [0.2201, 0.2032, 0.1911, 0.1928, 0.1928],\n",
      "         [0.1300, 0.1750, 0.2198, 0.2625, 0.2127],\n",
      "         [0.0993, 0.1569, 0.2231, 0.2934, 0.2273]],\n",
      "\n",
      "        [[0.1923, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1989, 0.2303, 0.1903, 0.1903, 0.1903],\n",
      "         [0.1723, 0.2438, 0.2735, 0.1552, 0.1552],\n",
      "         [0.1919, 0.1998, 0.2025, 0.2164, 0.1895],\n",
      "         [0.1885, 0.1984, 0.2018, 0.2196, 0.1917]],\n",
      "\n",
      "        [[0.2187, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.1880, 0.1944, 0.2059, 0.2059, 0.2059],\n",
      "         [0.1974, 0.1986, 0.2027, 0.2007, 0.2007],\n",
      "         [0.1877, 0.1939, 0.2160, 0.1976, 0.2049],\n",
      "         [0.2364, 0.2063, 0.1316, 0.1906, 0.2351]],\n",
      "\n",
      "        [[0.2622, 0.1845, 0.1845, 0.1845, 0.1845],\n",
      "         [0.3017, 0.1496, 0.1829, 0.1829, 0.1829],\n",
      "         [0.2408, 0.1750, 0.2007, 0.1917, 0.1917],\n",
      "         [0.2182, 0.1806, 0.1959, 0.2146, 0.1907],\n",
      "         [0.2288, 0.0872, 0.1321, 0.2102, 0.3417]],\n",
      "\n",
      "        [[0.2208, 0.1948, 0.1948, 0.1948, 0.1948],\n",
      "         [0.1997, 0.1968, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1860, 0.1695, 0.2535, 0.1955, 0.1955],\n",
      "         [0.1628, 0.1394, 0.2721, 0.2489, 0.1768],\n",
      "         [0.1934, 0.1869, 0.2167, 0.2125, 0.1905]],\n",
      "\n",
      "        [[0.1729, 0.2068, 0.2068, 0.2068, 0.2068],\n",
      "         [0.2147, 0.1979, 0.1958, 0.1958, 0.1958],\n",
      "         [0.3784, 0.1585, 0.1796, 0.1418, 0.1418],\n",
      "         [0.1243, 0.2177, 0.2009, 0.2232, 0.2339],\n",
      "         [0.1959, 0.2029, 0.2019, 0.2032, 0.1962]],\n",
      "\n",
      "        [[0.1999, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1527, 0.3358, 0.1705, 0.1705, 0.1705],\n",
      "         [0.1864, 0.2237, 0.2075, 0.1912, 0.1912],\n",
      "         [0.2504, 0.1485, 0.1841, 0.1844, 0.2327],\n",
      "         [0.1584, 0.2288, 0.1967, 0.1965, 0.2195]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2121, 0.1581, 0.2099, 0.2099, 0.2099],\n",
      "         [0.1460, 0.3490, 0.2037, 0.1507, 0.1507],\n",
      "         [0.1985, 0.1506, 0.1786, 0.2757, 0.1966],\n",
      "         [0.1860, 0.2324, 0.2025, 0.1428, 0.2364]],\n",
      "\n",
      "        [[0.1940, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.2004, 0.2094, 0.1967, 0.1967, 0.1967],\n",
      "         [0.1977, 0.1944, 0.2099, 0.1991, 0.1991],\n",
      "         [0.2186, 0.2312, 0.1794, 0.1572, 0.2136],\n",
      "         [0.2303, 0.2518, 0.1681, 0.1361, 0.2137]],\n",
      "\n",
      "        [[0.2317, 0.1921, 0.1921, 0.1921, 0.1921],\n",
      "         [0.3048, 0.0648, 0.2101, 0.2101, 0.2101],\n",
      "         [0.2557, 0.1228, 0.1927, 0.2144, 0.2144],\n",
      "         [0.2715, 0.1402, 0.2104, 0.1463, 0.2317],\n",
      "         [0.3192, 0.1351, 0.2291, 0.1428, 0.1738]],\n",
      "\n",
      "        [[0.1775, 0.2056, 0.2056, 0.2056, 0.2056],\n",
      "         [0.1717, 0.1751, 0.2177, 0.2177, 0.2177],\n",
      "         [0.1742, 0.1769, 0.2300, 0.2095, 0.2095],\n",
      "         [0.1854, 0.1876, 0.2289, 0.1848, 0.2133],\n",
      "         [0.0759, 0.0831, 0.3914, 0.0742, 0.3753]],\n",
      "\n",
      "        [[0.1924, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1796, 0.2145, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1903, 0.1525, 0.3285, 0.1644, 0.1644],\n",
      "         [0.1637, 0.1282, 0.2994, 0.2695, 0.1393],\n",
      "         [0.2088, 0.3225, 0.0714, 0.0862, 0.3111]],\n",
      "\n",
      "        [[0.1767, 0.2058, 0.2058, 0.2058, 0.2058],\n",
      "         [0.2945, 0.1695, 0.1787, 0.1787, 0.1787],\n",
      "         [0.1094, 0.2582, 0.1567, 0.2378, 0.2378],\n",
      "         [0.2020, 0.1988, 0.2007, 0.1994, 0.1991],\n",
      "         [0.1442, 0.2404, 0.1786, 0.2170, 0.2197]],\n",
      "\n",
      "        [[0.2493, 0.1877, 0.1877, 0.1877, 0.1877],\n",
      "         [0.1912, 0.1763, 0.2108, 0.2108, 0.2108],\n",
      "         [0.1754, 0.1553, 0.2631, 0.2031, 0.2031],\n",
      "         [0.2032, 0.2083, 0.1873, 0.2039, 0.1973],\n",
      "         [0.1705, 0.1338, 0.3813, 0.1649, 0.1495]],\n",
      "\n",
      "        [[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1931, 0.1825, 0.2082, 0.2082, 0.2082],\n",
      "         [0.2002, 0.2005, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1976, 0.1992, 0.1952, 0.2126, 0.1954],\n",
      "         [0.2099, 0.2065, 0.2150, 0.1810, 0.1877]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1995, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1014, 0.1793, 0.2398, 0.2398, 0.2398],\n",
      "         [0.3109, 0.1747, 0.2542, 0.1301, 0.1301],\n",
      "         [0.1936, 0.2009, 0.1961, 0.2046, 0.2048],\n",
      "         [0.1674, 0.2000, 0.1782, 0.2178, 0.2365]],\n",
      "\n",
      "        [[0.1588, 0.2103, 0.2103, 0.2103, 0.2103],\n",
      "         [0.1371, 0.1683, 0.2315, 0.2315, 0.2315],\n",
      "         [0.2605, 0.1800, 0.3567, 0.1014, 0.1014],\n",
      "         [0.1982, 0.1605, 0.2372, 0.2885, 0.1156],\n",
      "         [0.1820, 0.2133, 0.1591, 0.1373, 0.3083]],\n",
      "\n",
      "        [[0.1779, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.2042, 0.2620, 0.1779, 0.1779, 0.1779],\n",
      "         [0.1852, 0.4296, 0.1527, 0.1163, 0.1163],\n",
      "         [0.2008, 0.3120, 0.1815, 0.1484, 0.1574],\n",
      "         [0.1857, 0.1341, 0.2001, 0.2322, 0.2480]],\n",
      "\n",
      "        [[0.1977, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1910, 0.2384, 0.1902, 0.1902, 0.1902],\n",
      "         [0.2066, 0.1744, 0.2045, 0.2073, 0.2073],\n",
      "         [0.1401, 0.3795, 0.1489, 0.1938, 0.1377],\n",
      "         [0.2108, 0.1835, 0.2090, 0.2015, 0.1953]],\n",
      "\n",
      "        [[0.1798, 0.2051, 0.2051, 0.2051, 0.2051],\n",
      "         [0.1806, 0.1728, 0.2155, 0.2155, 0.2155],\n",
      "         [0.2102, 0.2216, 0.2275, 0.1704, 0.1704],\n",
      "         [0.2060, 0.2119, 0.2149, 0.1832, 0.1841],\n",
      "         [0.2037, 0.1987, 0.1962, 0.2259, 0.1756]],\n",
      "\n",
      "        [[0.1506, 0.2124, 0.2124, 0.2124, 0.2124],\n",
      "         [0.1573, 0.1523, 0.2301, 0.2301, 0.2301],\n",
      "         [0.2318, 0.2506, 0.3311, 0.0932, 0.0932],\n",
      "         [0.2072, 0.2183, 0.2630, 0.1989, 0.1127],\n",
      "         [0.1598, 0.1517, 0.1258, 0.1665, 0.3962]],\n",
      "\n",
      "        [[0.1999, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1949, 0.2207, 0.1948, 0.1948, 0.1948],\n",
      "         [0.1978, 0.2107, 0.1960, 0.1978, 0.1978],\n",
      "         [0.1911, 0.2439, 0.1845, 0.1896, 0.1910],\n",
      "         [0.1876, 0.2490, 0.1800, 0.1858, 0.1975]],\n",
      "\n",
      "        [[0.1417, 0.2146, 0.2146, 0.2146, 0.2146],\n",
      "         [0.1911, 0.1938, 0.2050, 0.2050, 0.2050],\n",
      "         [0.2561, 0.2347, 0.1773, 0.1660, 0.1660],\n",
      "         [0.1679, 0.1766, 0.2075, 0.2324, 0.2156],\n",
      "         [0.1875, 0.1912, 0.2039, 0.2132, 0.2042]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2628, 0.1843, 0.1843, 0.1843, 0.1843],\n",
      "         [0.2107, 0.1968, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1989, 0.2006, 0.1995, 0.2005, 0.2005],\n",
      "         [0.1611, 0.2525, 0.1871, 0.1524, 0.2469],\n",
      "         [0.2187, 0.1816, 0.2055, 0.2238, 0.1704]],\n",
      "\n",
      "        [[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1969, 0.1723, 0.2102, 0.2102, 0.2102],\n",
      "         [0.1249, 0.1551, 0.4955, 0.1123, 0.1123],\n",
      "         [0.2061, 0.2034, 0.1896, 0.1936, 0.2074],\n",
      "         [0.2419, 0.2188, 0.1277, 0.1499, 0.2617]],\n",
      "\n",
      "        [[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1867, 0.1476, 0.2219, 0.2219, 0.2219],\n",
      "         [0.1958, 0.2265, 0.2260, 0.1759, 0.1759],\n",
      "         [0.2210, 0.1783, 0.1788, 0.1630, 0.2589],\n",
      "         [0.2138, 0.1909, 0.1912, 0.1821, 0.2219]],\n",
      "\n",
      "        [[0.1958, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1995, 0.2042, 0.1988, 0.1988, 0.1988],\n",
      "         [0.2118, 0.1426, 0.1935, 0.2261, 0.2261],\n",
      "         [0.1535, 0.2670, 0.1742, 0.2652, 0.1401],\n",
      "         [0.2031, 0.1962, 0.2015, 0.1963, 0.2029]],\n",
      "\n",
      "        [[0.1698, 0.2075, 0.2075, 0.2075, 0.2075],\n",
      "         [0.1661, 0.1921, 0.2140, 0.2140, 0.2140],\n",
      "         [0.2062, 0.1964, 0.2186, 0.1894, 0.1894],\n",
      "         [0.1932, 0.1644, 0.2349, 0.2617, 0.1458],\n",
      "         [0.2014, 0.2119, 0.1895, 0.1831, 0.2141]],\n",
      "\n",
      "        [[0.1975, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1963, 0.2018, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2096, 0.1823, 0.2321, 0.1880, 0.1880],\n",
      "         [0.1945, 0.1688, 0.2159, 0.2467, 0.1741],\n",
      "         [0.1897, 0.2431, 0.1582, 0.1253, 0.2838]],\n",
      "\n",
      "        [[0.1963, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.1490, 0.2010, 0.2167, 0.2167, 0.2167],\n",
      "         [0.1979, 0.2003, 0.1999, 0.2009, 0.2009],\n",
      "         [0.2011, 0.1946, 0.1957, 0.2156, 0.1930],\n",
      "         [0.2054, 0.2302, 0.2258, 0.1613, 0.1773]],\n",
      "\n",
      "        [[0.1989, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1829, 0.1784, 0.2129, 0.2129, 0.2129],\n",
      "         [0.2432, 0.2675, 0.2152, 0.1371, 0.1371],\n",
      "         [0.1807, 0.1712, 0.1936, 0.2046, 0.2500],\n",
      "         [0.2053, 0.2158, 0.1927, 0.1832, 0.2030]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1922, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1900, 0.1613, 0.2162, 0.2162, 0.2162],\n",
      "         [0.1944, 0.1905, 0.2199, 0.1976, 0.1976],\n",
      "         [0.2108, 0.2259, 0.1391, 0.2247, 0.1995],\n",
      "         [0.1915, 0.1806, 0.2721, 0.1815, 0.1743]],\n",
      "\n",
      "        [[0.1776, 0.2056, 0.2056, 0.2056, 0.2056],\n",
      "         [0.1619, 0.2279, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1947, 0.2055, 0.1961, 0.2019, 0.2019],\n",
      "         [0.2788, 0.1362, 0.2538, 0.1584, 0.1728],\n",
      "         [0.1463, 0.2685, 0.1584, 0.2362, 0.1906]],\n",
      "\n",
      "        [[0.2202, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2388, 0.1902, 0.1903, 0.1903, 0.1903],\n",
      "         [0.1746, 0.2023, 0.2186, 0.2022, 0.2022],\n",
      "         [0.2645, 0.2063, 0.1811, 0.1417, 0.2064],\n",
      "         [0.1713, 0.2045, 0.2245, 0.2673, 0.1324]],\n",
      "\n",
      "        [[0.1278, 0.2181, 0.2181, 0.2181, 0.2181],\n",
      "         [0.2045, 0.1984, 0.1990, 0.1990, 0.1990],\n",
      "         [0.3450, 0.1573, 0.1569, 0.1704, 0.1704],\n",
      "         [0.1725, 0.2135, 0.2137, 0.1914, 0.2089],\n",
      "         [0.0898, 0.2498, 0.2507, 0.1479, 0.2618]],\n",
      "\n",
      "        [[0.2120, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1986, 0.1900, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1996, 0.2196, 0.2033, 0.1888, 0.1888],\n",
      "         [0.2187, 0.1813, 0.2110, 0.1451, 0.2439],\n",
      "         [0.2186, 0.1922, 0.2132, 0.1650, 0.2110]],\n",
      "\n",
      "        [[0.2206, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2029, 0.2003, 0.1989, 0.1989, 0.1989],\n",
      "         [0.3082, 0.2081, 0.1420, 0.1709, 0.1709],\n",
      "         [0.1608, 0.1862, 0.2147, 0.2380, 0.2004],\n",
      "         [0.1315, 0.1781, 0.2395, 0.2964, 0.1545]],\n",
      "\n",
      "        [[0.2013, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2915, 0.1313, 0.1924, 0.1924, 0.1924],\n",
      "         [0.1669, 0.2144, 0.2383, 0.1902, 0.1902],\n",
      "         [0.2185, 0.1904, 0.1797, 0.2079, 0.2034],\n",
      "         [0.2028, 0.1998, 0.1985, 0.2017, 0.1972]],\n",
      "\n",
      "        [[0.1939, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1983, 0.2026, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2004, 0.1981, 0.2022, 0.1996, 0.1996],\n",
      "         [0.1988, 0.1744, 0.2218, 0.2147, 0.1904],\n",
      "         [0.2201, 0.3043, 0.1680, 0.1819, 0.1257]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1555, 0.2111, 0.2111, 0.2111, 0.2111],\n",
      "         [0.1594, 0.2218, 0.2063, 0.2063, 0.2063],\n",
      "         [0.1600, 0.2218, 0.2055, 0.2064, 0.2064],\n",
      "         [0.1494, 0.2344, 0.2111, 0.1929, 0.2123],\n",
      "         [0.2070, 0.1964, 0.1988, 0.2009, 0.1969]],\n",
      "\n",
      "        [[0.1616, 0.2096, 0.2096, 0.2096, 0.2096],\n",
      "         [0.1365, 0.1800, 0.2279, 0.2279, 0.2279],\n",
      "         [0.2068, 0.2016, 0.1968, 0.1974, 0.1974],\n",
      "         [0.1851, 0.2045, 0.2251, 0.1625, 0.2227],\n",
      "         [0.1747, 0.2089, 0.2480, 0.1384, 0.2300]],\n",
      "\n",
      "        [[0.1702, 0.2074, 0.2074, 0.2074, 0.2074],\n",
      "         [0.1691, 0.1905, 0.2135, 0.2135, 0.2135],\n",
      "         [0.2115, 0.2031, 0.1945, 0.1955, 0.1955],\n",
      "         [0.2102, 0.2051, 0.1998, 0.1846, 0.2004],\n",
      "         [0.1414, 0.1582, 0.1785, 0.2563, 0.2656]],\n",
      "\n",
      "        [[0.2102, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.2635, 0.1737, 0.1876, 0.1876, 0.1876],\n",
      "         [0.2816, 0.1820, 0.1419, 0.1973, 0.1973],\n",
      "         [0.2253, 0.1794, 0.1576, 0.2505, 0.1871],\n",
      "         [0.2181, 0.1784, 0.1592, 0.2395, 0.2048]],\n",
      "\n",
      "        [[0.1721, 0.2070, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1042, 0.1427, 0.2510, 0.2510, 0.2510],\n",
      "         [0.2271, 0.2107, 0.1936, 0.1843, 0.1843],\n",
      "         [0.2153, 0.2066, 0.1971, 0.1893, 0.1918],\n",
      "         [0.1946, 0.1978, 0.2016, 0.2049, 0.2011]],\n",
      "\n",
      "        [[0.2596, 0.1851, 0.1851, 0.1851, 0.1851],\n",
      "         [0.2106, 0.1885, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1864, 0.2105, 0.2093, 0.1969, 0.1969],\n",
      "         [0.2297, 0.1882, 0.1899, 0.1822, 0.2099],\n",
      "         [0.2523, 0.1619, 0.1652, 0.1506, 0.2700]],\n",
      "\n",
      "        [[0.1798, 0.2050, 0.2050, 0.2050, 0.2050],\n",
      "         [0.2096, 0.2075, 0.1943, 0.1943, 0.1943],\n",
      "         [0.1817, 0.1839, 0.2367, 0.1989, 0.1989],\n",
      "         [0.2056, 0.2012, 0.1264, 0.2928, 0.1741],\n",
      "         [0.1999, 0.1998, 0.1970, 0.2020, 0.2014]],\n",
      "\n",
      "        [[0.1426, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1885, 0.1866, 0.2083, 0.2083, 0.2083],\n",
      "         [0.2317, 0.2378, 0.1686, 0.1809, 0.1809],\n",
      "         [0.1831, 0.1816, 0.2021, 0.2354, 0.1978],\n",
      "         [0.1969, 0.1967, 0.2000, 0.2047, 0.2017]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2006, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.3041, 0.1349, 0.1870, 0.1870, 0.1870],\n",
      "         [0.2048, 0.1895, 0.2147, 0.1955, 0.1955],\n",
      "         [0.2080, 0.1721, 0.2335, 0.2007, 0.1857],\n",
      "         [0.1791, 0.2305, 0.1535, 0.1877, 0.2492]],\n",
      "\n",
      "        [[0.2341, 0.1915, 0.1915, 0.1915, 0.1915],\n",
      "         [0.2337, 0.1801, 0.1954, 0.1954, 0.1954],\n",
      "         [0.2083, 0.1899, 0.2106, 0.1955, 0.1955],\n",
      "         [0.2160, 0.1095, 0.2339, 0.3051, 0.1356],\n",
      "         [0.2157, 0.1625, 0.2230, 0.2492, 0.1495]],\n",
      "\n",
      "        [[0.1814, 0.2046, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1995, 0.2471, 0.1845, 0.1845, 0.1845],\n",
      "         [0.1766, 0.3236, 0.2172, 0.1413, 0.1413],\n",
      "         [0.1934, 0.2238, 0.2033, 0.1961, 0.1833],\n",
      "         [0.2001, 0.1987, 0.1996, 0.2000, 0.2017]],\n",
      "\n",
      "        [[0.1601, 0.2100, 0.2100, 0.2100, 0.2100],\n",
      "         [0.1797, 0.2203, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2618, 0.1574, 0.1800, 0.2004, 0.2004],\n",
      "         [0.1970, 0.2019, 0.2006, 0.2010, 0.1996],\n",
      "         [0.1608, 0.2278, 0.2078, 0.2135, 0.1901]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2180, 0.1827, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1877, 0.1209, 0.3892, 0.1511, 0.1511],\n",
      "         [0.1687, 0.0960, 0.4297, 0.1778, 0.1277],\n",
      "         [0.1941, 0.1494, 0.2992, 0.1988, 0.1585]],\n",
      "\n",
      "        [[0.1682, 0.2079, 0.2079, 0.2079, 0.2079],\n",
      "         [0.2243, 0.3083, 0.1558, 0.1558, 0.1558],\n",
      "         [0.1918, 0.2224, 0.2618, 0.1620, 0.1620],\n",
      "         [0.1482, 0.2281, 0.3673, 0.1660, 0.0904],\n",
      "         [0.2025, 0.1700, 0.1401, 0.1934, 0.2941]],\n",
      "\n",
      "        [[0.2188, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.1932, 0.2663, 0.1802, 0.1802, 0.1802],\n",
      "         [0.1613, 0.4397, 0.1400, 0.1295, 0.1295],\n",
      "         [0.2119, 0.0973, 0.2366, 0.2029, 0.2512],\n",
      "         [0.2054, 0.1842, 0.2086, 0.2041, 0.1977]],\n",
      "\n",
      "        [[0.2126, 0.1969, 0.1969, 0.1969, 0.1969],\n",
      "         [0.2253, 0.2367, 0.1793, 0.1793, 0.1793],\n",
      "         [0.1966, 0.1953, 0.2019, 0.2031, 0.2031],\n",
      "         [0.1891, 0.1852, 0.2041, 0.2138, 0.2078],\n",
      "         [0.2337, 0.2437, 0.2000, 0.1819, 0.1407]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2246, 0.1939, 0.1939, 0.1939, 0.1939],\n",
      "         [0.2400, 0.1285, 0.2105, 0.2105, 0.2105],\n",
      "         [0.2293, 0.1456, 0.2083, 0.2084, 0.2084],\n",
      "         [0.2249, 0.1798, 0.2146, 0.1662, 0.2146],\n",
      "         [0.1091, 0.2980, 0.1348, 0.4245, 0.0337]],\n",
      "\n",
      "        [[0.1112, 0.2222, 0.2222, 0.2222, 0.2222],\n",
      "         [0.2070, 0.2010, 0.1973, 0.1973, 0.1973],\n",
      "         [0.1685, 0.1931, 0.2173, 0.2105, 0.2105],\n",
      "         [0.1751, 0.2012, 0.2270, 0.1770, 0.2197],\n",
      "         [0.2659, 0.1967, 0.1515, 0.2596, 0.1262]],\n",
      "\n",
      "        [[0.1873, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.2154, 0.1850, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2132, 0.1910, 0.1920, 0.2020, 0.2020],\n",
      "         [0.2229, 0.1791, 0.1809, 0.2170, 0.2002],\n",
      "         [0.2310, 0.1813, 0.1834, 0.2242, 0.1800]],\n",
      "\n",
      "        [[0.1402, 0.2150, 0.2150, 0.2150, 0.2150],\n",
      "         [0.2124, 0.1887, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2343, 0.1775, 0.1829, 0.2027, 0.2027],\n",
      "         [0.1834, 0.2093, 0.2063, 0.2045, 0.1965],\n",
      "         [0.1740, 0.2238, 0.2178, 0.2141, 0.1702]],\n",
      "\n",
      "        [[0.2067, 0.1983, 0.1983, 0.1983, 0.1983],\n",
      "         [0.2155, 0.2090, 0.1918, 0.1918, 0.1918],\n",
      "         [0.1951, 0.1974, 0.1992, 0.2041, 0.2041],\n",
      "         [0.1901, 0.1999, 0.2074, 0.1723, 0.2303],\n",
      "         [0.2326, 0.2044, 0.1859, 0.2999, 0.0770]],\n",
      "\n",
      "        [[0.1983, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.2020, 0.1988, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2010, 0.1987, 0.2015, 0.1994, 0.1994],\n",
      "         [0.2324, 0.2071, 0.2394, 0.1070, 0.2141],\n",
      "         [0.1615, 0.1854, 0.1558, 0.4087, 0.0887]],\n",
      "\n",
      "        [[0.2053, 0.1987, 0.1987, 0.1987, 0.1987],\n",
      "         [0.1991, 0.2180, 0.1943, 0.1943, 0.1943],\n",
      "         [0.1997, 0.2056, 0.1983, 0.1982, 0.1982],\n",
      "         [0.2010, 0.1977, 0.2018, 0.1977, 0.2019],\n",
      "         [0.1990, 0.2177, 0.1944, 0.2177, 0.1711]],\n",
      "\n",
      "        [[0.1908, 0.2023, 0.2023, 0.2023, 0.2023],\n",
      "         [0.2073, 0.2097, 0.1943, 0.1943, 0.1943],\n",
      "         [0.2482, 0.2571, 0.0881, 0.2033, 0.2033],\n",
      "         [0.2655, 0.2769, 0.0772, 0.1713, 0.2092],\n",
      "         [0.2887, 0.3022, 0.0752, 0.1791, 0.1548]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1399, 0.2150, 0.2150, 0.2150, 0.2150],\n",
      "         [0.2003, 0.1275, 0.2241, 0.2241, 0.2241],\n",
      "         [0.1644, 0.4232, 0.1524, 0.1300, 0.1300],\n",
      "         [0.2163, 0.1313, 0.2251, 0.1825, 0.2449],\n",
      "         [0.1655, 0.3137, 0.1573, 0.2058, 0.1578]],\n",
      "\n",
      "        [[0.0799, 0.2300, 0.2300, 0.2300, 0.2300],\n",
      "         [0.1770, 0.1733, 0.2166, 0.2166, 0.2166],\n",
      "         [0.1915, 0.1897, 0.2006, 0.2091, 0.2091],\n",
      "         [0.1936, 0.1920, 0.2014, 0.2042, 0.2088],\n",
      "         [0.2173, 0.2320, 0.1565, 0.1399, 0.2542]],\n",
      "\n",
      "        [[0.1865, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1515, 0.2221, 0.2088, 0.2088, 0.2088],\n",
      "         [0.3426, 0.1356, 0.2068, 0.1575, 0.1575],\n",
      "         [0.2019, 0.1993, 0.2005, 0.1987, 0.1997],\n",
      "         [0.2552, 0.1477, 0.1895, 0.1307, 0.2769]],\n",
      "\n",
      "        [[0.1561, 0.2110, 0.2110, 0.2110, 0.2110],\n",
      "         [0.1255, 0.1783, 0.2321, 0.2321, 0.2321],\n",
      "         [0.2303, 0.1912, 0.2459, 0.1663, 0.1663],\n",
      "         [0.2184, 0.1813, 0.2331, 0.2095, 0.1577],\n",
      "         [0.2075, 0.1619, 0.2264, 0.1963, 0.2078]],\n",
      "\n",
      "        [[0.1730, 0.2068, 0.2068, 0.2068, 0.2068],\n",
      "         [0.2261, 0.2852, 0.1629, 0.1629, 0.1629],\n",
      "         [0.1566, 0.1153, 0.2457, 0.2412, 0.2412],\n",
      "         [0.1815, 0.1444, 0.2542, 0.1691, 0.2508],\n",
      "         [0.1976, 0.2616, 0.1306, 0.2156, 0.1946]],\n",
      "\n",
      "        [[0.1624, 0.2094, 0.2094, 0.2094, 0.2094],\n",
      "         [0.1627, 0.1878, 0.2165, 0.2165, 0.2165],\n",
      "         [0.1817, 0.1891, 0.2359, 0.1967, 0.1967],\n",
      "         [0.1860, 0.1947, 0.2512, 0.1645, 0.2037],\n",
      "         [0.2041, 0.1944, 0.1478, 0.2329, 0.2208]],\n",
      "\n",
      "        [[0.1903, 0.2024, 0.2024, 0.2024, 0.2024],\n",
      "         [0.1870, 0.2413, 0.1906, 0.1906, 0.1906],\n",
      "         [0.1955, 0.1019, 0.3301, 0.1863, 0.1863],\n",
      "         [0.1996, 0.1797, 0.2171, 0.2056, 0.1980],\n",
      "         [0.1898, 0.2306, 0.1623, 0.1795, 0.2379]],\n",
      "\n",
      "        [[0.1132, 0.2217, 0.2217, 0.2217, 0.2217],\n",
      "         [0.1792, 0.1887, 0.2107, 0.2107, 0.2107],\n",
      "         [0.2113, 0.2053, 0.1969, 0.1932, 0.1932],\n",
      "         [0.1835, 0.1916, 0.2040, 0.2109, 0.2100],\n",
      "         [0.2032, 0.2005, 0.1967, 0.1947, 0.2050]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1365, 0.2159, 0.2159, 0.2159, 0.2159],\n",
      "         [0.2951, 0.1824, 0.1742, 0.1742, 0.1742],\n",
      "         [0.1050, 0.1853, 0.3186, 0.1956, 0.1956],\n",
      "         [0.2626, 0.2017, 0.1568, 0.1821, 0.1967],\n",
      "         [0.2380, 0.2038, 0.1758, 0.1919, 0.1905]],\n",
      "\n",
      "        [[0.1967, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1902, 0.2305, 0.1931, 0.1931, 0.1931],\n",
      "         [0.1786, 0.0890, 0.3942, 0.1691, 0.1691],\n",
      "         [0.1777, 0.1200, 0.2778, 0.2521, 0.1724],\n",
      "         [0.1833, 0.2631, 0.1215, 0.1329, 0.2992]],\n",
      "\n",
      "        [[0.1887, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1878, 0.1981, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1739, 0.1837, 0.2622, 0.1901, 0.1901],\n",
      "         [0.1756, 0.1831, 0.2405, 0.2129, 0.1879],\n",
      "         [0.2172, 0.2027, 0.1296, 0.1583, 0.2922]],\n",
      "\n",
      "        [[0.2789, 0.1803, 0.1803, 0.1803, 0.1803],\n",
      "         [0.1589, 0.1943, 0.2156, 0.2156, 0.2156],\n",
      "         [0.1599, 0.1895, 0.2367, 0.2070, 0.2070],\n",
      "         [0.1489, 0.1777, 0.2240, 0.2546, 0.1948],\n",
      "         [0.1748, 0.1905, 0.2130, 0.2266, 0.1952]],\n",
      "\n",
      "        [[0.2032, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1970, 0.2330, 0.1900, 0.1900, 0.1900],\n",
      "         [0.1988, 0.1950, 0.2070, 0.1996, 0.1996],\n",
      "         [0.2041, 0.2215, 0.1710, 0.2030, 0.2005],\n",
      "         [0.2024, 0.2166, 0.1747, 0.2015, 0.2047]],\n",
      "\n",
      "        [[0.1966, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2300, 0.1525, 0.2058, 0.2058, 0.2058],\n",
      "         [0.1034, 0.3021, 0.3184, 0.1381, 0.1381],\n",
      "         [0.1295, 0.2535, 0.2620, 0.1998, 0.1552],\n",
      "         [0.2611, 0.1250, 0.1206, 0.1622, 0.3311]],\n",
      "\n",
      "        [[0.1589, 0.2103, 0.2103, 0.2103, 0.2103],\n",
      "         [0.1876, 0.1622, 0.2167, 0.2167, 0.2167],\n",
      "         [0.1983, 0.1880, 0.1958, 0.2090, 0.2090],\n",
      "         [0.1927, 0.2677, 0.2084, 0.1920, 0.1392],\n",
      "         [0.1805, 0.2867, 0.2015, 0.1796, 0.1517]],\n",
      "\n",
      "        [[0.1874, 0.2031, 0.2031, 0.2031, 0.2031],\n",
      "         [0.1845, 0.2123, 0.2011, 0.2011, 0.2011],\n",
      "         [0.1749, 0.2036, 0.2374, 0.1920, 0.1920],\n",
      "         [0.2395, 0.1969, 0.1614, 0.1899, 0.2123],\n",
      "         [0.2472, 0.1911, 0.1474, 0.1823, 0.2320]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1738, 0.2066, 0.2066, 0.2066, 0.2066],\n",
      "         [0.1548, 0.2421, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1175, 0.2713, 0.2280, 0.1916, 0.1916],\n",
      "         [0.1990, 0.2005, 0.2002, 0.2003, 0.1999],\n",
      "         [0.1964, 0.2011, 0.2001, 0.2004, 0.2020]],\n",
      "\n",
      "        [[0.1949, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.1662, 0.2291, 0.2016, 0.2016, 0.2016],\n",
      "         [0.2189, 0.1914, 0.1858, 0.2019, 0.2019],\n",
      "         [0.2129, 0.1784, 0.1715, 0.2459, 0.1914],\n",
      "         [0.1752, 0.2223, 0.2344, 0.1443, 0.2239]],\n",
      "\n",
      "        [[0.2452, 0.1887, 0.1887, 0.1887, 0.1887],\n",
      "         [0.2125, 0.1871, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1554, 0.2507, 0.2046, 0.1947, 0.1947],\n",
      "         [0.1973, 0.2034, 0.2008, 0.1983, 0.2002],\n",
      "         [0.2329, 0.1174, 0.1570, 0.2087, 0.2839]],\n",
      "\n",
      "        [[0.2017, 0.1996, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2232, 0.1915, 0.1951, 0.1951, 0.1951],\n",
      "         [0.2262, 0.1711, 0.2488, 0.1769, 0.1769],\n",
      "         [0.2041, 0.1625, 0.2206, 0.2458, 0.1670],\n",
      "         [0.1907, 0.1312, 0.2168, 0.2591, 0.2022]],\n",
      "\n",
      "        [[0.2062, 0.1984, 0.1984, 0.1984, 0.1984],\n",
      "         [0.1894, 0.2162, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1916, 0.1366, 0.3303, 0.1707, 0.1707],\n",
      "         [0.1952, 0.1673, 0.2501, 0.2022, 0.1852],\n",
      "         [0.2000, 0.2004, 0.1993, 0.1999, 0.2004]],\n",
      "\n",
      "        [[0.2507, 0.1873, 0.1873, 0.1873, 0.1873],\n",
      "         [0.1694, 0.2335, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1655, 0.2185, 0.2355, 0.1903, 0.1903],\n",
      "         [0.1681, 0.2099, 0.2229, 0.2110, 0.1880],\n",
      "         [0.2715, 0.1289, 0.1054, 0.1266, 0.3676]],\n",
      "\n",
      "        [[0.1746, 0.2063, 0.2063, 0.2063, 0.2063],\n",
      "         [0.2045, 0.1992, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1572, 0.2055, 0.2175, 0.2099, 0.2099],\n",
      "         [0.1830, 0.1221, 0.1121, 0.4644, 0.1183],\n",
      "         [0.1815, 0.1304, 0.1216, 0.3890, 0.1776]],\n",
      "\n",
      "        [[0.1920, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2072, 0.2063, 0.1955, 0.1955, 0.1955],\n",
      "         [0.2281, 0.2262, 0.1405, 0.2026, 0.2026],\n",
      "         [0.1454, 0.1465, 0.2290, 0.3165, 0.1625],\n",
      "         [0.2203, 0.2197, 0.1920, 0.1742, 0.1938]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1940, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.2147, 0.1990, 0.1954, 0.1954, 0.1954],\n",
      "         [0.2088, 0.2039, 0.1819, 0.2027, 0.2027],\n",
      "         [0.1530, 0.1774, 0.3563, 0.1297, 0.1837],\n",
      "         [0.2148, 0.2029, 0.1553, 0.2289, 0.1981]],\n",
      "\n",
      "        [[0.2118, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1293, 0.2285, 0.2141, 0.2141, 0.2141],\n",
      "         [0.2158, 0.1956, 0.1930, 0.1978, 0.1978],\n",
      "         [0.1708, 0.2069, 0.2123, 0.2077, 0.2024],\n",
      "         [0.1797, 0.2061, 0.2099, 0.2066, 0.1977]],\n",
      "\n",
      "        [[0.1063, 0.2234, 0.2234, 0.2234, 0.2234],\n",
      "         [0.1481, 0.2098, 0.2140, 0.2140, 0.2140],\n",
      "         [0.3762, 0.1833, 0.0888, 0.1758, 0.1758],\n",
      "         [0.1920, 0.1994, 0.2071, 0.2017, 0.1998],\n",
      "         [0.1636, 0.2016, 0.2489, 0.2151, 0.1707]],\n",
      "\n",
      "        [[0.1934, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1174, 0.2167, 0.2220, 0.2220, 0.2220],\n",
      "         [0.0828, 0.2304, 0.2073, 0.2397, 0.2397],\n",
      "         [0.1575, 0.2068, 0.2011, 0.2255, 0.2090],\n",
      "         [0.2154, 0.1977, 0.1995, 0.1924, 0.1950]],\n",
      "\n",
      "        [[0.1110, 0.2223, 0.2223, 0.2223, 0.2223],\n",
      "         [0.1124, 0.1735, 0.2380, 0.2380, 0.2380],\n",
      "         [0.3258, 0.2142, 0.1441, 0.1579, 0.1579],\n",
      "         [0.3283, 0.2035, 0.1294, 0.1952, 0.1436],\n",
      "         [0.1732, 0.1994, 0.2278, 0.2019, 0.1977]],\n",
      "\n",
      "        [[0.1173, 0.2207, 0.2207, 0.2207, 0.2207],\n",
      "         [0.2354, 0.1999, 0.1882, 0.1882, 0.1882],\n",
      "         [0.2128, 0.2008, 0.1931, 0.1966, 0.1966],\n",
      "         [0.1967, 0.1993, 0.2010, 0.2028, 0.2002],\n",
      "         [0.2126, 0.2012, 0.1937, 0.1867, 0.2058]],\n",
      "\n",
      "        [[0.2031, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2051, 0.1982, 0.1989, 0.1989, 0.1989],\n",
      "         [0.2620, 0.1901, 0.1537, 0.1971, 0.1971],\n",
      "         [0.1254, 0.2011, 0.2751, 0.2076, 0.1908],\n",
      "         [0.2659, 0.2026, 0.1691, 0.1989, 0.1635]],\n",
      "\n",
      "        [[0.1713, 0.2072, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1816, 0.2067, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1929, 0.2049, 0.1949, 0.2036, 0.2036],\n",
      "         [0.1874, 0.2244, 0.1933, 0.1746, 0.2203],\n",
      "         [0.2001, 0.1956, 0.1993, 0.2019, 0.2030]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2370, 0.1907, 0.1907, 0.1907, 0.1907],\n",
      "         [0.2326, 0.1421, 0.2085, 0.2085, 0.2085],\n",
      "         [0.1976, 0.2027, 0.2022, 0.1987, 0.1987],\n",
      "         [0.1661, 0.2412, 0.2314, 0.1809, 0.1804],\n",
      "         [0.2270, 0.1825, 0.1870, 0.2159, 0.1876]],\n",
      "\n",
      "        [[0.2337, 0.1916, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2031, 0.2329, 0.1880, 0.1880, 0.1880],\n",
      "         [0.2047, 0.2110, 0.1818, 0.2012, 0.2012],\n",
      "         [0.1866, 0.1745, 0.2428, 0.2024, 0.1938],\n",
      "         [0.2113, 0.2272, 0.1589, 0.1935, 0.2091]],\n",
      "\n",
      "        [[0.1489, 0.2128, 0.2128, 0.2128, 0.2128],\n",
      "         [0.1598, 0.2166, 0.2079, 0.2079, 0.2079],\n",
      "         [0.2332, 0.1613, 0.2664, 0.1695, 0.1695],\n",
      "         [0.2148, 0.1255, 0.2606, 0.2641, 0.1350],\n",
      "         [0.1948, 0.2383, 0.1812, 0.1803, 0.2054]],\n",
      "\n",
      "        [[0.1304, 0.2174, 0.2174, 0.2174, 0.2174],\n",
      "         [0.1541, 0.1873, 0.2196, 0.2196, 0.2196],\n",
      "         [0.2254, 0.2019, 0.2034, 0.1846, 0.1846],\n",
      "         [0.2179, 0.1854, 0.1875, 0.2465, 0.1626],\n",
      "         [0.1933, 0.2366, 0.2334, 0.1655, 0.1712]],\n",
      "\n",
      "        [[0.1969, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1949, 0.2037, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1979, 0.2023, 0.1985, 0.2007, 0.2007],\n",
      "         [0.1929, 0.2402, 0.1992, 0.1460, 0.2217],\n",
      "         [0.2044, 0.2561, 0.2113, 0.1535, 0.1746]],\n",
      "\n",
      "        [[0.1860, 0.2035, 0.2035, 0.2035, 0.2035],\n",
      "         [0.1886, 0.2027, 0.2029, 0.2029, 0.2029],\n",
      "         [0.2017, 0.2042, 0.1856, 0.2043, 0.2043],\n",
      "         [0.1937, 0.1908, 0.2142, 0.2104, 0.1908],\n",
      "         [0.2173, 0.2286, 0.1553, 0.1649, 0.2339]],\n",
      "\n",
      "        [[0.1474, 0.2132, 0.2132, 0.2132, 0.2132],\n",
      "         [0.2919, 0.1758, 0.1774, 0.1774, 0.1774],\n",
      "         [0.2941, 0.1609, 0.2198, 0.1626, 0.1626],\n",
      "         [0.2606, 0.1497, 0.1994, 0.2391, 0.1512],\n",
      "         [0.1985, 0.2024, 0.2004, 0.1991, 0.1997]],\n",
      "\n",
      "        [[0.2204, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2033, 0.1930, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1931, 0.2035, 0.2131, 0.1951, 0.1951],\n",
      "         [0.1657, 0.2035, 0.2435, 0.2148, 0.1725],\n",
      "         [0.2481, 0.2050, 0.1735, 0.1949, 0.1785]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2115, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.1933, 0.2056, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1671, 0.2078, 0.2454, 0.1898, 0.1898],\n",
      "         [0.2056, 0.2009, 0.1974, 0.1933, 0.2028],\n",
      "         [0.2353, 0.2010, 0.1783, 0.1547, 0.2307]],\n",
      "\n",
      "        [[0.1854, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2272, 0.1491, 0.2079, 0.2079, 0.2079],\n",
      "         [0.1746, 0.2702, 0.1722, 0.1915, 0.1915],\n",
      "         [0.1317, 0.2279, 0.1293, 0.3633, 0.1478],\n",
      "         [0.3338, 0.1338, 0.3438, 0.0615, 0.1272]],\n",
      "\n",
      "        [[0.1246, 0.2188, 0.2188, 0.2188, 0.2188],\n",
      "         [0.1800, 0.1774, 0.2142, 0.2142, 0.2142],\n",
      "         [0.2276, 0.2325, 0.1877, 0.1761, 0.1761],\n",
      "         [0.1540, 0.1461, 0.2473, 0.1631, 0.2895],\n",
      "         [0.1283, 0.1136, 0.3826, 0.1463, 0.2292]],\n",
      "\n",
      "        [[0.0708, 0.2323, 0.2323, 0.2323, 0.2323],\n",
      "         [0.1646, 0.1791, 0.2188, 0.2188, 0.2188],\n",
      "         [0.1921, 0.1968, 0.1943, 0.2084, 0.2084],\n",
      "         [0.1838, 0.1944, 0.1886, 0.2110, 0.2223],\n",
      "         [0.1738, 0.2037, 0.1870, 0.2566, 0.1788]],\n",
      "\n",
      "        [[0.1108, 0.2223, 0.2223, 0.2223, 0.2223],\n",
      "         [0.0989, 0.1380, 0.2544, 0.2544, 0.2544],\n",
      "         [0.4076, 0.2599, 0.1047, 0.1139, 0.1139],\n",
      "         [0.1556, 0.1755, 0.2237, 0.2264, 0.2188],\n",
      "         [0.0932, 0.1412, 0.3263, 0.3399, 0.0993]],\n",
      "\n",
      "        [[0.1600, 0.2100, 0.2100, 0.2100, 0.2100],\n",
      "         [0.2994, 0.2647, 0.1453, 0.1453, 0.1453],\n",
      "         [0.2166, 0.2014, 0.3002, 0.1409, 0.1409],\n",
      "         [0.2030, 0.1935, 0.2514, 0.1990, 0.1531],\n",
      "         [0.1885, 0.2120, 0.1116, 0.1980, 0.2898]],\n",
      "\n",
      "        [[0.1950, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.3057, 0.3930, 0.1005, 0.1005, 0.1005],\n",
      "         [0.2394, 0.2677, 0.2007, 0.1461, 0.1461],\n",
      "         [0.1937, 0.1901, 0.1996, 0.2060, 0.2106],\n",
      "         [0.1908, 0.1847, 0.2010, 0.2122, 0.2112]],\n",
      "\n",
      "        [[0.1853, 0.2037, 0.2037, 0.2037, 0.2037],\n",
      "         [0.1962, 0.1800, 0.2079, 0.2079, 0.2079],\n",
      "         [0.1985, 0.1787, 0.1965, 0.2131, 0.2131],\n",
      "         [0.2106, 0.1188, 0.1992, 0.1608, 0.3105],\n",
      "         [0.2570, 0.1013, 0.2347, 0.1658, 0.2411]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2319, 0.1920, 0.1920, 0.1920, 0.1920],\n",
      "         [0.1990, 0.1919, 0.2031, 0.2031, 0.2031],\n",
      "         [0.2143, 0.2663, 0.1400, 0.1897, 0.1897],\n",
      "         [0.2061, 0.3224, 0.0857, 0.2255, 0.1603],\n",
      "         [0.1841, 0.1469, 0.2869, 0.1760, 0.2061]],\n",
      "\n",
      "        [[0.1821, 0.2045, 0.2045, 0.2045, 0.2045],\n",
      "         [0.2661, 0.1991, 0.1783, 0.1783, 0.1783],\n",
      "         [0.2441, 0.1930, 0.2098, 0.1765, 0.1765],\n",
      "         [0.2409, 0.2003, 0.2138, 0.1583, 0.1867],\n",
      "         [0.2564, 0.2048, 0.2218, 0.1539, 0.1631]],\n",
      "\n",
      "        [[0.1181, 0.2205, 0.2205, 0.2205, 0.2205],\n",
      "         [0.1318, 0.1765, 0.2306, 0.2306, 0.2306],\n",
      "         [0.1762, 0.1941, 0.2055, 0.2121, 0.2121],\n",
      "         [0.0962, 0.1650, 0.2262, 0.2423, 0.2702],\n",
      "         [0.1346, 0.1818, 0.2167, 0.2251, 0.2418]],\n",
      "\n",
      "        [[0.2557, 0.1861, 0.1861, 0.1861, 0.1861],\n",
      "         [0.3109, 0.2113, 0.1593, 0.1593, 0.1593],\n",
      "         [0.2786, 0.2222, 0.1227, 0.1882, 0.1882],\n",
      "         [0.3009, 0.2114, 0.0839, 0.2405, 0.1633],\n",
      "         [0.2637, 0.2200, 0.1368, 0.2351, 0.1444]],\n",
      "\n",
      "        [[0.1589, 0.2103, 0.2103, 0.2103, 0.2103],\n",
      "         [0.1761, 0.1967, 0.2090, 0.2090, 0.2090],\n",
      "         [0.2051, 0.2017, 0.1933, 0.1999, 0.1999],\n",
      "         [0.1366, 0.1608, 0.2464, 0.2802, 0.1759],\n",
      "         [0.1682, 0.1842, 0.2336, 0.2509, 0.1631]],\n",
      "\n",
      "        [[0.2419, 0.1895, 0.1895, 0.1895, 0.1895],\n",
      "         [0.1867, 0.1570, 0.2188, 0.2188, 0.2188],\n",
      "         [0.2219, 0.2602, 0.1342, 0.1918, 0.1918],\n",
      "         [0.2250, 0.2915, 0.0992, 0.2068, 0.1775],\n",
      "         [0.2074, 0.2176, 0.1781, 0.2042, 0.1927]],\n",
      "\n",
      "        [[0.2589, 0.1853, 0.1853, 0.1853, 0.1853],\n",
      "         [0.2010, 0.1995, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1573, 0.2132, 0.2295, 0.2000, 0.2000],\n",
      "         [0.3669, 0.1334, 0.1045, 0.2302, 0.1650],\n",
      "         [0.3531, 0.1389, 0.1110, 0.2298, 0.1672]],\n",
      "\n",
      "        [[0.2345, 0.1914, 0.1914, 0.1914, 0.1914],\n",
      "         [0.2454, 0.1699, 0.1949, 0.1949, 0.1949],\n",
      "         [0.2167, 0.1872, 0.2007, 0.1977, 0.1977],\n",
      "         [0.2436, 0.1432, 0.1843, 0.2544, 0.1745],\n",
      "         [0.2071, 0.1386, 0.1678, 0.2141, 0.2725]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2651, 0.1837, 0.1837, 0.1837, 0.1837],\n",
      "         [0.1919, 0.2027, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1955, 0.2027, 0.1978, 0.2020, 0.2020],\n",
      "         [0.1830, 0.1990, 0.1880, 0.2325, 0.1975],\n",
      "         [0.2023, 0.2001, 0.2016, 0.1959, 0.2001]],\n",
      "\n",
      "        [[0.1863, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.2002, 0.2152, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1948, 0.1673, 0.2252, 0.2064, 0.2064],\n",
      "         [0.1930, 0.1643, 0.2248, 0.2127, 0.2051],\n",
      "         [0.1941, 0.2292, 0.1657, 0.1756, 0.2355]],\n",
      "\n",
      "        [[0.2384, 0.1904, 0.1904, 0.1904, 0.1904],\n",
      "         [0.1667, 0.2402, 0.1977, 0.1977, 0.1977],\n",
      "         [0.1723, 0.2232, 0.2156, 0.1944, 0.1944],\n",
      "         [0.2612, 0.1692, 0.1794, 0.1770, 0.2133],\n",
      "         [0.3282, 0.1184, 0.1358, 0.1316, 0.2859]],\n",
      "\n",
      "        [[0.1733, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.2075, 0.2045, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1597, 0.1666, 0.2957, 0.1890, 0.1890],\n",
      "         [0.1529, 0.1603, 0.3047, 0.1975, 0.1846],\n",
      "         [0.1490, 0.1579, 0.3488, 0.2043, 0.1400]],\n",
      "\n",
      "        [[0.2096, 0.1976, 0.1976, 0.1976, 0.1976],\n",
      "         [0.2178, 0.2597, 0.1742, 0.1742, 0.1742],\n",
      "         [0.1197, 0.0756, 0.3754, 0.2146, 0.2146],\n",
      "         [0.1714, 0.1487, 0.2442, 0.2302, 0.2054],\n",
      "         [0.2333, 0.2748, 0.1552, 0.1661, 0.1706]],\n",
      "\n",
      "        [[0.1989, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2059, 0.3202, 0.1580, 0.1580, 0.1580],\n",
      "         [0.1786, 0.2380, 0.2827, 0.1503, 0.1503],\n",
      "         [0.1862, 0.2212, 0.2453, 0.1795, 0.1679],\n",
      "         [0.2120, 0.1645, 0.1412, 0.2236, 0.2587]],\n",
      "\n",
      "        [[0.2074, 0.1982, 0.1982, 0.1982, 0.1982],\n",
      "         [0.2288, 0.2202, 0.1837, 0.1837, 0.1837],\n",
      "         [0.2214, 0.2158, 0.1807, 0.1911, 0.1911],\n",
      "         [0.1269, 0.1384, 0.2543, 0.2703, 0.2101],\n",
      "         [0.1894, 0.1930, 0.2203, 0.2232, 0.1740]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1995, 0.2024, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1993, 0.2020, 0.2003, 0.1992, 0.1992],\n",
      "         [0.2156, 0.1698, 0.1961, 0.2014, 0.2170],\n",
      "         [0.2145, 0.1882, 0.2037, 0.2067, 0.1870]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2454, 0.1886, 0.1886, 0.1886, 0.1886],\n",
      "         [0.1333, 0.1474, 0.2398, 0.2398, 0.2398],\n",
      "         [0.2478, 0.2313, 0.1889, 0.1660, 0.1660],\n",
      "         [0.2068, 0.2044, 0.1977, 0.1975, 0.1935],\n",
      "         [0.1914, 0.1941, 0.2021, 0.2023, 0.2102]],\n",
      "\n",
      "        [[0.1981, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1744, 0.1080, 0.2392, 0.2392, 0.2392],\n",
      "         [0.2032, 0.3302, 0.1716, 0.1475, 0.1475],\n",
      "         [0.1961, 0.3814, 0.1556, 0.1403, 0.1265],\n",
      "         [0.2017, 0.1703, 0.2140, 0.2197, 0.1944]],\n",
      "\n",
      "        [[0.1968, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1970, 0.2113, 0.1972, 0.1972, 0.1972],\n",
      "         [0.1755, 0.3012, 0.1688, 0.1773, 0.1773],\n",
      "         [0.1850, 0.2634, 0.1803, 0.1850, 0.1862],\n",
      "         [0.2168, 0.1646, 0.2212, 0.2168, 0.1806]],\n",
      "\n",
      "        [[0.1511, 0.2122, 0.2122, 0.2122, 0.2122],\n",
      "         [0.2095, 0.2206, 0.1900, 0.1900, 0.1900],\n",
      "         [0.2425, 0.3256, 0.1541, 0.1389, 0.1389],\n",
      "         [0.1985, 0.1956, 0.2031, 0.1986, 0.2042],\n",
      "         [0.2021, 0.2156, 0.1830, 0.2016, 0.1977]],\n",
      "\n",
      "        [[0.1989, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2019, 0.1927, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1887, 0.2438, 0.1892, 0.1892, 0.1892],\n",
      "         [0.1841, 0.2439, 0.1847, 0.2028, 0.1846],\n",
      "         [0.1808, 0.2312, 0.1813, 0.1968, 0.2099]],\n",
      "\n",
      "        [[0.1349, 0.2163, 0.2163, 0.2163, 0.2163],\n",
      "         [0.1602, 0.1910, 0.2163, 0.2163, 0.2163],\n",
      "         [0.4318, 0.2081, 0.1117, 0.1242, 0.1242],\n",
      "         [0.3131, 0.2022, 0.1392, 0.1971, 0.1484],\n",
      "         [0.1275, 0.1940, 0.2776, 0.1988, 0.2021]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1958, 0.2020, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2290, 0.2090, 0.1357, 0.2131, 0.2131],\n",
      "         [0.1957, 0.2003, 0.2234, 0.1813, 0.1993],\n",
      "         [0.1989, 0.2059, 0.2426, 0.1774, 0.1752]],\n",
      "\n",
      "        [[0.1844, 0.2039, 0.2039, 0.2039, 0.2039],\n",
      "         [0.2973, 0.2137, 0.1630, 0.1630, 0.1630],\n",
      "         [0.3277, 0.2187, 0.1393, 0.1571, 0.1571],\n",
      "         [0.1149, 0.1753, 0.2807, 0.1816, 0.2476],\n",
      "         [0.1591, 0.1989, 0.2552, 0.2027, 0.1842]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1928, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.5554, 0.0894, 0.1184, 0.1184, 0.1184],\n",
      "         [0.3884, 0.1201, 0.2039, 0.1438, 0.1438],\n",
      "         [0.3656, 0.1247, 0.2026, 0.1600, 0.1471],\n",
      "         [0.1126, 0.2752, 0.1839, 0.2237, 0.2046]],\n",
      "\n",
      "        [[0.2241, 0.1940, 0.1940, 0.1940, 0.1940],\n",
      "         [0.2642, 0.1944, 0.1804, 0.1804, 0.1804],\n",
      "         [0.2012, 0.1807, 0.2661, 0.1760, 0.1760],\n",
      "         [0.1785, 0.1433, 0.3165, 0.2260, 0.1358],\n",
      "         [0.1929, 0.1768, 0.2418, 0.2117, 0.1768]],\n",
      "\n",
      "        [[0.1827, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1904, 0.2254, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1682, 0.2443, 0.2339, 0.1768, 0.1768],\n",
      "         [0.1584, 0.2610, 0.2462, 0.1650, 0.1693],\n",
      "         [0.1625, 0.2531, 0.2403, 0.1684, 0.1756]],\n",
      "\n",
      "        [[0.2013, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1776, 0.3085, 0.1713, 0.1713, 0.1713],\n",
      "         [0.1676, 0.3188, 0.1922, 0.1607, 0.1607],\n",
      "         [0.1376, 0.2730, 0.1592, 0.2987, 0.1316],\n",
      "         [0.1373, 0.2719, 0.1588, 0.2974, 0.1347]],\n",
      "\n",
      "        [[0.2046, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1957, 0.2428, 0.1872, 0.1872, 0.1872],\n",
      "         [0.2164, 0.1978, 0.1450, 0.2204, 0.2204],\n",
      "         [0.1568, 0.1803, 0.2925, 0.2180, 0.1524],\n",
      "         [0.1625, 0.1813, 0.2654, 0.2106, 0.1802]],\n",
      "\n",
      "        [[0.2278, 0.1931, 0.1931, 0.1931, 0.1931],\n",
      "         [0.2051, 0.2068, 0.1960, 0.1960, 0.1960],\n",
      "         [0.2047, 0.2082, 0.2126, 0.1873, 0.1873],\n",
      "         [0.2009, 0.2260, 0.2615, 0.2033, 0.1083],\n",
      "         [0.1953, 0.1913, 0.1864, 0.1949, 0.2322]],\n",
      "\n",
      "        [[0.1902, 0.2024, 0.2024, 0.2024, 0.2024],\n",
      "         [0.1926, 0.2030, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1875, 0.2029, 0.2083, 0.2006, 0.2006],\n",
      "         [0.1642, 0.2029, 0.2177, 0.2184, 0.1967],\n",
      "         [0.1841, 0.2008, 0.2068, 0.2070, 0.2013]],\n",
      "\n",
      "        [[0.2885, 0.1779, 0.1779, 0.1779, 0.1779],\n",
      "         [0.3074, 0.2242, 0.1561, 0.1561, 0.1561],\n",
      "         [0.1816, 0.1973, 0.1870, 0.2170, 0.2170],\n",
      "         [0.2485, 0.2061, 0.2326, 0.1464, 0.1664],\n",
      "         [0.2235, 0.1967, 0.2136, 0.1556, 0.2105]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1750, 0.2062, 0.2062, 0.2062, 0.2062],\n",
      "         [0.3811, 0.1460, 0.1577, 0.1577, 0.1577],\n",
      "         [0.2618, 0.1947, 0.1448, 0.1994, 0.1994],\n",
      "         [0.3112, 0.1753, 0.0988, 0.2311, 0.1836],\n",
      "         [0.1570, 0.1912, 0.2327, 0.1739, 0.2452]],\n",
      "\n",
      "        [[0.1856, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2744, 0.3250, 0.1335, 0.1335, 0.1335],\n",
      "         [0.1608, 0.1521, 0.2798, 0.2037, 0.2037],\n",
      "         [0.1918, 0.1901, 0.2102, 0.2084, 0.1995],\n",
      "         [0.2642, 0.2828, 0.1340, 0.1429, 0.1760]],\n",
      "\n",
      "        [[0.1877, 0.2031, 0.2031, 0.2031, 0.2031],\n",
      "         [0.1974, 0.2018, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1948, 0.1823, 0.2502, 0.1864, 0.1864],\n",
      "         [0.2046, 0.2323, 0.1267, 0.2138, 0.2226],\n",
      "         [0.2073, 0.2440, 0.1120, 0.2193, 0.2175]],\n",
      "\n",
      "        [[0.2171, 0.1957, 0.1957, 0.1957, 0.1957],\n",
      "         [0.1961, 0.2816, 0.1741, 0.1741, 0.1741],\n",
      "         [0.2097, 0.2865, 0.1252, 0.1892, 0.1892],\n",
      "         [0.2191, 0.3027, 0.1285, 0.1527, 0.1970],\n",
      "         [0.2407, 0.3642, 0.1215, 0.1516, 0.1220]],\n",
      "\n",
      "        [[0.1936, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.1763, 0.1844, 0.2131, 0.2131, 0.2131],\n",
      "         [0.2054, 0.1957, 0.2639, 0.1675, 0.1675],\n",
      "         [0.2008, 0.1979, 0.2168, 0.1957, 0.1887],\n",
      "         [0.2132, 0.2182, 0.1892, 0.2220, 0.1574]],\n",
      "\n",
      "        [[0.2542, 0.1864, 0.1864, 0.1864, 0.1864],\n",
      "         [0.2199, 0.1957, 0.1948, 0.1948, 0.1948],\n",
      "         [0.1451, 0.1979, 0.2567, 0.2002, 0.2002],\n",
      "         [0.3421, 0.1873, 0.1130, 0.1746, 0.1831],\n",
      "         [0.2145, 0.2003, 0.1891, 0.1987, 0.1973]],\n",
      "\n",
      "        [[0.2129, 0.1968, 0.1968, 0.1968, 0.1968],\n",
      "         [0.1999, 0.2002, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2234, 0.1999, 0.1471, 0.2147, 0.2147],\n",
      "         [0.2749, 0.1906, 0.0696, 0.2238, 0.2412],\n",
      "         [0.2084, 0.2003, 0.1796, 0.2038, 0.2080]],\n",
      "\n",
      "        [[0.2875, 0.1781, 0.1781, 0.1781, 0.1781],\n",
      "         [0.3007, 0.1508, 0.1828, 0.1828, 0.1828],\n",
      "         [0.1819, 0.2023, 0.2231, 0.1964, 0.1964],\n",
      "         [0.3552, 0.1471, 0.0653, 0.2443, 0.1881],\n",
      "         [0.2874, 0.1773, 0.1135, 0.2341, 0.1877]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2930, 0.1767, 0.1767, 0.1767, 0.1767],\n",
      "         [0.2063, 0.2028, 0.1970, 0.1970, 0.1970],\n",
      "         [0.2147, 0.2034, 0.2120, 0.1850, 0.1850],\n",
      "         [0.1753, 0.1924, 0.1791, 0.2267, 0.2264],\n",
      "         [0.2078, 0.2003, 0.2061, 0.1877, 0.1980]],\n",
      "\n",
      "        [[0.2216, 0.1946, 0.1946, 0.1946, 0.1946],\n",
      "         [0.2463, 0.1872, 0.1888, 0.1888, 0.1888],\n",
      "         [0.1498, 0.2295, 0.1677, 0.2265, 0.2265],\n",
      "         [0.2205, 0.1786, 0.2085, 0.2126, 0.1798],\n",
      "         [0.2439, 0.1476, 0.2135, 0.2236, 0.1714]],\n",
      "\n",
      "        [[0.2399, 0.1900, 0.1900, 0.1900, 0.1900],\n",
      "         [0.3175, 0.1921, 0.1634, 0.1634, 0.1634],\n",
      "         [0.2191, 0.1980, 0.1995, 0.1917, 0.1917],\n",
      "         [0.1757, 0.2068, 0.2042, 0.1954, 0.2179],\n",
      "         [0.2179, 0.1929, 0.1947, 0.2013, 0.1931]],\n",
      "\n",
      "        [[0.2206, 0.1948, 0.1948, 0.1948, 0.1948],\n",
      "         [0.2071, 0.2061, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2057, 0.2045, 0.2059, 0.1920, 0.1920],\n",
      "         [0.1688, 0.1749, 0.1680, 0.2346, 0.2537],\n",
      "         [0.2009, 0.1980, 0.2013, 0.1761, 0.2237]],\n",
      "\n",
      "        [[0.2904, 0.1774, 0.1774, 0.1774, 0.1774],\n",
      "         [0.2591, 0.1852, 0.1852, 0.1852, 0.1852],\n",
      "         [0.2245, 0.1927, 0.1973, 0.1927, 0.1927],\n",
      "         [0.2076, 0.1952, 0.1970, 0.2050, 0.1952],\n",
      "         [0.2360, 0.1519, 0.1625, 0.2156, 0.2340]],\n",
      "\n",
      "        [[0.1967, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.2130, 0.2129, 0.1914, 0.1914, 0.1914],\n",
      "         [0.2069, 0.2067, 0.2458, 0.1703, 0.1703],\n",
      "         [0.2001, 0.2001, 0.2047, 0.2000, 0.1950],\n",
      "         [0.1931, 0.1928, 0.2416, 0.1921, 0.1805]],\n",
      "\n",
      "        [[0.1424, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1818, 0.1988, 0.2065, 0.2065, 0.2065],\n",
      "         [0.2350, 0.1966, 0.2040, 0.1822, 0.1822],\n",
      "         [0.0939, 0.2115, 0.1787, 0.2170, 0.2988],\n",
      "         [0.1886, 0.2065, 0.2027, 0.2071, 0.1951]],\n",
      "\n",
      "        [[0.2228, 0.1943, 0.1943, 0.1943, 0.1943],\n",
      "         [0.2098, 0.2482, 0.1807, 0.1807, 0.1807],\n",
      "         [0.1981, 0.1721, 0.1807, 0.2245, 0.2245],\n",
      "         [0.2054, 0.1633, 0.1768, 0.2026, 0.2519],\n",
      "         [0.2034, 0.1929, 0.1965, 0.2027, 0.2045]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1754, 0.2061, 0.2061, 0.2061, 0.2061],\n",
      "         [0.3080, 0.2945, 0.1325, 0.1325, 0.1325],\n",
      "         [0.1875, 0.1890, 0.1888, 0.2174, 0.2174],\n",
      "         [0.1962, 0.1968, 0.1967, 0.2021, 0.2082],\n",
      "         [0.1907, 0.1925, 0.1922, 0.2079, 0.2166]],\n",
      "\n",
      "        [[0.2042, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1996, 0.2107, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2024, 0.1770, 0.2002, 0.2102, 0.2102],\n",
      "         [0.2026, 0.1719, 0.1999, 0.2135, 0.2121],\n",
      "         [0.1975, 0.2023, 0.1979, 0.1960, 0.2062]],\n",
      "\n",
      "        [[0.2105, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.1792, 0.1242, 0.2322, 0.2322, 0.2322],\n",
      "         [0.2000, 0.2030, 0.2010, 0.1980, 0.1980],\n",
      "         [0.2035, 0.1936, 0.2003, 0.1918, 0.2108],\n",
      "         [0.1957, 0.2002, 0.1971, 0.2010, 0.2061]],\n",
      "\n",
      "        [[0.3441, 0.1640, 0.1640, 0.1640, 0.1640],\n",
      "         [0.1961, 0.1984, 0.2018, 0.2018, 0.2018],\n",
      "         [0.1777, 0.1912, 0.2033, 0.2139, 0.2139],\n",
      "         [0.1965, 0.1997, 0.2024, 0.1968, 0.2046],\n",
      "         [0.2365, 0.1805, 0.1439, 0.2314, 0.2077]],\n",
      "\n",
      "        [[0.2012, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1993, 0.2057, 0.1983, 0.1983, 0.1983],\n",
      "         [0.2049, 0.1669, 0.2056, 0.2113, 0.2113],\n",
      "         [0.1618, 0.2284, 0.1609, 0.2952, 0.1537],\n",
      "         [0.1741, 0.2050, 0.1737, 0.2314, 0.2158]],\n",
      "\n",
      "        [[0.1421, 0.2145, 0.2145, 0.2145, 0.2145],\n",
      "         [0.2386, 0.1858, 0.1919, 0.1919, 0.1919],\n",
      "         [0.1495, 0.2214, 0.2079, 0.2106, 0.2106],\n",
      "         [0.3180, 0.1576, 0.1764, 0.1755, 0.1724],\n",
      "         [0.1582, 0.2456, 0.2289, 0.2296, 0.1377]],\n",
      "\n",
      "        [[0.2213, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1813, 0.1728, 0.2153, 0.2153, 0.2153],\n",
      "         [0.2119, 0.2158, 0.1747, 0.1988, 0.1988],\n",
      "         [0.1965, 0.1923, 0.2474, 0.1517, 0.2121],\n",
      "         [0.2155, 0.2091, 0.2981, 0.1499, 0.1274]],\n",
      "\n",
      "        [[0.1998, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2247, 0.2061, 0.1897, 0.1897, 0.1897],\n",
      "         [0.1956, 0.1987, 0.2022, 0.2017, 0.2017],\n",
      "         [0.1966, 0.2027, 0.2096, 0.1825, 0.2086],\n",
      "         [0.1831, 0.1627, 0.1426, 0.2451, 0.2665]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2317, 0.1921, 0.1921, 0.1921, 0.1921],\n",
      "         [0.2098, 0.1916, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1754, 0.2317, 0.1833, 0.2048, 0.2048],\n",
      "         [0.2609, 0.1504, 0.2391, 0.1576, 0.1921],\n",
      "         [0.1639, 0.2262, 0.1725, 0.2201, 0.2173]],\n",
      "\n",
      "        [[0.2003, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2061, 0.1728, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1997, 0.1952, 0.2056, 0.1998, 0.1998],\n",
      "         [0.1701, 0.1292, 0.2429, 0.2865, 0.1713],\n",
      "         [0.1769, 0.1358, 0.2489, 0.2917, 0.1466]],\n",
      "\n",
      "        [[0.2118, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.1982, 0.2313, 0.1902, 0.1902, 0.1902],\n",
      "         [0.1966, 0.2043, 0.2098, 0.1946, 0.1946],\n",
      "         [0.1938, 0.2067, 0.2162, 0.1927, 0.1905],\n",
      "         [0.1971, 0.2249, 0.2466, 0.1948, 0.1365]],\n",
      "\n",
      "        [[0.1274, 0.2182, 0.2182, 0.2182, 0.2182],\n",
      "         [0.1245, 0.2471, 0.2095, 0.2095, 0.2095],\n",
      "         [0.1155, 0.2578, 0.2017, 0.2125, 0.2125],\n",
      "         [0.1808, 0.2137, 0.2031, 0.1972, 0.2053],\n",
      "         [0.1418, 0.2459, 0.2079, 0.1886, 0.2158]],\n",
      "\n",
      "        [[0.2621, 0.1845, 0.1845, 0.1845, 0.1845],\n",
      "         [0.2612, 0.1415, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1883, 0.2282, 0.1735, 0.2050, 0.2050],\n",
      "         [0.2045, 0.1803, 0.2157, 0.2061, 0.1934],\n",
      "         [0.2154, 0.1111, 0.2851, 0.2245, 0.1640]],\n",
      "\n",
      "        [[0.1872, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.2027, 0.1998, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1561, 0.1800, 0.2922, 0.1858, 0.1858],\n",
      "         [0.1238, 0.1526, 0.3099, 0.2537, 0.1599],\n",
      "         [0.2316, 0.2055, 0.1372, 0.1538, 0.2718]],\n",
      "\n",
      "        [[0.1778, 0.2056, 0.2056, 0.2056, 0.2056],\n",
      "         [0.2406, 0.2216, 0.1792, 0.1792, 0.1792],\n",
      "         [0.1756, 0.1839, 0.2263, 0.2072, 0.2072],\n",
      "         [0.2136, 0.2048, 0.1694, 0.2285, 0.1836],\n",
      "         [0.2314, 0.2112, 0.1401, 0.2678, 0.1494]],\n",
      "\n",
      "        [[0.2169, 0.1958, 0.1958, 0.1958, 0.1958],\n",
      "         [0.2052, 0.2675, 0.1758, 0.1758, 0.1758],\n",
      "         [0.2060, 0.2318, 0.1777, 0.1923, 0.1923],\n",
      "         [0.2012, 0.2022, 0.1999, 0.1961, 0.2006],\n",
      "         [0.2464, 0.2926, 0.1987, 0.1018, 0.1605]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2049, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1983, 0.3176, 0.1614, 0.1614, 0.1614],\n",
      "         [0.1965, 0.1842, 0.2149, 0.2022, 0.2022],\n",
      "         [0.1943, 0.1584, 0.2576, 0.1772, 0.2125],\n",
      "         [0.1958, 0.1761, 0.2267, 0.1866, 0.2149]],\n",
      "\n",
      "        [[0.2654, 0.1837, 0.1837, 0.1837, 0.1837],\n",
      "         [0.2010, 0.2006, 0.1995, 0.1995, 0.1995],\n",
      "         [0.2052, 0.2033, 0.1970, 0.1972, 0.1972],\n",
      "         [0.1954, 0.1969, 0.2022, 0.2035, 0.2020],\n",
      "         [0.1408, 0.1604, 0.2511, 0.2793, 0.1683]],\n",
      "\n",
      "        [[0.1879, 0.2030, 0.2030, 0.2030, 0.2030],\n",
      "         [0.1921, 0.1939, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1897, 0.1912, 0.2187, 0.2002, 0.2002],\n",
      "         [0.1874, 0.1905, 0.2508, 0.1620, 0.2092],\n",
      "         [0.2055, 0.2100, 0.3009, 0.1700, 0.1137]],\n",
      "\n",
      "        [[0.1253, 0.2187, 0.2187, 0.2187, 0.2187],\n",
      "         [0.1846, 0.1969, 0.2061, 0.2061, 0.2061],\n",
      "         [0.1022, 0.1654, 0.2674, 0.2325, 0.2325],\n",
      "         [0.1198, 0.1791, 0.2674, 0.1957, 0.2380],\n",
      "         [0.1479, 0.1911, 0.2468, 0.2023, 0.2119]],\n",
      "\n",
      "        [[0.1733, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.1821, 0.1817, 0.2121, 0.2121, 0.2121],\n",
      "         [0.2076, 0.2077, 0.2004, 0.1921, 0.1921],\n",
      "         [0.2003, 0.2002, 0.2047, 0.1847, 0.2102],\n",
      "         [0.2060, 0.2056, 0.2231, 0.1529, 0.2124]],\n",
      "\n",
      "        [[0.1823, 0.2044, 0.2044, 0.2044, 0.2044],\n",
      "         [0.2834, 0.2415, 0.1583, 0.1583, 0.1583],\n",
      "         [0.1973, 0.1983, 0.2027, 0.2009, 0.2009],\n",
      "         [0.2266, 0.2148, 0.1690, 0.2031, 0.1865],\n",
      "         [0.1446, 0.1657, 0.3058, 0.1912, 0.1927]],\n",
      "\n",
      "        [[0.2307, 0.1923, 0.1923, 0.1923, 0.1923],\n",
      "         [0.2117, 0.1951, 0.1977, 0.1977, 0.1977],\n",
      "         [0.2011, 0.1997, 0.1994, 0.1999, 0.1999],\n",
      "         [0.1334, 0.2213, 0.2442, 0.1976, 0.2034],\n",
      "         [0.1416, 0.2169, 0.2357, 0.1972, 0.2086]],\n",
      "\n",
      "        [[0.2148, 0.1963, 0.1963, 0.1963, 0.1963],\n",
      "         [0.1951, 0.1975, 0.2024, 0.2024, 0.2024],\n",
      "         [0.1870, 0.1935, 0.2050, 0.2072, 0.2072],\n",
      "         [0.1810, 0.1960, 0.2241, 0.1689, 0.2299],\n",
      "         [0.1986, 0.2186, 0.2570, 0.1826, 0.1432]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2160, 0.1960, 0.1960, 0.1960, 0.1960],\n",
      "         [0.2107, 0.0942, 0.2317, 0.2317, 0.2317],\n",
      "         [0.1785, 0.4090, 0.0888, 0.1618, 0.1618],\n",
      "         [0.2003, 0.2043, 0.1969, 0.1986, 0.1998],\n",
      "         [0.1961, 0.1371, 0.2649, 0.2270, 0.1749]],\n",
      "\n",
      "        [[0.2427, 0.1893, 0.1893, 0.1893, 0.1893],\n",
      "         [0.2364, 0.1855, 0.1927, 0.1927, 0.1927],\n",
      "         [0.2659, 0.1785, 0.1754, 0.1901, 0.1901],\n",
      "         [0.3427, 0.1695, 0.1643, 0.1342, 0.1893],\n",
      "         [0.1303, 0.2074, 0.2116, 0.2418, 0.2089]],\n",
      "\n",
      "        [[0.1684, 0.2079, 0.2079, 0.2079, 0.2079],\n",
      "         [0.0934, 0.0896, 0.2723, 0.2723, 0.2723],\n",
      "         [0.2318, 0.2341, 0.1739, 0.1801, 0.1801],\n",
      "         [0.1848, 0.1836, 0.2237, 0.1892, 0.2186],\n",
      "         [0.2251, 0.2289, 0.1368, 0.2119, 0.1973]],\n",
      "\n",
      "        [[0.2212, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1933, 0.1226, 0.2280, 0.2280, 0.2280],\n",
      "         [0.2046, 0.1899, 0.1851, 0.2102, 0.2102],\n",
      "         [0.2090, 0.1950, 0.1905, 0.1912, 0.2143],\n",
      "         [0.1953, 0.2005, 0.2024, 0.2021, 0.1998]],\n",
      "\n",
      "        [[0.1953, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2095, 0.2063, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1891, 0.1910, 0.2226, 0.1987, 0.1987],\n",
      "         [0.3165, 0.2915, 0.0860, 0.0931, 0.2130],\n",
      "         [0.2112, 0.2091, 0.1805, 0.1823, 0.2170]],\n",
      "\n",
      "        [[0.0668, 0.2333, 0.2333, 0.2333, 0.2333],\n",
      "         [0.1126, 0.2692, 0.2061, 0.2061, 0.2061],\n",
      "         [0.0595, 0.4022, 0.0906, 0.2239, 0.2239],\n",
      "         [0.1680, 0.2286, 0.1799, 0.2154, 0.2080],\n",
      "         [0.2136, 0.1868, 0.2074, 0.1917, 0.2005]],\n",
      "\n",
      "        [[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1724, 0.1092, 0.2394, 0.2394, 0.2394],\n",
      "         [0.2178, 0.2728, 0.1388, 0.1853, 0.1853],\n",
      "         [0.1960, 0.1556, 0.3109, 0.1061, 0.2313],\n",
      "         [0.2072, 0.1347, 0.4905, 0.0659, 0.1017]],\n",
      "\n",
      "        [[0.1696, 0.2076, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1926, 0.1789, 0.2095, 0.2095, 0.2095],\n",
      "         [0.2025, 0.2148, 0.2039, 0.1894, 0.1894],\n",
      "         [0.2098, 0.1941, 0.2079, 0.1590, 0.2292],\n",
      "         [0.1664, 0.1964, 0.1697, 0.3006, 0.1669]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1780, 0.2939, 0.1761, 0.1761, 0.1761],\n",
      "         [0.1611, 0.3381, 0.1839, 0.1585, 0.1585],\n",
      "         [0.2165, 0.1379, 0.1998, 0.2271, 0.2186],\n",
      "         [0.1698, 0.3184, 0.1900, 0.1589, 0.1628]],\n",
      "\n",
      "        [[0.2272, 0.1932, 0.1932, 0.1932, 0.1932],\n",
      "         [0.1985, 0.2004, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1747, 0.2296, 0.1404, 0.2277, 0.2277],\n",
      "         [0.1996, 0.2012, 0.1982, 0.1998, 0.2012],\n",
      "         [0.2069, 0.1729, 0.2388, 0.2016, 0.1797]],\n",
      "\n",
      "        [[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1987, 0.2145, 0.1956, 0.1956, 0.1956],\n",
      "         [0.1921, 0.1818, 0.2373, 0.1944, 0.1944],\n",
      "         [0.2108, 0.2382, 0.1322, 0.2132, 0.2056],\n",
      "         [0.1968, 0.1921, 0.2161, 0.1964, 0.1986]],\n",
      "\n",
      "        [[0.1694, 0.2076, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1458, 0.1958, 0.2195, 0.2195, 0.2195],\n",
      "         [0.1996, 0.2000, 0.2003, 0.2001, 0.2001],\n",
      "         [0.1490, 0.2047, 0.2680, 0.1469, 0.2314],\n",
      "         [0.1987, 0.2016, 0.2040, 0.1986, 0.1971]],\n",
      "\n",
      "        [[0.2000, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1999, 0.2005, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1926, 0.2002, 0.2225, 0.1924, 0.1924],\n",
      "         [0.1823, 0.1871, 0.2008, 0.2477, 0.1822],\n",
      "         [0.2193, 0.2143, 0.2014, 0.1675, 0.1976]],\n",
      "\n",
      "        [[0.2368, 0.1908, 0.1908, 0.1908, 0.1908],\n",
      "         [0.1998, 0.2003, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2197, 0.1808, 0.1942, 0.2026, 0.2026],\n",
      "         [0.2475, 0.1795, 0.2020, 0.1543, 0.2167],\n",
      "         [0.1251, 0.2407, 0.1893, 0.3278, 0.1170]],\n",
      "\n",
      "        [[0.1664, 0.2084, 0.2084, 0.2084, 0.2084],\n",
      "         [0.2277, 0.1659, 0.2022, 0.2022, 0.2022],\n",
      "         [0.2272, 0.1334, 0.2674, 0.1860, 0.1860],\n",
      "         [0.1563, 0.2857, 0.1300, 0.2320, 0.1960],\n",
      "         [0.1922, 0.2111, 0.1868, 0.2044, 0.2055]],\n",
      "\n",
      "        [[0.1891, 0.2027, 0.2027, 0.2027, 0.2027],\n",
      "         [0.1963, 0.1667, 0.2124, 0.2124, 0.2124],\n",
      "         [0.2012, 0.1570, 0.1884, 0.2267, 0.2267],\n",
      "         [0.1845, 0.1390, 0.1712, 0.2939, 0.2114],\n",
      "         [0.2062, 0.1583, 0.1923, 0.3189, 0.1244]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1120, 0.2220, 0.2220, 0.2220, 0.2220],\n",
      "         [0.3618, 0.2043, 0.1446, 0.1446, 0.1446],\n",
      "         [0.3680, 0.2255, 0.0709, 0.1678, 0.1678],\n",
      "         [0.2384, 0.2070, 0.1482, 0.2164, 0.1900],\n",
      "         [0.2407, 0.2157, 0.1664, 0.2233, 0.1540]],\n",
      "\n",
      "        [[0.1891, 0.2027, 0.2027, 0.2027, 0.2027],\n",
      "         [0.1617, 0.1883, 0.2166, 0.2166, 0.2166],\n",
      "         [0.3350, 0.2254, 0.1268, 0.1564, 0.1564],\n",
      "         [0.2719, 0.2103, 0.1449, 0.2069, 0.1659],\n",
      "         [0.2504, 0.2121, 0.1666, 0.2098, 0.1611]],\n",
      "\n",
      "        [[0.1726, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.2770, 0.1601, 0.1876, 0.1876, 0.1876],\n",
      "         [0.0525, 0.4033, 0.0972, 0.2235, 0.2235],\n",
      "         [0.1706, 0.2349, 0.1879, 0.1925, 0.2141],\n",
      "         [0.1494, 0.3059, 0.1856, 0.1958, 0.1633]],\n",
      "\n",
      "        [[0.1992, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2156, 0.1676, 0.2056, 0.2056, 0.2056],\n",
      "         [0.1969, 0.2592, 0.1289, 0.2075, 0.2075],\n",
      "         [0.1988, 0.4000, 0.0677, 0.1065, 0.2270],\n",
      "         [0.2349, 0.3814, 0.1113, 0.1525, 0.1199]],\n",
      "\n",
      "        [[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2062, 0.1987, 0.1983, 0.1983, 0.1983],\n",
      "         [0.1861, 0.1764, 0.2856, 0.1759, 0.1759],\n",
      "         [0.1252, 0.1085, 0.3958, 0.2629, 0.1076],\n",
      "         [0.1868, 0.1785, 0.2685, 0.2360, 0.1303]],\n",
      "\n",
      "        [[0.2041, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2066, 0.1781, 0.2051, 0.2051, 0.2051],\n",
      "         [0.1973, 0.2115, 0.1952, 0.1980, 0.1980],\n",
      "         [0.1639, 0.2516, 0.1535, 0.2637, 0.1674],\n",
      "         [0.1999, 0.2013, 0.1997, 0.2015, 0.1976]],\n",
      "\n",
      "        [[0.2073, 0.1982, 0.1982, 0.1982, 0.1982],\n",
      "         [0.1993, 0.1994, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1906, 0.1960, 0.1025, 0.2554, 0.2554],\n",
      "         [0.2235, 0.2264, 0.1683, 0.1264, 0.2555],\n",
      "         [0.1932, 0.1928, 0.2022, 0.2118, 0.2000]],\n",
      "\n",
      "        [[0.1375, 0.2156, 0.2156, 0.2156, 0.2156],\n",
      "         [0.2014, 0.1999, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2407, 0.2068, 0.1527, 0.1999, 0.1999],\n",
      "         [0.1443, 0.1750, 0.2569, 0.2411, 0.1827],\n",
      "         [0.3194, 0.2306, 0.1206, 0.1342, 0.1952]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2773, 0.1807, 0.1807, 0.1807, 0.1807],\n",
      "         [0.2479, 0.2103, 0.1806, 0.1806, 0.1806],\n",
      "         [0.2800, 0.2095, 0.1903, 0.1601, 0.1601],\n",
      "         [0.2217, 0.2084, 0.2042, 0.1690, 0.1968],\n",
      "         [0.1907, 0.1972, 0.1994, 0.2208, 0.1919]],\n",
      "\n",
      "        [[0.2112, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2576, 0.1728, 0.1898, 0.1898, 0.1898],\n",
      "         [0.2374, 0.1830, 0.1905, 0.1946, 0.1946],\n",
      "         [0.2375, 0.1918, 0.1982, 0.1709, 0.2017],\n",
      "         [0.2127, 0.1904, 0.1937, 0.1794, 0.2238]],\n",
      "\n",
      "        [[0.1195, 0.2201, 0.2201, 0.2201, 0.2201],\n",
      "         [0.1037, 0.2862, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1882, 0.2056, 0.2070, 0.1996, 0.1996],\n",
      "         [0.1233, 0.2393, 0.2517, 0.1943, 0.1914],\n",
      "         [0.1285, 0.2222, 0.2316, 0.1871, 0.2306]],\n",
      "\n",
      "        [[0.2552, 0.1862, 0.1862, 0.1862, 0.1862],\n",
      "         [0.3344, 0.1262, 0.1798, 0.1798, 0.1798],\n",
      "         [0.2908, 0.1719, 0.1212, 0.2081, 0.2081],\n",
      "         [0.3469, 0.1582, 0.0939, 0.1905, 0.2104],\n",
      "         [0.3559, 0.1464, 0.0811, 0.1807, 0.2359]],\n",
      "\n",
      "        [[0.1676, 0.2081, 0.2081, 0.2081, 0.2081],\n",
      "         [0.1561, 0.1729, 0.2237, 0.2237, 0.2237],\n",
      "         [0.2790, 0.2477, 0.1056, 0.1838, 0.1838],\n",
      "         [0.2231, 0.2169, 0.1772, 0.1807, 0.2021],\n",
      "         [0.0843, 0.1004, 0.3507, 0.3112, 0.1533]],\n",
      "\n",
      "        [[0.2501, 0.1875, 0.1875, 0.1875, 0.1875],\n",
      "         [0.2140, 0.2150, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2245, 0.2260, 0.1745, 0.1875, 0.1875],\n",
      "         [0.2365, 0.2378, 0.1916, 0.1306, 0.2035],\n",
      "         [0.1994, 0.2000, 0.1816, 0.1531, 0.2659]],\n",
      "\n",
      "        [[0.1614, 0.2096, 0.2096, 0.2096, 0.2096],\n",
      "         [0.1746, 0.2221, 0.2011, 0.2011, 0.2011],\n",
      "         [0.2107, 0.1852, 0.2134, 0.1953, 0.1953],\n",
      "         [0.1606, 0.3112, 0.1506, 0.1407, 0.2369],\n",
      "         [0.1289, 0.2351, 0.1215, 0.1143, 0.4002]],\n",
      "\n",
      "        [[0.1706, 0.2074, 0.2074, 0.2074, 0.2074],\n",
      "         [0.1874, 0.1409, 0.2239, 0.2239, 0.2239],\n",
      "         [0.1999, 0.1776, 0.1920, 0.2152, 0.2152],\n",
      "         [0.2028, 0.1453, 0.1809, 0.2214, 0.2497],\n",
      "         [0.1892, 0.1542, 0.1764, 0.1997, 0.2805]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1934, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.1845, 0.2982, 0.1725, 0.1725, 0.1725],\n",
      "         [0.2030, 0.1696, 0.2110, 0.2082, 0.2082],\n",
      "         [0.1862, 0.2158, 0.1805, 0.2350, 0.1824],\n",
      "         [0.1689, 0.2448, 0.1561, 0.3030, 0.1272]],\n",
      "\n",
      "        [[0.2082, 0.1979, 0.1979, 0.1979, 0.1979],\n",
      "         [0.1392, 0.3835, 0.1591, 0.1591, 0.1591],\n",
      "         [0.2367, 0.0397, 0.3497, 0.1870, 0.1870],\n",
      "         [0.2261, 0.1197, 0.2598, 0.1866, 0.2079],\n",
      "         [0.0424, 0.5607, 0.0241, 0.0924, 0.2804]],\n",
      "\n",
      "        [[0.2170, 0.1957, 0.1957, 0.1957, 0.1957],\n",
      "         [0.1959, 0.1770, 0.2091, 0.2091, 0.2091],\n",
      "         [0.2036, 0.2226, 0.1893, 0.1922, 0.1922],\n",
      "         [0.2006, 0.1921, 0.2078, 0.1932, 0.2063],\n",
      "         [0.1537, 0.2468, 0.1046, 0.2326, 0.2624]],\n",
      "\n",
      "        [[0.3702, 0.1575, 0.1575, 0.1575, 0.1575],\n",
      "         [0.1569, 0.1524, 0.2302, 0.2302, 0.2302],\n",
      "         [0.2451, 0.2557, 0.2174, 0.1409, 0.1409],\n",
      "         [0.1857, 0.1825, 0.1948, 0.2055, 0.2315],\n",
      "         [0.2181, 0.2263, 0.1966, 0.1750, 0.1840]],\n",
      "\n",
      "        [[0.2955, 0.1761, 0.1761, 0.1761, 0.1761],\n",
      "         [0.2339, 0.4640, 0.1007, 0.1007, 0.1007],\n",
      "         [0.1657, 0.1321, 0.2641, 0.2191, 0.2191],\n",
      "         [0.2288, 0.2859, 0.1447, 0.1667, 0.1739],\n",
      "         [0.2276, 0.3726, 0.0826, 0.1130, 0.2043]],\n",
      "\n",
      "        [[0.1755, 0.2061, 0.2061, 0.2061, 0.2061],\n",
      "         [0.1773, 0.2068, 0.2053, 0.2053, 0.2053],\n",
      "         [0.1450, 0.2000, 0.2609, 0.1971, 0.1971],\n",
      "         [0.1867, 0.2120, 0.2354, 0.1552, 0.2107],\n",
      "         [0.2144, 0.1495, 0.1110, 0.3626, 0.1625]],\n",
      "\n",
      "        [[0.1921, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1817, 0.2109, 0.2025, 0.2025, 0.2025],\n",
      "         [0.2175, 0.1791, 0.2254, 0.1889, 0.1889],\n",
      "         [0.1916, 0.2222, 0.1865, 0.1862, 0.2134],\n",
      "         [0.1795, 0.2271, 0.1719, 0.1715, 0.2499]],\n",
      "\n",
      "        [[0.1840, 0.2040, 0.2040, 0.2040, 0.2040],\n",
      "         [0.2208, 0.2221, 0.1857, 0.1857, 0.1857],\n",
      "         [0.1966, 0.1965, 0.2030, 0.2020, 0.2020],\n",
      "         [0.1952, 0.1949, 0.2069, 0.1979, 0.2050],\n",
      "         [0.2370, 0.2418, 0.1153, 0.2006, 0.2053]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2926, 0.1769, 0.1769, 0.1769, 0.1769],\n",
      "         [0.1860, 0.1917, 0.2074, 0.2074, 0.2074],\n",
      "         [0.2118, 0.2056, 0.2024, 0.1901, 0.1901],\n",
      "         [0.1958, 0.1988, 0.2004, 0.1979, 0.2070],\n",
      "         [0.1945, 0.1962, 0.1972, 0.1957, 0.2164]],\n",
      "\n",
      "        [[0.1934, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.1483, 0.4454, 0.1354, 0.1354, 0.1354],\n",
      "         [0.1993, 0.1869, 0.2131, 0.2004, 0.2004],\n",
      "         [0.2000, 0.1852, 0.2165, 0.1971, 0.2012],\n",
      "         [0.1505, 0.3988, 0.0551, 0.1805, 0.2152]],\n",
      "\n",
      "        [[0.2060, 0.1985, 0.1985, 0.1985, 0.1985],\n",
      "         [0.1785, 0.3568, 0.1549, 0.1549, 0.1549],\n",
      "         [0.2007, 0.2104, 0.1914, 0.1988, 0.1988],\n",
      "         [0.1904, 0.3074, 0.1177, 0.2120, 0.1726],\n",
      "         [0.1898, 0.2666, 0.1349, 0.2048, 0.2039]],\n",
      "\n",
      "        [[0.2355, 0.1911, 0.1911, 0.1911, 0.1911],\n",
      "         [0.2604, 0.3503, 0.1298, 0.1298, 0.1298],\n",
      "         [0.2299, 0.2627, 0.1711, 0.1682, 0.1682],\n",
      "         [0.2309, 0.2623, 0.1741, 0.1615, 0.1712],\n",
      "         [0.2272, 0.2695, 0.1555, 0.1406, 0.2073]],\n",
      "\n",
      "        [[0.2111, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2279, 0.2490, 0.1743, 0.1743, 0.1743],\n",
      "         [0.1565, 0.1442, 0.2980, 0.2007, 0.2007],\n",
      "         [0.1866, 0.1828, 0.2191, 0.2131, 0.1985],\n",
      "         [0.2337, 0.2508, 0.1341, 0.1477, 0.2336]],\n",
      "\n",
      "        [[0.2818, 0.1795, 0.1795, 0.1795, 0.1795],\n",
      "         [0.1212, 0.1810, 0.2326, 0.2326, 0.2326],\n",
      "         [0.2124, 0.2010, 0.1983, 0.1942, 0.1942],\n",
      "         [0.2815, 0.1970, 0.1803, 0.1837, 0.1575],\n",
      "         [0.2273, 0.1973, 0.1905, 0.1919, 0.1931]],\n",
      "\n",
      "        [[0.1726, 0.2068, 0.2068, 0.2068, 0.2068],\n",
      "         [0.1691, 0.1190, 0.2373, 0.2373, 0.2373],\n",
      "         [0.1730, 0.1482, 0.2770, 0.2009, 0.2009],\n",
      "         [0.2023, 0.2054, 0.1933, 0.1995, 0.1994],\n",
      "         [0.2096, 0.2509, 0.1213, 0.1775, 0.2407]],\n",
      "\n",
      "        [[0.3837, 0.1541, 0.1541, 0.1541, 0.1541],\n",
      "         [0.2833, 0.2577, 0.1530, 0.1530, 0.1530],\n",
      "         [0.1842, 0.1870, 0.2233, 0.2028, 0.2028],\n",
      "         [0.2235, 0.2176, 0.1574, 0.2137, 0.1877],\n",
      "         [0.2665, 0.2451, 0.0895, 0.2317, 0.1673]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1864, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.1896, 0.1799, 0.2102, 0.2102, 0.2102],\n",
      "         [0.1995, 0.1991, 0.2006, 0.2003, 0.2003],\n",
      "         [0.2193, 0.2306, 0.1915, 0.1598, 0.1988],\n",
      "         [0.1814, 0.1706, 0.2144, 0.2681, 0.1655]],\n",
      "\n",
      "        [[0.1775, 0.2056, 0.2056, 0.2056, 0.2056],\n",
      "         [0.1674, 0.2577, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2057, 0.1438, 0.2828, 0.1839, 0.1839],\n",
      "         [0.2057, 0.1129, 0.3505, 0.1605, 0.1704],\n",
      "         [0.1779, 0.2727, 0.1216, 0.2123, 0.2154]],\n",
      "\n",
      "        [[0.2033, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2106, 0.1840, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2007, 0.1852, 0.2227, 0.1957, 0.1957],\n",
      "         [0.2078, 0.1511, 0.3133, 0.1399, 0.1879],\n",
      "         [0.1994, 0.2075, 0.1895, 0.2095, 0.1941]],\n",
      "\n",
      "        [[0.2093, 0.1977, 0.1977, 0.1977, 0.1977],\n",
      "         [0.2231, 0.1916, 0.1951, 0.1951, 0.1951],\n",
      "         [0.2046, 0.1987, 0.1978, 0.1994, 0.1994],\n",
      "         [0.2178, 0.1762, 0.1707, 0.2544, 0.1808],\n",
      "         [0.2029, 0.1942, 0.1929, 0.2096, 0.2004]],\n",
      "\n",
      "        [[0.1817, 0.2046, 0.2046, 0.2046, 0.2046],\n",
      "         [0.2061, 0.2070, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2021, 0.2022, 0.1961, 0.1998, 0.1998],\n",
      "         [0.2081, 0.2085, 0.1947, 0.1855, 0.2031],\n",
      "         [0.1727, 0.1717, 0.2107, 0.2435, 0.2015]],\n",
      "\n",
      "        [[0.2055, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.1994, 0.1999, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2199, 0.2018, 0.2012, 0.1885, 0.1885],\n",
      "         [0.2148, 0.2046, 0.2042, 0.1795, 0.1968],\n",
      "         [0.1820, 0.1946, 0.1950, 0.2327, 0.1958]],\n",
      "\n",
      "        [[0.2127, 0.1968, 0.1968, 0.1968, 0.1968],\n",
      "         [0.2233, 0.1427, 0.2113, 0.2113, 0.2113],\n",
      "         [0.2025, 0.1945, 0.2000, 0.2015, 0.2015],\n",
      "         [0.2210, 0.1476, 0.1948, 0.2264, 0.2103],\n",
      "         [0.2263, 0.1519, 0.1998, 0.2317, 0.1902]],\n",
      "\n",
      "        [[0.2026, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.1921, 0.1788, 0.2097, 0.2097, 0.2097],\n",
      "         [0.1803, 0.1714, 0.2650, 0.1917, 0.1917],\n",
      "         [0.1705, 0.1576, 0.3125, 0.1717, 0.1877],\n",
      "         [0.1749, 0.1679, 0.2397, 0.1756, 0.2419]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1882, 0.2029, 0.2029, 0.2029, 0.2029],\n",
      "         [0.0893, 0.0985, 0.2708, 0.2708, 0.2708],\n",
      "         [0.0378, 0.0443, 0.4597, 0.2291, 0.2291],\n",
      "         [0.1808, 0.1837, 0.2322, 0.1867, 0.2166],\n",
      "         [0.2412, 0.2340, 0.1498, 0.2268, 0.1482]],\n",
      "\n",
      "        [[0.2098, 0.1976, 0.1976, 0.1976, 0.1976],\n",
      "         [0.1022, 0.1190, 0.2596, 0.2596, 0.2596],\n",
      "         [0.2444, 0.2295, 0.1936, 0.1663, 0.1663],\n",
      "         [0.1379, 0.1602, 0.2399, 0.1174, 0.3445],\n",
      "         [0.1224, 0.1554, 0.2959, 0.0947, 0.3316]],\n",
      "\n",
      "        [[0.2232, 0.1942, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2593, 0.0930, 0.2159, 0.2159, 0.2159],\n",
      "         [0.3539, 0.0302, 0.1597, 0.2281, 0.2281],\n",
      "         [0.2570, 0.1099, 0.1952, 0.2171, 0.2208],\n",
      "         [0.2868, 0.0774, 0.1877, 0.2212, 0.2269]],\n",
      "\n",
      "        [[0.2070, 0.1983, 0.1983, 0.1983, 0.1983],\n",
      "         [0.1872, 0.1804, 0.2108, 0.2108, 0.2108],\n",
      "         [0.1309, 0.1088, 0.2846, 0.2378, 0.2378],\n",
      "         [0.1496, 0.1310, 0.2611, 0.2287, 0.2295],\n",
      "         [0.0728, 0.0485, 0.4012, 0.2672, 0.2104]],\n",
      "\n",
      "        [[0.1424, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.0997, 0.1078, 0.2642, 0.2642, 0.2642],\n",
      "         [0.2075, 0.2056, 0.2185, 0.1842, 0.1842],\n",
      "         [0.1723, 0.1813, 0.1308, 0.1903, 0.3254],\n",
      "         [0.1983, 0.2084, 0.1513, 0.2186, 0.2233]],\n",
      "\n",
      "        [[0.2782, 0.1804, 0.1804, 0.1804, 0.1804],\n",
      "         [0.2116, 0.2716, 0.1723, 0.1723, 0.1723],\n",
      "         [0.1834, 0.1203, 0.1775, 0.2594, 0.2594],\n",
      "         [0.1892, 0.0733, 0.1759, 0.1485, 0.4131],\n",
      "         [0.1797, 0.0995, 0.1717, 0.1545, 0.3945]],\n",
      "\n",
      "        [[0.3032, 0.1742, 0.1742, 0.1742, 0.1742],\n",
      "         [0.1834, 0.1848, 0.2106, 0.2106, 0.2106],\n",
      "         [0.1268, 0.1323, 0.1980, 0.2715, 0.2715],\n",
      "         [0.1951, 0.1954, 0.1988, 0.2092, 0.2015],\n",
      "         [0.1480, 0.1505, 0.1764, 0.2839, 0.2413]],\n",
      "\n",
      "        [[0.1549, 0.2113, 0.2113, 0.2113, 0.2113],\n",
      "         [0.1034, 0.1089, 0.2626, 0.2626, 0.2626],\n",
      "         [0.3958, 0.3596, 0.1035, 0.0706, 0.0706],\n",
      "         [0.3210, 0.3027, 0.1414, 0.1230, 0.1119],\n",
      "         [0.2053, 0.2046, 0.1956, 0.1940, 0.2006]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.5367, 0.1158, 0.1158, 0.1158, 0.1158],\n",
      "         [0.1860, 0.2052, 0.2030, 0.2030, 0.2030],\n",
      "         [0.2514, 0.1745, 0.2107, 0.1817, 0.1817],\n",
      "         [0.1511, 0.2160, 0.1796, 0.2457, 0.2076],\n",
      "         [0.0862, 0.2510, 0.1445, 0.3689, 0.1493]],\n",
      "\n",
      "        [[0.2901, 0.1775, 0.1775, 0.1775, 0.1775],\n",
      "         [0.2210, 0.1958, 0.1944, 0.1944, 0.1944],\n",
      "         [0.2142, 0.1519, 0.3365, 0.1487, 0.1487],\n",
      "         [0.2090, 0.1543, 0.3115, 0.1738, 0.1514],\n",
      "         [0.1284, 0.2638, 0.0498, 0.1989, 0.3592]],\n",
      "\n",
      "        [[0.1019, 0.2245, 0.2245, 0.2245, 0.2245],\n",
      "         [0.3339, 0.1880, 0.1594, 0.1594, 0.1594],\n",
      "         [0.1519, 0.1980, 0.2228, 0.2137, 0.2137],\n",
      "         [0.2091, 0.2017, 0.1985, 0.1912, 0.1996],\n",
      "         [0.0768, 0.1418, 0.1862, 0.3504, 0.2448]],\n",
      "\n",
      "        [[0.2161, 0.1960, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1193, 0.3542, 0.1755, 0.1755, 0.1755],\n",
      "         [0.3908, 0.0538, 0.1684, 0.1935, 0.1935],\n",
      "         [0.2113, 0.1877, 0.2009, 0.1974, 0.2026],\n",
      "         [0.2567, 0.1580, 0.2089, 0.1944, 0.1820]],\n",
      "\n",
      "        [[0.1967, 0.2008, 0.2008, 0.2008, 0.2008],\n",
      "         [0.1906, 0.2199, 0.1965, 0.1965, 0.1965],\n",
      "         [0.2103, 0.1812, 0.2011, 0.2037, 0.2037],\n",
      "         [0.1986, 0.2081, 0.2014, 0.1913, 0.2006],\n",
      "         [0.2070, 0.2227, 0.2116, 0.1951, 0.1636]],\n",
      "\n",
      "        [[0.4146, 0.1464, 0.1464, 0.1464, 0.1464],\n",
      "         [0.4454, 0.1859, 0.1229, 0.1229, 0.1229],\n",
      "         [0.2905, 0.1490, 0.3434, 0.1086, 0.1086],\n",
      "         [0.2363, 0.1755, 0.2545, 0.1813, 0.1524],\n",
      "         [0.1447, 0.2218, 0.1300, 0.2117, 0.2918]],\n",
      "\n",
      "        [[0.1787, 0.2053, 0.2053, 0.2053, 0.2053],\n",
      "         [0.1907, 0.0643, 0.2483, 0.2483, 0.2483],\n",
      "         [0.1623, 0.3692, 0.2027, 0.1329, 0.1329],\n",
      "         [0.2008, 0.1949, 0.1992, 0.2030, 0.2022],\n",
      "         [0.1939, 0.0703, 0.1474, 0.2827, 0.3057]],\n",
      "\n",
      "        [[0.3458, 0.1635, 0.1635, 0.1635, 0.1635],\n",
      "         [0.2682, 0.2002, 0.1772, 0.1772, 0.1772],\n",
      "         [0.1881, 0.2031, 0.1895, 0.2097, 0.2097],\n",
      "         [0.2507, 0.1916, 0.2446, 0.1417, 0.1713],\n",
      "         [0.1865, 0.2014, 0.1879, 0.2196, 0.2046]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1912, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1967, 0.2004, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1927, 0.2055, 0.1867, 0.2075, 0.2075],\n",
      "         [0.1963, 0.2023, 0.1934, 0.2047, 0.2032],\n",
      "         [0.2161, 0.1750, 0.2402, 0.1606, 0.2081]],\n",
      "\n",
      "        [[0.2111, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2183, 0.1851, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1861, 0.2031, 0.2198, 0.1955, 0.1955],\n",
      "         [0.1983, 0.2001, 0.2017, 0.2005, 0.1993],\n",
      "         [0.1574, 0.1910, 0.2272, 0.1996, 0.2248]],\n",
      "\n",
      "        [[0.1908, 0.2023, 0.2023, 0.2023, 0.2023],\n",
      "         [0.2050, 0.2381, 0.1856, 0.1856, 0.1856],\n",
      "         [0.1889, 0.2488, 0.2475, 0.1574, 0.1574],\n",
      "         [0.1902, 0.2244, 0.2237, 0.1912, 0.1705],\n",
      "         [0.1778, 0.2152, 0.2144, 0.1789, 0.2137]],\n",
      "\n",
      "        [[0.2869, 0.1783, 0.1783, 0.1783, 0.1783],\n",
      "         [0.1818, 0.1843, 0.2113, 0.2113, 0.2113],\n",
      "         [0.2161, 0.2125, 0.2093, 0.1811, 0.1811],\n",
      "         [0.2018, 0.2006, 0.1995, 0.2086, 0.1896],\n",
      "         [0.1962, 0.1933, 0.1907, 0.2132, 0.2066]],\n",
      "\n",
      "        [[0.2038, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1998, 0.1987, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2015, 0.2583, 0.1913, 0.1745, 0.1745],\n",
      "         [0.1985, 0.2141, 0.1954, 0.2020, 0.1900],\n",
      "         [0.1783, 0.2114, 0.1721, 0.1854, 0.2528]],\n",
      "\n",
      "        [[0.1991, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2303, 0.1870, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2089, 0.1927, 0.2073, 0.1956, 0.1956],\n",
      "         [0.1732, 0.2265, 0.1777, 0.2068, 0.2157],\n",
      "         [0.2271, 0.1582, 0.2192, 0.1788, 0.2168]],\n",
      "\n",
      "        [[0.1924, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1955, 0.1859, 0.2062, 0.2062, 0.2062],\n",
      "         [0.1931, 0.1835, 0.2153, 0.2040, 0.2040],\n",
      "         [0.2036, 0.2062, 0.1984, 0.1908, 0.2010],\n",
      "         [0.1929, 0.1893, 0.2008, 0.2133, 0.2037]],\n",
      "\n",
      "        [[0.1862, 0.2035, 0.2035, 0.2035, 0.2035],\n",
      "         [0.2114, 0.2183, 0.1901, 0.1901, 0.1901],\n",
      "         [0.2139, 0.2421, 0.2605, 0.1418, 0.1418],\n",
      "         [0.2026, 0.2164, 0.2250, 0.1934, 0.1626],\n",
      "         [0.1780, 0.1948, 0.2055, 0.1671, 0.2547]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2468, 0.1883, 0.1883, 0.1883, 0.1883],\n",
      "         [0.1640, 0.1884, 0.2159, 0.2159, 0.2159],\n",
      "         [0.1675, 0.1880, 0.2236, 0.2105, 0.2105],\n",
      "         [0.3073, 0.2310, 0.1503, 0.1369, 0.1745],\n",
      "         [0.0729, 0.1205, 0.2573, 0.3030, 0.2462]],\n",
      "\n",
      "        [[0.1421, 0.2145, 0.2145, 0.2145, 0.2145],\n",
      "         [0.4107, 0.1187, 0.1569, 0.1569, 0.1569],\n",
      "         [0.2654, 0.1680, 0.1944, 0.1861, 0.1861],\n",
      "         [0.2646, 0.1682, 0.1944, 0.1865, 0.1863],\n",
      "         [0.0928, 0.2485, 0.1814, 0.1986, 0.2787]],\n",
      "\n",
      "        [[0.3292, 0.1677, 0.1677, 0.1677, 0.1677],\n",
      "         [0.1435, 0.2381, 0.2061, 0.2061, 0.2061],\n",
      "         [0.2622, 0.1668, 0.1914, 0.1898, 0.1898],\n",
      "         [0.1781, 0.2139, 0.2023, 0.2027, 0.2030],\n",
      "         [0.2972, 0.1304, 0.1674, 0.1659, 0.2391]],\n",
      "\n",
      "        [[0.1696, 0.2076, 0.2076, 0.2076, 0.2076],\n",
      "         [0.2392, 0.2042, 0.1855, 0.1855, 0.1855],\n",
      "         [0.1283, 0.1439, 0.4192, 0.1543, 0.1543],\n",
      "         [0.2529, 0.2348, 0.1179, 0.1700, 0.2244],\n",
      "         [0.1168, 0.1305, 0.3649, 0.2114, 0.1764]],\n",
      "\n",
      "        [[0.2234, 0.1941, 0.1941, 0.1941, 0.1941],\n",
      "         [0.1970, 0.1765, 0.2088, 0.2088, 0.2088],\n",
      "         [0.2061, 0.2202, 0.1758, 0.1990, 0.1990],\n",
      "         [0.2436, 0.3651, 0.0925, 0.1023, 0.1966],\n",
      "         [0.2264, 0.3421, 0.0843, 0.0935, 0.2536]],\n",
      "\n",
      "        [[0.2156, 0.1961, 0.1961, 0.1961, 0.1961],\n",
      "         [0.1991, 0.1941, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1999, 0.1967, 0.1998, 0.2018, 0.2018],\n",
      "         [0.1876, 0.3682, 0.1920, 0.1288, 0.1235],\n",
      "         [0.1712, 0.0638, 0.1655, 0.2970, 0.3025]],\n",
      "\n",
      "        [[0.1425, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1942, 0.1954, 0.2035, 0.2035, 0.2035],\n",
      "         [0.1153, 0.1257, 0.3146, 0.2222, 0.2222],\n",
      "         [0.2448, 0.2373, 0.1696, 0.1558, 0.1926],\n",
      "         [0.2015, 0.2012, 0.1983, 0.1976, 0.2014]],\n",
      "\n",
      "        [[0.2218, 0.1945, 0.1945, 0.1945, 0.1945],\n",
      "         [0.2111, 0.1889, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1787, 0.2087, 0.2270, 0.1928, 0.1928],\n",
      "         [0.2073, 0.1982, 0.1934, 0.1984, 0.2028],\n",
      "         [0.1940, 0.2015, 0.2058, 0.2013, 0.1974]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3461, 0.1635, 0.1635, 0.1635, 0.1635],\n",
      "         [0.2713, 0.2857, 0.1477, 0.1477, 0.1477],\n",
      "         [0.2122, 0.2209, 0.3030, 0.1320, 0.1320],\n",
      "         [0.1300, 0.1225, 0.0771, 0.4096, 0.2608],\n",
      "         [0.2139, 0.2226, 0.3044, 0.0985, 0.1607]],\n",
      "\n",
      "        [[0.2529, 0.1868, 0.1868, 0.1868, 0.1868],\n",
      "         [0.2462, 0.2264, 0.1758, 0.1758, 0.1758],\n",
      "         [0.2324, 0.2127, 0.2293, 0.1628, 0.1628],\n",
      "         [0.1944, 0.2010, 0.1954, 0.1870, 0.2222],\n",
      "         [0.2139, 0.1884, 0.2098, 0.2485, 0.1394]],\n",
      "\n",
      "        [[0.2021, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1939, 0.2465, 0.1865, 0.1865, 0.1865],\n",
      "         [0.1801, 0.2484, 0.2296, 0.1710, 0.1710],\n",
      "         [0.2119, 0.1260, 0.1431, 0.2885, 0.2305],\n",
      "         [0.1798, 0.2817, 0.2524, 0.1377, 0.1483]],\n",
      "\n",
      "        [[0.2133, 0.1967, 0.1967, 0.1967, 0.1967],\n",
      "         [0.2005, 0.2284, 0.1904, 0.1904, 0.1904],\n",
      "         [0.2016, 0.2201, 0.1892, 0.1946, 0.1946],\n",
      "         [0.1992, 0.2409, 0.1737, 0.2016, 0.1846],\n",
      "         [0.1556, 0.2204, 0.1211, 0.1592, 0.3437]],\n",
      "\n",
      "        [[0.1948, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2006, 0.2126, 0.1956, 0.1956, 0.1956],\n",
      "         [0.1997, 0.2004, 0.2011, 0.1994, 0.1994],\n",
      "         [0.1924, 0.2060, 0.2216, 0.1932, 0.1868],\n",
      "         [0.1717, 0.1875, 0.2061, 0.1725, 0.2622]],\n",
      "\n",
      "        [[0.2628, 0.1843, 0.1843, 0.1843, 0.1843],\n",
      "         [0.2151, 0.2743, 0.1702, 0.1702, 0.1702],\n",
      "         [0.1999, 0.1894, 0.1896, 0.2105, 0.2105],\n",
      "         [0.2055, 0.1736, 0.1741, 0.2051, 0.2417],\n",
      "         [0.1637, 0.2776, 0.2750, 0.1645, 0.1192]],\n",
      "\n",
      "        [[0.1873, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1987, 0.1384, 0.2210, 0.2210, 0.2210],\n",
      "         [0.2009, 0.1968, 0.1979, 0.2022, 0.2022],\n",
      "         [0.2016, 0.1942, 0.1962, 0.2040, 0.2039],\n",
      "         [0.2053, 0.1944, 0.1974, 0.2089, 0.1940]],\n",
      "\n",
      "        [[0.2995, 0.1751, 0.1751, 0.1751, 0.1751],\n",
      "         [0.1998, 0.1998, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2136, 0.2163, 0.1952, 0.1875, 0.1875],\n",
      "         [0.1719, 0.1676, 0.2059, 0.2315, 0.2231],\n",
      "         [0.2536, 0.2668, 0.1757, 0.1385, 0.1653]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2121, 0.1970, 0.1970, 0.1970, 0.1970],\n",
      "         [0.2070, 0.1864, 0.2022, 0.2022, 0.2022],\n",
      "         [0.2147, 0.1642, 0.2167, 0.2022, 0.2022],\n",
      "         [0.1241, 0.2395, 0.1213, 0.3713, 0.1438],\n",
      "         [0.2022, 0.1103, 0.2064, 0.0737, 0.4073]],\n",
      "\n",
      "        [[0.1858, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.1990, 0.1808, 0.2068, 0.2068, 0.2068],\n",
      "         [0.1987, 0.1924, 0.2065, 0.2012, 0.2012],\n",
      "         [0.2017, 0.2884, 0.1302, 0.2047, 0.1750],\n",
      "         [0.2006, 0.2119, 0.1876, 0.2011, 0.1989]],\n",
      "\n",
      "        [[0.2157, 0.1961, 0.1961, 0.1961, 0.1961],\n",
      "         [0.2842, 0.2349, 0.1603, 0.1603, 0.1603],\n",
      "         [0.2342, 0.2198, 0.1587, 0.1936, 0.1936],\n",
      "         [0.2970, 0.2166, 0.0427, 0.3286, 0.1151],\n",
      "         [0.2227, 0.2037, 0.1288, 0.2292, 0.2156]],\n",
      "\n",
      "        [[0.3824, 0.1544, 0.1544, 0.1544, 0.1544],\n",
      "         [0.2669, 0.2754, 0.1526, 0.1526, 0.1526],\n",
      "         [0.1835, 0.1820, 0.2122, 0.2112, 0.2112],\n",
      "         [0.2405, 0.2465, 0.1540, 0.2028, 0.1562],\n",
      "         [0.2236, 0.2274, 0.1645, 0.1989, 0.1856]],\n",
      "\n",
      "        [[0.3137, 0.1716, 0.1716, 0.1716, 0.1716],\n",
      "         [0.2144, 0.2129, 0.1909, 0.1909, 0.1909],\n",
      "         [0.2344, 0.2315, 0.1511, 0.1915, 0.1915],\n",
      "         [0.2485, 0.2442, 0.1330, 0.1879, 0.1864],\n",
      "         [0.2662, 0.2586, 0.0953, 0.1681, 0.2119]],\n",
      "\n",
      "        [[0.2652, 0.1837, 0.1837, 0.1837, 0.1837],\n",
      "         [0.2026, 0.1996, 0.1993, 0.1993, 0.1993],\n",
      "         [0.3376, 0.1739, 0.1651, 0.1617, 0.1617],\n",
      "         [0.2731, 0.1733, 0.1673, 0.2214, 0.1649],\n",
      "         [0.1649, 0.2118, 0.2160, 0.1851, 0.2223]],\n",
      "\n",
      "        [[0.5215, 0.1196, 0.1196, 0.1196, 0.1196],\n",
      "         [0.3704, 0.2033, 0.1421, 0.1421, 0.1421],\n",
      "         [0.1933, 0.1984, 0.2052, 0.2015, 0.2015],\n",
      "         [0.2525, 0.1992, 0.1463, 0.2291, 0.1729],\n",
      "         [0.2918, 0.1856, 0.1029, 0.2425, 0.1772]],\n",
      "\n",
      "        [[0.2103, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.2744, 0.3162, 0.1365, 0.1365, 0.1365],\n",
      "         [0.2278, 0.2374, 0.1627, 0.1860, 0.1860],\n",
      "         [0.2110, 0.2410, 0.0711, 0.3673, 0.1096],\n",
      "         [0.1941, 0.2199, 0.0703, 0.3258, 0.1899]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2066, 0.1984, 0.1984, 0.1984, 0.1984],\n",
      "         [0.1698, 0.1951, 0.2117, 0.2117, 0.2117],\n",
      "         [0.1729, 0.2189, 0.1058, 0.2512, 0.2512],\n",
      "         [0.1775, 0.1941, 0.1473, 0.2766, 0.2045],\n",
      "         [0.1919, 0.2014, 0.1736, 0.2437, 0.1894]],\n",
      "\n",
      "        [[0.1845, 0.2039, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1891, 0.2107, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2683, 0.1083, 0.2883, 0.1675, 0.1675],\n",
      "         [0.2338, 0.1369, 0.2439, 0.2085, 0.1770],\n",
      "         [0.2259, 0.1690, 0.2312, 0.2123, 0.1616]],\n",
      "\n",
      "        [[0.1825, 0.2044, 0.2044, 0.2044, 0.2044],\n",
      "         [0.2108, 0.2161, 0.1910, 0.1910, 0.1910],\n",
      "         [0.1782, 0.1675, 0.1971, 0.2286, 0.2286],\n",
      "         [0.1970, 0.1923, 0.2049, 0.1887, 0.2171],\n",
      "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000]],\n",
      "\n",
      "        [[0.2118, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.1992, 0.1998, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1880, 0.1991, 0.1913, 0.2108, 0.2108],\n",
      "         [0.1806, 0.1913, 0.1838, 0.2415, 0.2028],\n",
      "         [0.2056, 0.2136, 0.2079, 0.2491, 0.1238]],\n",
      "\n",
      "        [[0.2144, 0.1964, 0.1964, 0.1964, 0.1964],\n",
      "         [0.2121, 0.2012, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2003, 0.1956, 0.2179, 0.1931, 0.1931],\n",
      "         [0.1743, 0.1487, 0.3084, 0.2322, 0.1364],\n",
      "         [0.1995, 0.1894, 0.2404, 0.2191, 0.1515]],\n",
      "\n",
      "        [[0.2405, 0.1899, 0.1899, 0.1899, 0.1899],\n",
      "         [0.2279, 0.1775, 0.1982, 0.1982, 0.1982],\n",
      "         [0.1855, 0.2254, 0.1755, 0.2068, 0.2068],\n",
      "         [0.1575, 0.2713, 0.1349, 0.2228, 0.2134],\n",
      "         [0.1949, 0.1503, 0.2098, 0.1651, 0.2799]],\n",
      "\n",
      "        [[0.1900, 0.2025, 0.2025, 0.2025, 0.2025],\n",
      "         [0.1826, 0.1812, 0.2121, 0.2121, 0.2121],\n",
      "         [0.2284, 0.2309, 0.1734, 0.1837, 0.1837],\n",
      "         [0.2084, 0.2091, 0.1916, 0.1960, 0.1950],\n",
      "         [0.1886, 0.1883, 0.1967, 0.1945, 0.2319]],\n",
      "\n",
      "        [[0.2036, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1994, 0.1882, 0.2041, 0.2041, 0.2041],\n",
      "         [0.1973, 0.1826, 0.2128, 0.2036, 0.2036],\n",
      "         [0.1985, 0.2255, 0.1752, 0.2124, 0.1885],\n",
      "         [0.1999, 0.1920, 0.2079, 0.1956, 0.2046]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1591, 0.2102, 0.2102, 0.2102, 0.2102],\n",
      "         [0.2667, 0.1292, 0.2013, 0.2013, 0.2013],\n",
      "         [0.1865, 0.2268, 0.1843, 0.2012, 0.2012],\n",
      "         [0.1961, 0.2037, 0.1956, 0.2057, 0.1990],\n",
      "         [0.1852, 0.2141, 0.1836, 0.2224, 0.1946]],\n",
      "\n",
      "        [[0.2033, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1815, 0.2645, 0.1847, 0.1847, 0.1847],\n",
      "         [0.1961, 0.2388, 0.1695, 0.1978, 0.1978],\n",
      "         [0.2032, 0.2445, 0.1772, 0.1702, 0.2049],\n",
      "         [0.1795, 0.3213, 0.1167, 0.1027, 0.2799]],\n",
      "\n",
      "        [[0.1918, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1918, 0.1687, 0.2132, 0.2132, 0.2132],\n",
      "         [0.1957, 0.1818, 0.2067, 0.2079, 0.2079],\n",
      "         [0.1907, 0.1800, 0.1991, 0.2302, 0.2000],\n",
      "         [0.1830, 0.1716, 0.1920, 0.2256, 0.2278]],\n",
      "\n",
      "        [[0.1832, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2230, 0.1903, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2183, 0.1807, 0.2275, 0.1867, 0.1867],\n",
      "         [0.2023, 0.1859, 0.2061, 0.2169, 0.1887],\n",
      "         [0.2035, 0.1933, 0.2058, 0.2123, 0.1850]],\n",
      "\n",
      "        [[0.1779, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.1558, 0.2452, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2119, 0.1864, 0.2067, 0.1975, 0.1975],\n",
      "         [0.2076, 0.1937, 0.2048, 0.1940, 0.1999],\n",
      "         [0.1778, 0.2441, 0.1889, 0.2428, 0.1464]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2235, 0.2662, 0.1701, 0.1701, 0.1701],\n",
      "         [0.2043, 0.1961, 0.1639, 0.2178, 0.2178],\n",
      "         [0.2043, 0.1893, 0.1355, 0.2408, 0.2302],\n",
      "         [0.1998, 0.2009, 0.2057, 0.1975, 0.1961]],\n",
      "\n",
      "        [[0.1921, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1924, 0.2031, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1998, 0.2004, 0.1991, 0.2003, 0.2003],\n",
      "         [0.2023, 0.2034, 0.2012, 0.1898, 0.2033],\n",
      "         [0.2047, 0.2068, 0.2027, 0.1825, 0.2033]],\n",
      "\n",
      "        [[0.1822, 0.2044, 0.2044, 0.2044, 0.2044],\n",
      "         [0.1967, 0.1745, 0.2096, 0.2096, 0.2096],\n",
      "         [0.2014, 0.1918, 0.1935, 0.2067, 0.2067],\n",
      "         [0.2012, 0.2365, 0.2298, 0.1477, 0.1848],\n",
      "         [0.1960, 0.1806, 0.1833, 0.2293, 0.2108]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1986, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1949, 0.2195, 0.1952, 0.1952, 0.1952],\n",
      "         [0.1937, 0.2229, 0.1952, 0.1941, 0.1941],\n",
      "         [0.1808, 0.2158, 0.1825, 0.2396, 0.1813],\n",
      "         [0.1990, 0.1773, 0.1978, 0.1656, 0.2603]],\n",
      "\n",
      "        [[0.1803, 0.2049, 0.2049, 0.2049, 0.2049],\n",
      "         [0.1907, 0.2287, 0.1935, 0.1935, 0.1935],\n",
      "         [0.1480, 0.4729, 0.0536, 0.1627, 0.1627],\n",
      "         [0.1818, 0.2922, 0.1201, 0.2169, 0.1890],\n",
      "         [0.1978, 0.2078, 0.1894, 0.2015, 0.2035]],\n",
      "\n",
      "        [[0.2183, 0.1954, 0.1954, 0.1954, 0.1954],\n",
      "         [0.1863, 0.2793, 0.1782, 0.1782, 0.1782],\n",
      "         [0.1991, 0.1627, 0.2310, 0.2036, 0.2036],\n",
      "         [0.1938, 0.2323, 0.1697, 0.2144, 0.1899],\n",
      "         [0.1662, 0.2853, 0.1119, 0.2246, 0.2120]],\n",
      "\n",
      "        [[0.2635, 0.1841, 0.1841, 0.1841, 0.1841],\n",
      "         [0.2791, 0.3073, 0.1379, 0.1379, 0.1379],\n",
      "         [0.1898, 0.1877, 0.2106, 0.2060, 0.2060],\n",
      "         [0.2584, 0.2749, 0.1451, 0.1577, 0.1638],\n",
      "         [0.2202, 0.2263, 0.1713, 0.1776, 0.2045]],\n",
      "\n",
      "        [[0.2570, 0.1857, 0.1857, 0.1857, 0.1857],\n",
      "         [0.2040, 0.2070, 0.1963, 0.1963, 0.1963],\n",
      "         [0.2184, 0.2294, 0.1673, 0.1925, 0.1925],\n",
      "         [0.2930, 0.3755, 0.0763, 0.1002, 0.1550],\n",
      "         [0.2197, 0.2321, 0.1633, 0.1734, 0.2115]],\n",
      "\n",
      "        [[0.4081, 0.1480, 0.1480, 0.1480, 0.1480],\n",
      "         [0.2289, 0.1999, 0.1904, 0.1904, 0.1904],\n",
      "         [0.2844, 0.1961, 0.1766, 0.1714, 0.1714],\n",
      "         [0.2324, 0.1925, 0.1826, 0.2127, 0.1799],\n",
      "         [0.2297, 0.1904, 0.1806, 0.2103, 0.1890]],\n",
      "\n",
      "        [[0.2577, 0.1856, 0.1856, 0.1856, 0.1856],\n",
      "         [0.2017, 0.2042, 0.1980, 0.1980, 0.1980],\n",
      "         [0.1648, 0.1397, 0.2723, 0.2116, 0.2116],\n",
      "         [0.2308, 0.2789, 0.1297, 0.1873, 0.1732],\n",
      "         [0.2164, 0.2547, 0.1316, 0.1807, 0.2165]],\n",
      "\n",
      "        [[0.3099, 0.1725, 0.1725, 0.1725, 0.1725],\n",
      "         [0.2115, 0.2149, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2036, 0.2043, 0.1939, 0.1991, 0.1991],\n",
      "         [0.2333, 0.2443, 0.1244, 0.2236, 0.1745],\n",
      "         [0.2384, 0.2543, 0.0987, 0.2246, 0.1840]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2138, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.1664, 0.2763, 0.1858, 0.1858, 0.1858],\n",
      "         [0.1979, 0.2023, 0.2023, 0.1988, 0.1988],\n",
      "         [0.1665, 0.2373, 0.2374, 0.1789, 0.1798],\n",
      "         [0.2483, 0.1642, 0.1641, 0.2283, 0.1951]],\n",
      "\n",
      "        [[0.1731, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.2202, 0.1804, 0.1998, 0.1998, 0.1998],\n",
      "         [0.2305, 0.1714, 0.1990, 0.1995, 0.1995],\n",
      "         [0.1735, 0.2264, 0.1980, 0.2046, 0.1975],\n",
      "         [0.2298, 0.1912, 0.2098, 0.2050, 0.1641]],\n",
      "\n",
      "        [[0.1929, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2297, 0.1901, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2018, 0.1974, 0.2051, 0.1978, 0.1978],\n",
      "         [0.2071, 0.1611, 0.2492, 0.2178, 0.1648],\n",
      "         [0.2002, 0.2041, 0.1974, 0.1994, 0.1989]],\n",
      "\n",
      "        [[0.2638, 0.1841, 0.1841, 0.1841, 0.1841],\n",
      "         [0.1817, 0.2046, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1624, 0.2297, 0.1485, 0.2297, 0.2297],\n",
      "         [0.1793, 0.2407, 0.1662, 0.1730, 0.2408],\n",
      "         [0.1994, 0.1925, 0.2012, 0.2002, 0.2067]],\n",
      "\n",
      "        [[0.2076, 0.1981, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1988, 0.1803, 0.2070, 0.2070, 0.2070],\n",
      "         [0.2089, 0.2525, 0.1523, 0.1931, 0.1931],\n",
      "         [0.1992, 0.2212, 0.1673, 0.2216, 0.1908],\n",
      "         [0.1992, 0.2008, 0.1966, 0.2008, 0.2026]],\n",
      "\n",
      "        [[0.1936, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.1727, 0.1880, 0.2131, 0.2131, 0.2131],\n",
      "         [0.1811, 0.1901, 0.2201, 0.2043, 0.2043],\n",
      "         [0.2010, 0.2007, 0.1996, 0.1985, 0.2002],\n",
      "         [0.1826, 0.1864, 0.1981, 0.2110, 0.2219]],\n",
      "\n",
      "        [[0.2538, 0.1866, 0.1866, 0.1866, 0.1866],\n",
      "         [0.2746, 0.1716, 0.1846, 0.1846, 0.1846],\n",
      "         [0.2009, 0.1859, 0.2370, 0.1881, 0.1881],\n",
      "         [0.2022, 0.2153, 0.1769, 0.1924, 0.2132],\n",
      "         [0.1760, 0.1443, 0.2677, 0.2058, 0.2061]],\n",
      "\n",
      "        [[0.2054, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.1853, 0.2279, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2121, 0.1846, 0.1943, 0.2045, 0.2045],\n",
      "         [0.1745, 0.2212, 0.2026, 0.2161, 0.1856],\n",
      "         [0.2047, 0.1984, 0.2007, 0.1990, 0.1972]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.1995, 0.1793, 0.2071, 0.2071, 0.2071],\n",
      "         [0.1828, 0.2567, 0.2356, 0.1625, 0.1625],\n",
      "         [0.1659, 0.2408, 0.2191, 0.2284, 0.1457],\n",
      "         [0.2187, 0.1838, 0.1921, 0.1884, 0.2169]],\n",
      "\n",
      "        [[0.1738, 0.2065, 0.2065, 0.2065, 0.2065],\n",
      "         [0.2748, 0.2208, 0.1681, 0.1681, 0.1681],\n",
      "         [0.2235, 0.2085, 0.1855, 0.1913, 0.1913],\n",
      "         [0.2293, 0.1773, 0.1149, 0.3497, 0.1288],\n",
      "         [0.2072, 0.1820, 0.1462, 0.2565, 0.2081]],\n",
      "\n",
      "        [[0.2761, 0.1810, 0.1810, 0.1810, 0.1810],\n",
      "         [0.4311, 0.1756, 0.1311, 0.1311, 0.1311],\n",
      "         [0.2508, 0.1950, 0.1949, 0.1797, 0.1797],\n",
      "         [0.2993, 0.1892, 0.1889, 0.1597, 0.1630],\n",
      "         [0.2510, 0.1914, 0.1912, 0.1731, 0.1933]],\n",
      "\n",
      "        [[0.1920, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2518, 0.2074, 0.1802, 0.1802, 0.1802],\n",
      "         [0.2099, 0.1862, 0.2626, 0.1707, 0.1707],\n",
      "         [0.2201, 0.1869, 0.2989, 0.1281, 0.1660],\n",
      "         [0.2021, 0.1851, 0.2383, 0.1511, 0.2233]],\n",
      "\n",
      "        [[0.3989, 0.1503, 0.1503, 0.1503, 0.1503],\n",
      "         [0.4337, 0.2628, 0.1012, 0.1012, 0.1012],\n",
      "         [0.1847, 0.1927, 0.2049, 0.2089, 0.2089],\n",
      "         [0.1369, 0.1689, 0.2292, 0.2132, 0.2518],\n",
      "         [0.2727, 0.2088, 0.1414, 0.1551, 0.2219]],\n",
      "\n",
      "        [[0.2039, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2027, 0.1839, 0.2045, 0.2045, 0.2045],\n",
      "         [0.1660, 0.2411, 0.2716, 0.1606, 0.1606],\n",
      "         [0.1484, 0.2256, 0.2577, 0.2253, 0.1430],\n",
      "         [0.2074, 0.1969, 0.1936, 0.1969, 0.2052]],\n",
      "\n",
      "        [[0.1782, 0.2054, 0.2054, 0.2054, 0.2054],\n",
      "         [0.2298, 0.2127, 0.1858, 0.1858, 0.1858],\n",
      "         [0.2012, 0.1969, 0.2225, 0.1896, 0.1896],\n",
      "         [0.2067, 0.1867, 0.3314, 0.1188, 0.1564],\n",
      "         [0.2103, 0.2021, 0.2527, 0.1694, 0.1655]],\n",
      "\n",
      "        [[0.2631, 0.1842, 0.1842, 0.1842, 0.1842],\n",
      "         [0.2213, 0.3293, 0.1498, 0.1498, 0.1498],\n",
      "         [0.1982, 0.1924, 0.2012, 0.2041, 0.2041],\n",
      "         [0.2110, 0.2464, 0.1951, 0.1664, 0.1811],\n",
      "         [0.2050, 0.2203, 0.1978, 0.1838, 0.1932]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1659, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.1929, 0.1976, 0.2031, 0.2031, 0.2031],\n",
      "         [0.2099, 0.1993, 0.2152, 0.1878, 0.1878],\n",
      "         [0.1656, 0.2024, 0.1504, 0.2267, 0.2549],\n",
      "         [0.2145, 0.1914, 0.2265, 0.1794, 0.1882]],\n",
      "\n",
      "        [[0.2248, 0.1938, 0.1938, 0.1938, 0.1938],\n",
      "         [0.2100, 0.1841, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2280, 0.1606, 0.2001, 0.2057, 0.2057],\n",
      "         [0.3082, 0.1335, 0.2257, 0.0917, 0.2410],\n",
      "         [0.1968, 0.2004, 0.1981, 0.2021, 0.2025]],\n",
      "\n",
      "        [[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2146, 0.1156, 0.2233, 0.2233, 0.2233],\n",
      "         [0.2139, 0.1843, 0.1698, 0.2160, 0.2160],\n",
      "         [0.2396, 0.1658, 0.1355, 0.2137, 0.2454],\n",
      "         [0.1981, 0.2012, 0.2030, 0.1991, 0.1986]],\n",
      "\n",
      "        [[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.2168, 0.1670, 0.2054, 0.2054, 0.2054],\n",
      "         [0.1881, 0.2047, 0.2243, 0.1914, 0.1914],\n",
      "         [0.2046, 0.1975, 0.1900, 0.2048, 0.2031],\n",
      "         [0.1912, 0.1996, 0.2091, 0.1910, 0.2092]],\n",
      "\n",
      "        [[0.1072, 0.2232, 0.2232, 0.2232, 0.2232],\n",
      "         [0.1686, 0.1627, 0.2229, 0.2229, 0.2229],\n",
      "         [0.1276, 0.1143, 0.1520, 0.3030, 0.3030],\n",
      "         [0.1631, 0.1515, 0.1833, 0.2114, 0.2906],\n",
      "         [0.1953, 0.1884, 0.2068, 0.2217, 0.1878]],\n",
      "\n",
      "        [[0.2448, 0.1888, 0.1888, 0.1888, 0.1888],\n",
      "         [0.2815, 0.1648, 0.1846, 0.1846, 0.1846],\n",
      "         [0.3182, 0.1753, 0.1086, 0.1989, 0.1989],\n",
      "         [0.3354, 0.1990, 0.1308, 0.1125, 0.2223],\n",
      "         [0.1372, 0.1875, 0.2409, 0.2636, 0.1709]],\n",
      "\n",
      "        [[0.3105, 0.1724, 0.1724, 0.1724, 0.1724],\n",
      "         [0.3073, 0.2089, 0.1612, 0.1612, 0.1612],\n",
      "         [0.2173, 0.2062, 0.1782, 0.1992, 0.1992],\n",
      "         [0.1791, 0.1940, 0.2424, 0.1799, 0.2046],\n",
      "         [0.2145, 0.2015, 0.1688, 0.2138, 0.2014]],\n",
      "\n",
      "        [[0.1432, 0.2142, 0.2142, 0.2142, 0.2142],\n",
      "         [0.1496, 0.1058, 0.2482, 0.2482, 0.2482],\n",
      "         [0.1894, 0.1740, 0.2078, 0.2144, 0.2144],\n",
      "         [0.1822, 0.1667, 0.2007, 0.2430, 0.2074],\n",
      "         [0.1642, 0.1352, 0.2030, 0.3090, 0.1887]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1695, 0.2076, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1823, 0.1637, 0.2180, 0.2180, 0.2180],\n",
      "         [0.1925, 0.1642, 0.1423, 0.2505, 0.2505],\n",
      "         [0.2015, 0.1965, 0.1922, 0.1999, 0.2099],\n",
      "         [0.2044, 0.2003, 0.1967, 0.2031, 0.1955]],\n",
      "\n",
      "        [[0.1514, 0.2122, 0.2122, 0.2122, 0.2122],\n",
      "         [0.0854, 0.1594, 0.2517, 0.2517, 0.2517],\n",
      "         [0.2151, 0.2024, 0.1953, 0.1936, 0.1936],\n",
      "         [0.1495, 0.2010, 0.2395, 0.1603, 0.2497],\n",
      "         [0.1168, 0.2144, 0.3069, 0.1347, 0.2272]],\n",
      "\n",
      "        [[0.1654, 0.2086, 0.2086, 0.2086, 0.2086],\n",
      "         [0.0642, 0.3119, 0.2080, 0.2080, 0.2080],\n",
      "         [0.1291, 0.2472, 0.2050, 0.2093, 0.2093],\n",
      "         [0.2213, 0.1930, 0.2008, 0.1850, 0.1999],\n",
      "         [0.0783, 0.2346, 0.1710, 0.3290, 0.1872]],\n",
      "\n",
      "        [[0.1979, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2167, 0.2175, 0.1886, 0.1886, 0.1886],\n",
      "         [0.2003, 0.2003, 0.1993, 0.2001, 0.2001],\n",
      "         [0.2048, 0.2049, 0.1792, 0.2121, 0.1991],\n",
      "         [0.2287, 0.2300, 0.0878, 0.2946, 0.1590]],\n",
      "\n",
      "        [[0.1459, 0.2135, 0.2135, 0.2135, 0.2135],\n",
      "         [0.0718, 0.0977, 0.2768, 0.2768, 0.2768],\n",
      "         [0.1923, 0.1951, 0.2024, 0.2051, 0.2051],\n",
      "         [0.1771, 0.1855, 0.2082, 0.2124, 0.2169],\n",
      "         [0.1366, 0.1613, 0.2442, 0.2624, 0.1955]],\n",
      "\n",
      "        [[0.2255, 0.1936, 0.1936, 0.1936, 0.1936],\n",
      "         [0.2218, 0.1691, 0.2030, 0.2030, 0.2030],\n",
      "         [0.2277, 0.1718, 0.1850, 0.2077, 0.2077],\n",
      "         [0.2524, 0.1605, 0.1808, 0.1885, 0.2178],\n",
      "         [0.2988, 0.1357, 0.1670, 0.1796, 0.2190]],\n",
      "\n",
      "        [[0.1754, 0.2061, 0.2061, 0.2061, 0.2061],\n",
      "         [0.2064, 0.2037, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2244, 0.2192, 0.1454, 0.2055, 0.2055],\n",
      "         [0.2038, 0.2028, 0.1860, 0.2073, 0.2001],\n",
      "         [0.2007, 0.1973, 0.1465, 0.2128, 0.2428]],\n",
      "\n",
      "        [[0.1505, 0.2124, 0.2124, 0.2124, 0.2124],\n",
      "         [0.1761, 0.1878, 0.2121, 0.2121, 0.2121],\n",
      "         [0.2259, 0.2089, 0.2047, 0.1802, 0.1802],\n",
      "         [0.2229, 0.2140, 0.2117, 0.1533, 0.1981],\n",
      "         [0.1875, 0.1906, 0.1914, 0.2182, 0.2124]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2816, 0.1796, 0.1796, 0.1796, 0.1796],\n",
      "         [0.3592, 0.3013, 0.1131, 0.1131, 0.1131],\n",
      "         [0.1811, 0.1863, 0.1975, 0.2175, 0.2175],\n",
      "         [0.2667, 0.2455, 0.2062, 0.1267, 0.1549],\n",
      "         [0.1068, 0.1254, 0.1758, 0.4523, 0.1397]],\n",
      "\n",
      "        [[0.2331, 0.1917, 0.1917, 0.1917, 0.1917],\n",
      "         [0.3586, 0.2439, 0.1325, 0.1325, 0.1325],\n",
      "         [0.2017, 0.2007, 0.1991, 0.1992, 0.1992],\n",
      "         [0.1554, 0.1817, 0.2373, 0.1928, 0.2328],\n",
      "         [0.2191, 0.1961, 0.1623, 0.1880, 0.2345]],\n",
      "\n",
      "        [[0.1635, 0.2091, 0.2091, 0.2091, 0.2091],\n",
      "         [0.2301, 0.2198, 0.1834, 0.1834, 0.1834],\n",
      "         [0.1765, 0.1851, 0.1919, 0.2233, 0.2233],\n",
      "         [0.2606, 0.2354, 0.2180, 0.1282, 0.1578],\n",
      "         [0.2317, 0.2136, 0.2008, 0.1311, 0.2227]],\n",
      "\n",
      "        [[0.2659, 0.1835, 0.1835, 0.1835, 0.1835],\n",
      "         [0.3525, 0.1947, 0.1510, 0.1510, 0.1510],\n",
      "         [0.1423, 0.2065, 0.1667, 0.2422, 0.2422],\n",
      "         [0.3130, 0.1911, 0.2538, 0.0875, 0.1547],\n",
      "         [0.2143, 0.1889, 0.2032, 0.1547, 0.2389]],\n",
      "\n",
      "        [[0.1370, 0.2157, 0.2157, 0.2157, 0.2157],\n",
      "         [0.2369, 0.2184, 0.1816, 0.1816, 0.1816],\n",
      "         [0.1374, 0.1662, 0.1840, 0.2562, 0.2562],\n",
      "         [0.2346, 0.2055, 0.1915, 0.2164, 0.1520],\n",
      "         [0.1924, 0.2086, 0.2178, 0.2022, 0.1789]],\n",
      "\n",
      "        [[0.3002, 0.1749, 0.1749, 0.1749, 0.1749],\n",
      "         [0.1957, 0.1921, 0.2041, 0.2041, 0.2041],\n",
      "         [0.1844, 0.1635, 0.1664, 0.2428, 0.2428],\n",
      "         [0.2095, 0.2509, 0.2445, 0.1563, 0.1387],\n",
      "         [0.2033, 0.1979, 0.1986, 0.2125, 0.1876]],\n",
      "\n",
      "        [[0.1952, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2019, 0.2127, 0.1952, 0.1952, 0.1952],\n",
      "         [0.2189, 0.2382, 0.1285, 0.2072, 0.2072],\n",
      "         [0.1887, 0.1728, 0.3291, 0.1094, 0.1999],\n",
      "         [0.1927, 0.1805, 0.2914, 0.1285, 0.2069]],\n",
      "\n",
      "        [[0.3220, 0.1695, 0.1695, 0.1695, 0.1695],\n",
      "         [0.2265, 0.2236, 0.1833, 0.1833, 0.1833],\n",
      "         [0.1523, 0.1563, 0.2276, 0.2319, 0.2319],\n",
      "         [0.2987, 0.2870, 0.1602, 0.0987, 0.1555],\n",
      "         [0.2849, 0.2733, 0.1487, 0.0897, 0.2034]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1990, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2050, 0.1833, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1955, 0.2712, 0.1362, 0.1986, 0.1986],\n",
      "         [0.2049, 0.2186, 0.1908, 0.1802, 0.2055],\n",
      "         [0.1691, 0.1121, 0.2663, 0.3839, 0.0686]],\n",
      "\n",
      "        [[0.1940, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1623, 0.2333, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1994, 0.1709, 0.2659, 0.1819, 0.1819],\n",
      "         [0.2185, 0.3260, 0.1035, 0.0747, 0.2773],\n",
      "         [0.1991, 0.1957, 0.2055, 0.2084, 0.1913]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2311, 0.0679, 0.2337, 0.2337, 0.2337],\n",
      "         [0.1719, 0.2951, 0.1908, 0.1711, 0.1711],\n",
      "         [0.1591, 0.2983, 0.1796, 0.2047, 0.1582],\n",
      "         [0.1953, 0.2154, 0.1991, 0.2031, 0.1871]],\n",
      "\n",
      "        [[0.1603, 0.2099, 0.2099, 0.2099, 0.2099],\n",
      "         [0.2713, 0.1270, 0.2005, 0.2005, 0.2005],\n",
      "         [0.1149, 0.3889, 0.1227, 0.1867, 0.1867],\n",
      "         [0.1599, 0.2432, 0.1636, 0.2444, 0.1890],\n",
      "         [0.1990, 0.2012, 0.1992, 0.2013, 0.1993]],\n",
      "\n",
      "        [[0.2043, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1891, 0.2027, 0.2027, 0.2027, 0.2027],\n",
      "         [0.1975, 0.1882, 0.2380, 0.1881, 0.1881],\n",
      "         [0.1856, 0.1643, 0.2973, 0.1886, 0.1642],\n",
      "         [0.1998, 0.2006, 0.1966, 0.1997, 0.2034]],\n",
      "\n",
      "        [[0.1343, 0.2164, 0.2164, 0.2164, 0.2164],\n",
      "         [0.1730, 0.1980, 0.2097, 0.2097, 0.2097],\n",
      "         [0.1766, 0.2179, 0.1292, 0.2381, 0.2381],\n",
      "         [0.2000, 0.2031, 0.1955, 0.1970, 0.2044],\n",
      "         [0.2015, 0.1985, 0.2060, 0.2044, 0.1895]],\n",
      "\n",
      "        [[0.2278, 0.1931, 0.1931, 0.1931, 0.1931],\n",
      "         [0.1777, 0.0701, 0.2507, 0.2507, 0.2507],\n",
      "         [0.1987, 0.3057, 0.1569, 0.1694, 0.1694],\n",
      "         [0.2029, 0.1810, 0.2161, 0.1883, 0.2117],\n",
      "         [0.2089, 0.1743, 0.2307, 0.1856, 0.2006]],\n",
      "\n",
      "        [[0.1489, 0.2128, 0.2128, 0.2128, 0.2128],\n",
      "         [0.1812, 0.1778, 0.2137, 0.2137, 0.2137],\n",
      "         [0.2496, 0.2620, 0.1607, 0.1638, 0.1638],\n",
      "         [0.2156, 0.2196, 0.1825, 0.1984, 0.1839],\n",
      "         [0.2230, 0.2286, 0.1780, 0.1993, 0.1710]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0703, 0.2324, 0.2324, 0.2324, 0.2324],\n",
      "         [0.0824, 0.2327, 0.2283, 0.2283, 0.2283],\n",
      "         [0.1161, 0.2175, 0.2364, 0.2150, 0.2150],\n",
      "         [0.0947, 0.2316, 0.2609, 0.1849, 0.2279],\n",
      "         [0.3058, 0.1854, 0.1734, 0.2103, 0.1252]],\n",
      "\n",
      "        [[0.0593, 0.2352, 0.2352, 0.2352, 0.2352],\n",
      "         [0.0875, 0.0988, 0.2712, 0.2712, 0.2712],\n",
      "         [0.1154, 0.1240, 0.3087, 0.2260, 0.2260],\n",
      "         [0.0910, 0.1012, 0.3859, 0.1778, 0.2441],\n",
      "         [0.1378, 0.1454, 0.2881, 0.1939, 0.2348]],\n",
      "\n",
      "        [[0.2350, 0.1912, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2439, 0.1388, 0.2058, 0.2058, 0.2058],\n",
      "         [0.1970, 0.2159, 0.1819, 0.2026, 0.2026],\n",
      "         [0.1902, 0.3216, 0.1203, 0.1450, 0.2229],\n",
      "         [0.1867, 0.1262, 0.2626, 0.2285, 0.1960]],\n",
      "\n",
      "        [[0.1963, 0.2009, 0.2009, 0.2009, 0.2009],\n",
      "         [0.4079, 0.0890, 0.1677, 0.1677, 0.1677],\n",
      "         [0.2015, 0.1979, 0.2020, 0.1993, 0.1993],\n",
      "         [0.2297, 0.1289, 0.2511, 0.2262, 0.1640],\n",
      "         [0.2102, 0.1488, 0.2217, 0.2083, 0.2109]],\n",
      "\n",
      "        [[0.1854, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.1901, 0.1562, 0.2179, 0.2179, 0.2179],\n",
      "         [0.1984, 0.1908, 0.2032, 0.2038, 0.2038],\n",
      "         [0.1932, 0.1725, 0.2071, 0.2182, 0.2091],\n",
      "         [0.1887, 0.1791, 0.1948, 0.1996, 0.2377]],\n",
      "\n",
      "        [[0.0955, 0.2261, 0.2261, 0.2261, 0.2261],\n",
      "         [0.1313, 0.1736, 0.2317, 0.2317, 0.2317],\n",
      "         [0.1543, 0.1841, 0.2199, 0.2209, 0.2209],\n",
      "         [0.1244, 0.1774, 0.2535, 0.1887, 0.2559],\n",
      "         [0.2006, 0.2002, 0.1997, 0.2001, 0.1994]],\n",
      "\n",
      "        [[0.1874, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1913, 0.2027, 0.2020, 0.2020, 0.2020],\n",
      "         [0.1995, 0.2005, 0.1991, 0.2004, 0.2004],\n",
      "         [0.2110, 0.1746, 0.2316, 0.2060, 0.1768],\n",
      "         [0.1870, 0.2318, 0.1682, 0.1922, 0.2207]],\n",
      "\n",
      "        [[0.0521, 0.2370, 0.2370, 0.2370, 0.2370],\n",
      "         [0.1276, 0.2391, 0.2111, 0.2111, 0.2111],\n",
      "         [0.3062, 0.1798, 0.1142, 0.1999, 0.1999],\n",
      "         [0.3062, 0.1920, 0.1289, 0.1624, 0.2106],\n",
      "         [0.1491, 0.1915, 0.2371, 0.2095, 0.2128]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1065, 0.2234, 0.2234, 0.2234, 0.2234],\n",
      "         [0.1082, 0.3532, 0.1795, 0.1795, 0.1795],\n",
      "         [0.3143, 0.1190, 0.1519, 0.2074, 0.2074],\n",
      "         [0.0956, 0.3333, 0.2436, 0.1644, 0.1631],\n",
      "         [0.3055, 0.1236, 0.1551, 0.2062, 0.2096]],\n",
      "\n",
      "        [[0.0923, 0.2269, 0.2269, 0.2269, 0.2269],\n",
      "         [0.1333, 0.3025, 0.1881, 0.1881, 0.1881],\n",
      "         [0.1431, 0.3263, 0.1260, 0.2023, 0.2023],\n",
      "         [0.1422, 0.3312, 0.1249, 0.1988, 0.2028],\n",
      "         [0.2467, 0.1370, 0.2701, 0.1954, 0.1507]],\n",
      "\n",
      "        [[0.1645, 0.2089, 0.2089, 0.2089, 0.2089],\n",
      "         [0.2190, 0.2090, 0.1906, 0.1906, 0.1906],\n",
      "         [0.2384, 0.2219, 0.1545, 0.1926, 0.1926],\n",
      "         [0.2322, 0.2155, 0.1477, 0.2186, 0.1859],\n",
      "         [0.2234, 0.2112, 0.1587, 0.2135, 0.1932]],\n",
      "\n",
      "        [[0.2025, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1878, 0.1731, 0.2130, 0.2130, 0.2130],\n",
      "         [0.1918, 0.1866, 0.2214, 0.2001, 0.2001],\n",
      "         [0.2044, 0.2094, 0.1804, 0.2089, 0.1970],\n",
      "         [0.2150, 0.2217, 0.1829, 0.2210, 0.1593]],\n",
      "\n",
      "        [[0.1816, 0.2046, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1680, 0.2999, 0.1774, 0.1774, 0.1774],\n",
      "         [0.1931, 0.2420, 0.1704, 0.1972, 0.1972],\n",
      "         [0.1758, 0.4056, 0.1106, 0.1180, 0.1901],\n",
      "         [0.1985, 0.2152, 0.1897, 0.1909, 0.2056]],\n",
      "\n",
      "        [[0.1423, 0.2144, 0.2144, 0.2144, 0.2144],\n",
      "         [0.1109, 0.2331, 0.2187, 0.2187, 0.2187],\n",
      "         [0.1633, 0.2214, 0.1841, 0.2157, 0.2157],\n",
      "         [0.0696, 0.2325, 0.1119, 0.3764, 0.2096],\n",
      "         [0.3986, 0.1209, 0.2491, 0.0751, 0.1563]],\n",
      "\n",
      "        [[0.1847, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.3220, 0.2416, 0.1455, 0.1455, 0.1455],\n",
      "         [0.2235, 0.2139, 0.1666, 0.1980, 0.1980],\n",
      "         [0.2739, 0.2324, 0.0912, 0.2285, 0.1740],\n",
      "         [0.2749, 0.2073, 0.0414, 0.2012, 0.2752]],\n",
      "\n",
      "        [[0.2310, 0.1922, 0.1922, 0.1922, 0.1922],\n",
      "         [0.1977, 0.2575, 0.1816, 0.1816, 0.1816],\n",
      "         [0.2009, 0.2053, 0.1949, 0.1995, 0.1995],\n",
      "         [0.1761, 0.1357, 0.2518, 0.2448, 0.1915],\n",
      "         [0.2009, 0.2029, 0.1981, 0.1983, 0.1998]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2189, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.2397, 0.1802, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2089, 0.1756, 0.2488, 0.1833, 0.1833],\n",
      "         [0.1842, 0.1302, 0.2615, 0.2823, 0.1419],\n",
      "         [0.1669, 0.2262, 0.1228, 0.1148, 0.3694]],\n",
      "\n",
      "        [[0.2335, 0.1916, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2306, 0.4970, 0.0908, 0.0908, 0.0908],\n",
      "         [0.1674, 0.1196, 0.2093, 0.2518, 0.2518],\n",
      "         [0.1879, 0.2253, 0.1666, 0.2693, 0.1508],\n",
      "         [0.1657, 0.2497, 0.1262, 0.3740, 0.0844]],\n",
      "\n",
      "        [[0.2027, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.1759, 0.3261, 0.1660, 0.1660, 0.1660],\n",
      "         [0.1990, 0.1279, 0.2583, 0.2074, 0.2074],\n",
      "         [0.1900, 0.1496, 0.2188, 0.2472, 0.1943],\n",
      "         [0.2099, 0.3386, 0.1583, 0.1240, 0.1692]],\n",
      "\n",
      "        [[0.2403, 0.1899, 0.1899, 0.1899, 0.1899],\n",
      "         [0.1041, 0.3176, 0.1928, 0.1928, 0.1928],\n",
      "         [0.2561, 0.1514, 0.2093, 0.1916, 0.1916],\n",
      "         [0.2019, 0.1980, 0.2004, 0.1999, 0.1997],\n",
      "         [0.1860, 0.2180, 0.1977, 0.2015, 0.1968]],\n",
      "\n",
      "        [[0.2153, 0.1962, 0.1962, 0.1962, 0.1962],\n",
      "         [0.1529, 0.5330, 0.1047, 0.1047, 0.1047],\n",
      "         [0.1941, 0.1137, 0.2358, 0.2282, 0.2282],\n",
      "         [0.1890, 0.1513, 0.2049, 0.2525, 0.2022],\n",
      "         [0.2007, 0.2042, 0.1995, 0.1962, 0.1993]],\n",
      "\n",
      "        [[0.2574, 0.1856, 0.1856, 0.1856, 0.1856],\n",
      "         [0.2070, 0.2080, 0.1950, 0.1950, 0.1950],\n",
      "         [0.1812, 0.1778, 0.1810, 0.2300, 0.2300],\n",
      "         [0.2132, 0.2208, 0.2138, 0.2137, 0.1384],\n",
      "         [0.2078, 0.2097, 0.2080, 0.2079, 0.1666]],\n",
      "\n",
      "        [[0.2664, 0.1834, 0.1834, 0.1834, 0.1834],\n",
      "         [0.2150, 0.1821, 0.2010, 0.2010, 0.2010],\n",
      "         [0.2101, 0.1877, 0.2008, 0.2007, 0.2007],\n",
      "         [0.1784, 0.2257, 0.1961, 0.2035, 0.1963],\n",
      "         [0.2327, 0.1567, 0.1984, 0.1864, 0.2259]],\n",
      "\n",
      "        [[0.4938, 0.1265, 0.1265, 0.1265, 0.1265],\n",
      "         [0.2650, 0.2077, 0.1758, 0.1758, 0.1758],\n",
      "         [0.0445, 0.1068, 0.4604, 0.1941, 0.1941],\n",
      "         [0.1518, 0.1790, 0.2360, 0.2328, 0.2004],\n",
      "         [0.1546, 0.1775, 0.2237, 0.2211, 0.2231]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1737, 0.2066, 0.2066, 0.2066, 0.2066],\n",
      "         [0.1689, 0.2680, 0.1877, 0.1877, 0.1877],\n",
      "         [0.2073, 0.1801, 0.2110, 0.2008, 0.2008],\n",
      "         [0.1868, 0.2269, 0.1822, 0.2088, 0.1953],\n",
      "         [0.2881, 0.1213, 0.3216, 0.1754, 0.0936]],\n",
      "\n",
      "        [[0.2263, 0.1934, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2326, 0.1469, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1956, 0.2426, 0.1487, 0.2066, 0.2066],\n",
      "         [0.1983, 0.1710, 0.2393, 0.2004, 0.1909],\n",
      "         [0.2023, 0.2331, 0.1688, 0.2001, 0.1957]],\n",
      "\n",
      "        [[0.1830, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.2497, 0.1241, 0.2087, 0.2087, 0.2087],\n",
      "         [0.1870, 0.2986, 0.0927, 0.2109, 0.2109],\n",
      "         [0.1992, 0.1765, 0.2389, 0.1922, 0.1932],\n",
      "         [0.2005, 0.2228, 0.1711, 0.2069, 0.1987]],\n",
      "\n",
      "        [[0.2052, 0.1987, 0.1987, 0.1987, 0.1987],\n",
      "         [0.2840, 0.1400, 0.1920, 0.1920, 0.1920],\n",
      "         [0.1302, 0.3678, 0.0394, 0.2313, 0.2313],\n",
      "         [0.1987, 0.2199, 0.1769, 0.1943, 0.2102],\n",
      "         [0.1982, 0.2456, 0.1548, 0.1890, 0.2124]],\n",
      "\n",
      "        [[0.1808, 0.2048, 0.2048, 0.2048, 0.2048],\n",
      "         [0.1836, 0.1951, 0.2071, 0.2071, 0.2071],\n",
      "         [0.2029, 0.1939, 0.2322, 0.1855, 0.1855],\n",
      "         [0.1870, 0.1704, 0.2465, 0.2406, 0.1555],\n",
      "         [0.2151, 0.2446, 0.1468, 0.1518, 0.2417]],\n",
      "\n",
      "        [[0.0783, 0.2304, 0.2304, 0.2304, 0.2304],\n",
      "         [0.1117, 0.2809, 0.2025, 0.2025, 0.2025],\n",
      "         [0.1565, 0.2362, 0.1992, 0.2041, 0.2041],\n",
      "         [0.1179, 0.2610, 0.1879, 0.2364, 0.1969],\n",
      "         [0.2331, 0.1794, 0.1999, 0.1853, 0.2023]],\n",
      "\n",
      "        [[0.1952, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2040, 0.1574, 0.2129, 0.2129, 0.2129],\n",
      "         [0.2144, 0.3212, 0.0629, 0.2008, 0.2008],\n",
      "         [0.2014, 0.2062, 0.1872, 0.2046, 0.2006],\n",
      "         [0.1649, 0.1128, 0.5221, 0.1277, 0.0725]],\n",
      "\n",
      "        [[0.1685, 0.2079, 0.2079, 0.2079, 0.2079],\n",
      "         [0.1914, 0.2075, 0.2004, 0.2004, 0.2004],\n",
      "         [0.2925, 0.1896, 0.0601, 0.2289, 0.2289],\n",
      "         [0.1900, 0.1996, 0.2275, 0.1876, 0.1953],\n",
      "         [0.1974, 0.1992, 0.2041, 0.1970, 0.2022]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1870, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1359, 0.1324, 0.2439, 0.2439, 0.2439],\n",
      "         [0.1666, 0.1636, 0.1691, 0.2503, 0.2503],\n",
      "         [0.1761, 0.1737, 0.1780, 0.2353, 0.2369],\n",
      "         [0.2330, 0.2361, 0.2305, 0.1735, 0.1270]],\n",
      "\n",
      "        [[0.1682, 0.2080, 0.2080, 0.2080, 0.2080],\n",
      "         [0.1926, 0.1617, 0.2152, 0.2152, 0.2152],\n",
      "         [0.1781, 0.1229, 0.2480, 0.2255, 0.2255],\n",
      "         [0.1527, 0.0799, 0.2721, 0.2649, 0.2305],\n",
      "         [0.2094, 0.2587, 0.1734, 0.1750, 0.1835]],\n",
      "\n",
      "        [[0.1928, 0.2018, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2282, 0.1417, 0.2100, 0.2100, 0.2100],\n",
      "         [0.1901, 0.1129, 0.3499, 0.1736, 0.1736],\n",
      "         [0.1669, 0.0807, 0.3907, 0.2146, 0.1470],\n",
      "         [0.1991, 0.2330, 0.1657, 0.1886, 0.2136]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2248, 0.1043, 0.2236, 0.2236, 0.2236],\n",
      "         [0.1879, 0.3181, 0.1169, 0.1886, 0.1886],\n",
      "         [0.1949, 0.0953, 0.3716, 0.1441, 0.1940],\n",
      "         [0.2052, 0.1265, 0.3175, 0.1673, 0.1836]],\n",
      "\n",
      "        [[0.1871, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1658, 0.2281, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2521, 0.1135, 0.3267, 0.1539, 0.1539],\n",
      "         [0.2279, 0.0949, 0.3031, 0.2416, 0.1325],\n",
      "         [0.1995, 0.2019, 0.1987, 0.1993, 0.2005]],\n",
      "\n",
      "        [[0.1379, 0.2155, 0.2155, 0.2155, 0.2155],\n",
      "         [0.0763, 0.1962, 0.2425, 0.2425, 0.2425],\n",
      "         [0.0619, 0.1817, 0.2935, 0.2315, 0.2315],\n",
      "         [0.0409, 0.1741, 0.3317, 0.2124, 0.2410],\n",
      "         [0.3216, 0.1835, 0.1429, 0.1699, 0.1822]],\n",
      "\n",
      "        [[0.1803, 0.2049, 0.2049, 0.2049, 0.2049],\n",
      "         [0.1880, 0.1580, 0.2180, 0.2180, 0.2180],\n",
      "         [0.1775, 0.1665, 0.2811, 0.1875, 0.1875],\n",
      "         [0.1836, 0.1689, 0.3342, 0.1161, 0.1972],\n",
      "         [0.1565, 0.1306, 0.5750, 0.0577, 0.0802]],\n",
      "\n",
      "        [[0.1467, 0.2133, 0.2133, 0.2133, 0.2133],\n",
      "         [0.0896, 0.2319, 0.2262, 0.2262, 0.2262],\n",
      "         [0.1946, 0.2003, 0.2049, 0.2001, 0.2001],\n",
      "         [0.1080, 0.2041, 0.3369, 0.1502, 0.2007],\n",
      "         [0.1177, 0.2029, 0.3116, 0.1561, 0.2117]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1797, 0.2051, 0.2051, 0.2051, 0.2051],\n",
      "         [0.1733, 0.2383, 0.1961, 0.1961, 0.1961],\n",
      "         [0.2047, 0.1944, 0.1996, 0.2006, 0.2006],\n",
      "         [0.1865, 0.2293, 0.2062, 0.1759, 0.2021],\n",
      "         [0.2095, 0.1892, 0.1994, 0.2156, 0.1862]],\n",
      "\n",
      "        [[0.2259, 0.1935, 0.1935, 0.1935, 0.1935],\n",
      "         [0.1970, 0.2053, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2488, 0.1432, 0.1806, 0.2138, 0.2138],\n",
      "         [0.2504, 0.1440, 0.1817, 0.2088, 0.2151],\n",
      "         [0.1242, 0.2898, 0.2031, 0.1641, 0.2188]],\n",
      "\n",
      "        [[0.2155, 0.1961, 0.1961, 0.1961, 0.1961],\n",
      "         [0.1964, 0.1980, 0.2019, 0.2019, 0.2019],\n",
      "         [0.2296, 0.2190, 0.1623, 0.1945, 0.1945],\n",
      "         [0.2569, 0.2286, 0.1085, 0.2357, 0.1702],\n",
      "         [0.2494, 0.2091, 0.0680, 0.2190, 0.2544]],\n",
      "\n",
      "        [[0.2031, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2115, 0.1303, 0.2194, 0.2194, 0.2194],\n",
      "         [0.2020, 0.1849, 0.2063, 0.2034, 0.2034],\n",
      "         [0.1888, 0.2289, 0.1804, 0.2158, 0.1860],\n",
      "         [0.1790, 0.2532, 0.1650, 0.2278, 0.1750]],\n",
      "\n",
      "        [[0.1948, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2035, 0.2119, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1611, 0.1381, 0.3207, 0.1900, 0.1900],\n",
      "         [0.2105, 0.2200, 0.1729, 0.1957, 0.2008],\n",
      "         [0.2097, 0.2193, 0.1717, 0.1947, 0.2046]],\n",
      "\n",
      "        [[0.1794, 0.2051, 0.2051, 0.2051, 0.2051],\n",
      "         [0.0417, 0.4967, 0.1538, 0.1538, 0.1538],\n",
      "         [0.0890, 0.3550, 0.1870, 0.1845, 0.1845],\n",
      "         [0.0979, 0.3551, 0.1955, 0.1585, 0.1930],\n",
      "         [0.4890, 0.0428, 0.1324, 0.1967, 0.1391]],\n",
      "\n",
      "        [[0.2023, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1918, 0.2407, 0.1892, 0.1892, 0.1892],\n",
      "         [0.1891, 0.2100, 0.2250, 0.1879, 0.1879],\n",
      "         [0.2032, 0.1678, 0.1480, 0.2754, 0.2056],\n",
      "         [0.1970, 0.1499, 0.1252, 0.3047, 0.2233]],\n",
      "\n",
      "        [[0.1396, 0.2151, 0.2151, 0.2151, 0.2151],\n",
      "         [0.1569, 0.1952, 0.2160, 0.2160, 0.2160],\n",
      "         [0.1612, 0.1925, 0.2284, 0.2090, 0.2090],\n",
      "         [0.2233, 0.1947, 0.1706, 0.2288, 0.1827],\n",
      "         [0.2131, 0.1923, 0.1743, 0.2170, 0.2033]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1940, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.2181, 0.1757, 0.2021, 0.2021, 0.2021],\n",
      "         [0.1931, 0.2271, 0.1708, 0.2045, 0.2045],\n",
      "         [0.2132, 0.1808, 0.2417, 0.1632, 0.2011],\n",
      "         [0.1740, 0.2341, 0.1388, 0.2813, 0.1718]],\n",
      "\n",
      "        [[0.1748, 0.2063, 0.2063, 0.2063, 0.2063],\n",
      "         [0.1220, 0.3408, 0.1791, 0.1791, 0.1791],\n",
      "         [0.3400, 0.0598, 0.2448, 0.1777, 0.1777],\n",
      "         [0.2079, 0.1883, 0.2041, 0.1994, 0.2004],\n",
      "         [0.1964, 0.2046, 0.1979, 0.1998, 0.2014]],\n",
      "\n",
      "        [[0.2765, 0.1809, 0.1809, 0.1809, 0.1809],\n",
      "         [0.1501, 0.2246, 0.2084, 0.2084, 0.2084],\n",
      "         [0.3835, 0.1312, 0.1652, 0.1600, 0.1600],\n",
      "         [0.2656, 0.1293, 0.1509, 0.3064, 0.1477],\n",
      "         [0.2063, 0.1895, 0.1930, 0.2098, 0.2015]],\n",
      "\n",
      "        [[0.2348, 0.1913, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2458, 0.2002, 0.1847, 0.1847, 0.1847],\n",
      "         [0.4285, 0.1803, 0.1344, 0.1284, 0.1284],\n",
      "         [0.4280, 0.1604, 0.1150, 0.1875, 0.1091],\n",
      "         [0.3064, 0.1780, 0.1481, 0.1941, 0.1733]],\n",
      "\n",
      "        [[0.2214, 0.1946, 0.1946, 0.1946, 0.1946],\n",
      "         [0.1376, 0.2572, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2513, 0.1614, 0.2040, 0.1916, 0.1916],\n",
      "         [0.2634, 0.0988, 0.1661, 0.3271, 0.1446],\n",
      "         [0.2397, 0.1186, 0.1722, 0.2801, 0.1893]],\n",
      "\n",
      "        [[0.1842, 0.2039, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1403, 0.2201, 0.2132, 0.2132, 0.2132],\n",
      "         [0.2801, 0.1623, 0.2203, 0.1687, 0.1687],\n",
      "         [0.2300, 0.1831, 0.2081, 0.1928, 0.1860],\n",
      "         [0.1903, 0.2070, 0.1975, 0.2031, 0.2021]],\n",
      "\n",
      "        [[0.2370, 0.1907, 0.1907, 0.1907, 0.1907],\n",
      "         [0.0639, 0.1985, 0.2459, 0.2459, 0.2459],\n",
      "         [0.2575, 0.1943, 0.1798, 0.1842, 0.1842],\n",
      "         [0.3531, 0.1678, 0.1368, 0.1965, 0.1458],\n",
      "         [0.2628, 0.1791, 0.1612, 0.1943, 0.2025]],\n",
      "\n",
      "        [[0.2206, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1855, 0.2169, 0.1992, 0.1992, 0.1992],\n",
      "         [0.0963, 0.2450, 0.3638, 0.1475, 0.1475],\n",
      "         [0.1527, 0.1951, 0.2164, 0.2651, 0.1708],\n",
      "         [0.0944, 0.1589, 0.1982, 0.3053, 0.2432]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1834, 0.2041, 0.2041, 0.2041, 0.2041],\n",
      "         [0.2069, 0.1307, 0.2208, 0.2208, 0.2208],\n",
      "         [0.2435, 0.0708, 0.1062, 0.2897, 0.2897],\n",
      "         [0.1660, 0.2886, 0.2406, 0.1512, 0.1536],\n",
      "         [0.2409, 0.0950, 0.1289, 0.2819, 0.2532]],\n",
      "\n",
      "        [[0.1489, 0.2128, 0.2128, 0.2128, 0.2128],\n",
      "         [0.1659, 0.1631, 0.2237, 0.2237, 0.2237],\n",
      "         [0.1898, 0.1889, 0.2064, 0.2075, 0.2075],\n",
      "         [0.1803, 0.1780, 0.2237, 0.1913, 0.2267],\n",
      "         [0.1933, 0.1890, 0.2792, 0.2138, 0.1247]],\n",
      "\n",
      "        [[0.1720, 0.2070, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1753, 0.1980, 0.2089, 0.2089, 0.2089],\n",
      "         [0.1818, 0.2112, 0.1558, 0.2256, 0.2256],\n",
      "         [0.2294, 0.1725, 0.3079, 0.1378, 0.1523],\n",
      "         [0.1960, 0.2149, 0.1782, 0.2311, 0.1798]],\n",
      "\n",
      "        [[0.2214, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.2172, 0.1956, 0.1957, 0.1957, 0.1957],\n",
      "         [0.1828, 0.2073, 0.1958, 0.2071, 0.2071],\n",
      "         [0.2121, 0.1884, 0.1988, 0.2121, 0.1886],\n",
      "         [0.2006, 0.2385, 0.2204, 0.2006, 0.1398]],\n",
      "\n",
      "        [[0.1883, 0.2029, 0.2029, 0.2029, 0.2029],\n",
      "         [0.1774, 0.1436, 0.2263, 0.2263, 0.2263],\n",
      "         [0.1903, 0.1695, 0.2056, 0.2173, 0.2173],\n",
      "         [0.2003, 0.1980, 0.2018, 0.1970, 0.2029],\n",
      "         [0.2162, 0.1809, 0.2436, 0.1680, 0.1913]],\n",
      "\n",
      "        [[0.1998, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2038, 0.1804, 0.2053, 0.2053, 0.2053],\n",
      "         [0.2040, 0.1884, 0.1975, 0.2050, 0.2050],\n",
      "         [0.1959, 0.2027, 0.1986, 0.2074, 0.1954],\n",
      "         [0.2530, 0.2056, 0.2326, 0.1792, 0.1296]],\n",
      "\n",
      "        [[0.1688, 0.2078, 0.2078, 0.2078, 0.2078],\n",
      "         [0.2098, 0.2039, 0.1954, 0.1954, 0.1954],\n",
      "         [0.2411, 0.2251, 0.1267, 0.2035, 0.2035],\n",
      "         [0.1863, 0.1904, 0.2291, 0.1975, 0.1967],\n",
      "         [0.1959, 0.1973, 0.2087, 0.1995, 0.1987]],\n",
      "\n",
      "        [[0.1855, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2034, 0.2048, 0.1972, 0.1972, 0.1972],\n",
      "         [0.2079, 0.2100, 0.1851, 0.1985, 0.1985],\n",
      "         [0.2467, 0.2575, 0.1514, 0.1414, 0.2029],\n",
      "         [0.2202, 0.2260, 0.1632, 0.1565, 0.2341]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1487, 0.2128, 0.2128, 0.2128, 0.2128],\n",
      "         [0.2094, 0.2485, 0.1807, 0.1807, 0.1807],\n",
      "         [0.1578, 0.1074, 0.2954, 0.2197, 0.2197],\n",
      "         [0.2007, 0.2038, 0.1958, 0.2015, 0.1981],\n",
      "         [0.2101, 0.3446, 0.0937, 0.2398, 0.1119]],\n",
      "\n",
      "        [[0.2043, 0.1989, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1488, 0.1835, 0.2226, 0.2226, 0.2226],\n",
      "         [0.1386, 0.1806, 0.2197, 0.2305, 0.2305],\n",
      "         [0.1167, 0.1809, 0.2501, 0.1815, 0.2708],\n",
      "         [0.2491, 0.1965, 0.1649, 0.1961, 0.1935]],\n",
      "\n",
      "        [[0.1898, 0.2025, 0.2025, 0.2025, 0.2025],\n",
      "         [0.2368, 0.1112, 0.2173, 0.2173, 0.2173],\n",
      "         [0.1881, 0.2213, 0.2075, 0.1916, 0.1916],\n",
      "         [0.1706, 0.2366, 0.2079, 0.2079, 0.1770],\n",
      "         [0.1188, 0.2695, 0.1949, 0.1950, 0.2218]],\n",
      "\n",
      "        [[0.3983, 0.1504, 0.1504, 0.1504, 0.1504],\n",
      "         [0.1037, 0.1313, 0.2550, 0.2550, 0.2550],\n",
      "         [0.2892, 0.2486, 0.1371, 0.1625, 0.1625],\n",
      "         [0.1982, 0.1989, 0.2017, 0.2002, 0.2009],\n",
      "         [0.2213, 0.2119, 0.1787, 0.1957, 0.1924]],\n",
      "\n",
      "        [[0.2275, 0.1931, 0.1931, 0.1931, 0.1931],\n",
      "         [0.1966, 0.1984, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1994, 0.2002, 0.1970, 0.2017, 0.2017],\n",
      "         [0.1850, 0.1688, 0.2417, 0.2620, 0.1424],\n",
      "         [0.1966, 0.1922, 0.2098, 0.2140, 0.1875]],\n",
      "\n",
      "        [[0.1721, 0.2070, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1831, 0.1587, 0.2194, 0.2194, 0.2194],\n",
      "         [0.1522, 0.0983, 0.2198, 0.2649, 0.2649],\n",
      "         [0.1736, 0.1124, 0.2501, 0.1629, 0.3010],\n",
      "         [0.1999, 0.3276, 0.1320, 0.2149, 0.1255]],\n",
      "\n",
      "        [[0.2893, 0.1777, 0.1777, 0.1777, 0.1777],\n",
      "         [0.1798, 0.1908, 0.2098, 0.2098, 0.2098],\n",
      "         [0.2383, 0.2123, 0.1962, 0.1766, 0.1766],\n",
      "         [0.2137, 0.2056, 0.2004, 0.1869, 0.1935],\n",
      "         [0.1904, 0.1961, 0.2001, 0.2112, 0.2021]],\n",
      "\n",
      "        [[0.1098, 0.2225, 0.2225, 0.2225, 0.2225],\n",
      "         [0.1409, 0.1509, 0.2360, 0.2360, 0.2360],\n",
      "         [0.2408, 0.2312, 0.1729, 0.1776, 0.1776],\n",
      "         [0.2503, 0.2370, 0.1602, 0.1864, 0.1661],\n",
      "         [0.2178, 0.2099, 0.1609, 0.1783, 0.2331]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1841, 0.2040, 0.2040, 0.2040, 0.2040],\n",
      "         [0.1999, 0.1978, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2061, 0.1687, 0.1804, 0.2224, 0.2224],\n",
      "         [0.1828, 0.0960, 0.1191, 0.3689, 0.2333],\n",
      "         [0.1859, 0.2478, 0.2250, 0.1359, 0.2055]],\n",
      "\n",
      "        [[0.1400, 0.2150, 0.2150, 0.2150, 0.2150],\n",
      "         [0.1121, 0.1817, 0.2354, 0.2354, 0.2354],\n",
      "         [0.1315, 0.1752, 0.2848, 0.2042, 0.2042],\n",
      "         [0.0808, 0.1369, 0.3354, 0.2653, 0.1817],\n",
      "         [0.0782, 0.1348, 0.3397, 0.2667, 0.1806]],\n",
      "\n",
      "        [[0.1854, 0.2037, 0.2037, 0.2037, 0.2037],\n",
      "         [0.1797, 0.2390, 0.1938, 0.1938, 0.1938],\n",
      "         [0.1920, 0.2040, 0.2138, 0.1951, 0.1951],\n",
      "         [0.1768, 0.1984, 0.2169, 0.2256, 0.1823],\n",
      "         [0.1324, 0.1910, 0.2532, 0.2870, 0.1364]],\n",
      "\n",
      "        [[0.2002, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1961, 0.2917, 0.1707, 0.1707, 0.1707],\n",
      "         [0.1864, 0.2615, 0.2209, 0.1656, 0.1656],\n",
      "         [0.1754, 0.2418, 0.2060, 0.2200, 0.1567],\n",
      "         [0.1919, 0.2119, 0.2017, 0.2058, 0.1888]],\n",
      "\n",
      "        [[0.2900, 0.1775, 0.1775, 0.1775, 0.1775],\n",
      "         [0.2853, 0.2328, 0.1606, 0.1606, 0.1606],\n",
      "         [0.2070, 0.2022, 0.2035, 0.1937, 0.1937],\n",
      "         [0.2200, 0.1941, 0.2006, 0.2310, 0.1544],\n",
      "         [0.2030, 0.1881, 0.1919, 0.2092, 0.2078]],\n",
      "\n",
      "        [[0.1509, 0.2123, 0.2123, 0.2123, 0.2123],\n",
      "         [0.2074, 0.2110, 0.1939, 0.1939, 0.1939],\n",
      "         [0.1749, 0.1677, 0.2448, 0.2063, 0.2063],\n",
      "         [0.1310, 0.1211, 0.2454, 0.3243, 0.1782],\n",
      "         [0.1818, 0.1766, 0.2290, 0.2537, 0.1589]],\n",
      "\n",
      "        [[0.2375, 0.1906, 0.1906, 0.1906, 0.1906],\n",
      "         [0.2070, 0.2190, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2026, 0.2064, 0.1963, 0.1974, 0.1974],\n",
      "         [0.1730, 0.1395, 0.2502, 0.2032, 0.2342],\n",
      "         [0.2033, 0.2163, 0.1828, 0.1941, 0.2034]],\n",
      "\n",
      "        [[0.2254, 0.1937, 0.1937, 0.1937, 0.1937],\n",
      "         [0.1961, 0.2721, 0.1773, 0.1773, 0.1773],\n",
      "         [0.2099, 0.1752, 0.1712, 0.2219, 0.2219],\n",
      "         [0.1975, 0.2047, 0.2056, 0.1969, 0.1953],\n",
      "         [0.1757, 0.2114, 0.2165, 0.1731, 0.2234]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1428, 0.2143, 0.2143, 0.2143, 0.2143],\n",
      "         [0.1853, 0.2096, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2050, 0.1981, 0.1966, 0.2002, 0.2002],\n",
      "         [0.2287, 0.1982, 0.1923, 0.1736, 0.2072],\n",
      "         [0.2666, 0.1428, 0.1253, 0.0801, 0.3853]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2149, 0.1964, 0.1962, 0.1962, 0.1962],\n",
      "         [0.1983, 0.2186, 0.1453, 0.2189, 0.2189],\n",
      "         [0.2053, 0.2118, 0.1858, 0.1853, 0.2118],\n",
      "         [0.1999, 0.2008, 0.1969, 0.1968, 0.2055]],\n",
      "\n",
      "        [[0.2085, 0.1979, 0.1979, 0.1979, 0.1979],\n",
      "         [0.1860, 0.1859, 0.2094, 0.2094, 0.2094],\n",
      "         [0.2243, 0.2246, 0.1730, 0.1890, 0.1890],\n",
      "         [0.2067, 0.2068, 0.1944, 0.1935, 0.1985],\n",
      "         [0.1995, 0.1994, 0.2033, 0.2036, 0.1942]],\n",
      "\n",
      "        [[0.2027, 0.1993, 0.1993, 0.1993, 0.1993],\n",
      "         [0.1339, 0.2493, 0.2056, 0.2056, 0.2056],\n",
      "         [0.3446, 0.1162, 0.2139, 0.1627, 0.1627],\n",
      "         [0.2496, 0.1784, 0.2154, 0.1586, 0.1979],\n",
      "         [0.2252, 0.1850, 0.2066, 0.1727, 0.2104]],\n",
      "\n",
      "        [[0.2062, 0.1985, 0.1985, 0.1985, 0.1985],\n",
      "         [0.2026, 0.1992, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2153, 0.1980, 0.1883, 0.1992, 0.1992],\n",
      "         [0.2107, 0.1994, 0.1929, 0.1968, 0.2002],\n",
      "         [0.2021, 0.1918, 0.1859, 0.1894, 0.2308]],\n",
      "\n",
      "        [[0.1539, 0.2115, 0.2115, 0.2115, 0.2115],\n",
      "         [0.1871, 0.2060, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1993, 0.2003, 0.2002, 0.2001, 0.2001],\n",
      "         [0.2128, 0.2018, 0.2031, 0.1784, 0.2039],\n",
      "         [0.1905, 0.2069, 0.2048, 0.2507, 0.1471]],\n",
      "\n",
      "        [[0.1983, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1912, 0.2245, 0.1948, 0.1948, 0.1948],\n",
      "         [0.2302, 0.1347, 0.2022, 0.2164, 0.2164],\n",
      "         [0.2010, 0.1995, 0.2006, 0.1982, 0.2008],\n",
      "         [0.1607, 0.2254, 0.1744, 0.3007, 0.1388]],\n",
      "\n",
      "        [[0.2042, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1992, 0.2017, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1776, 0.2359, 0.2113, 0.1876, 0.1876],\n",
      "         [0.2003, 0.1990, 0.1995, 0.2012, 0.2001],\n",
      "         [0.1938, 0.2060, 0.2012, 0.1870, 0.2120]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1975, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2000, 0.2003, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1828, 0.2427, 0.2343, 0.1701, 0.1701],\n",
      "         [0.2005, 0.1994, 0.1995, 0.1998, 0.2008],\n",
      "         [0.2064, 0.1964, 0.1976, 0.1997, 0.1998]],\n",
      "\n",
      "        [[0.1811, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.2029, 0.2010, 0.1987, 0.1987, 0.1987],\n",
      "         [0.2460, 0.2098, 0.2026, 0.1708, 0.1708],\n",
      "         [0.2019, 0.1982, 0.1974, 0.2091, 0.1934],\n",
      "         [0.1989, 0.1967, 0.1963, 0.2030, 0.2051]],\n",
      "\n",
      "        [[0.2033, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2063, 0.2006, 0.1977, 0.1977, 0.1977],\n",
      "         [0.2144, 0.1755, 0.2926, 0.1587, 0.1587],\n",
      "         [0.2032, 0.1764, 0.2531, 0.2030, 0.1643],\n",
      "         [0.1988, 0.1858, 0.2208, 0.1987, 0.1958]],\n",
      "\n",
      "        [[0.1995, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2171, 0.1912, 0.1973, 0.1973, 0.1973],\n",
      "         [0.2009, 0.1955, 0.2100, 0.1968, 0.1968],\n",
      "         [0.2001, 0.1463, 0.3318, 0.1638, 0.1581],\n",
      "         [0.1911, 0.1655, 0.2409, 0.1743, 0.2281]],\n",
      "\n",
      "        [[0.1527, 0.2118, 0.2118, 0.2118, 0.2118],\n",
      "         [0.2024, 0.2011, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1848, 0.1958, 0.1879, 0.2158, 0.2158],\n",
      "         [0.2081, 0.1951, 0.2043, 0.2177, 0.1748],\n",
      "         [0.2023, 0.1942, 0.1999, 0.2080, 0.1956]],\n",
      "\n",
      "        [[0.1936, 0.2016, 0.2016, 0.2016, 0.2016],\n",
      "         [0.2269, 0.1723, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1869, 0.2080, 0.2127, 0.1962, 0.1962],\n",
      "         [0.1520, 0.2108, 0.2255, 0.2354, 0.1763],\n",
      "         [0.2282, 0.1969, 0.1910, 0.1873, 0.1966]],\n",
      "\n",
      "        [[0.2001, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2016, 0.1951, 0.2011, 0.2011, 0.2011],\n",
      "         [0.1966, 0.1993, 0.2105, 0.1968, 0.1968],\n",
      "         [0.1358, 0.1719, 0.4365, 0.1177, 0.1382],\n",
      "         [0.1794, 0.1959, 0.2769, 0.1701, 0.1777]],\n",
      "\n",
      "        [[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1978, 0.2126, 0.1965, 0.1965, 0.1965],\n",
      "         [0.1836, 0.2450, 0.2134, 0.1790, 0.1790],\n",
      "         [0.1891, 0.2563, 0.2215, 0.1491, 0.1841],\n",
      "         [0.1838, 0.2490, 0.2153, 0.1449, 0.2070]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1725, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1939, 0.2013, 0.2016, 0.2016, 0.2016],\n",
      "         [0.2221, 0.2036, 0.1686, 0.2029, 0.2029],\n",
      "         [0.1986, 0.1774, 0.1390, 0.3083, 0.1767],\n",
      "         [0.1904, 0.2080, 0.2519, 0.1348, 0.2149]],\n",
      "\n",
      "        [[0.1760, 0.2060, 0.2060, 0.2060, 0.2060],\n",
      "         [0.1715, 0.2247, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2025, 0.1929, 0.2110, 0.1968, 0.1968],\n",
      "         [0.1832, 0.1158, 0.2701, 0.2914, 0.1395],\n",
      "         [0.1948, 0.1759, 0.2123, 0.2159, 0.2009]],\n",
      "\n",
      "        [[0.3828, 0.1543, 0.1543, 0.1543, 0.1543],\n",
      "         [0.2080, 0.2007, 0.1971, 0.1971, 0.1971],\n",
      "         [0.2852, 0.1903, 0.2136, 0.1554, 0.1554],\n",
      "         [0.2703, 0.1814, 0.2032, 0.1965, 0.1485],\n",
      "         [0.2440, 0.1713, 0.1894, 0.1839, 0.2114]],\n",
      "\n",
      "        [[0.2634, 0.1841, 0.1841, 0.1841, 0.1841],\n",
      "         [0.1910, 0.1954, 0.2045, 0.2045, 0.2045],\n",
      "         [0.2240, 0.2081, 0.2084, 0.1798, 0.1798],\n",
      "         [0.2473, 0.2088, 0.2094, 0.1852, 0.1493],\n",
      "         [0.2495, 0.2015, 0.2022, 0.1731, 0.1737]],\n",
      "\n",
      "        [[0.3732, 0.1567, 0.1567, 0.1567, 0.1567],\n",
      "         [0.2707, 0.2157, 0.1712, 0.1712, 0.1712],\n",
      "         [0.1604, 0.1907, 0.1943, 0.2273, 0.2273],\n",
      "         [0.2146, 0.1965, 0.1946, 0.2148, 0.1796],\n",
      "         [0.2403, 0.1669, 0.1603, 0.2415, 0.1909]],\n",
      "\n",
      "        [[0.1826, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1702, 0.2076, 0.2074, 0.2074, 0.2074],\n",
      "         [0.2103, 0.1743, 0.2663, 0.1745, 0.1745],\n",
      "         [0.1977, 0.1800, 0.2223, 0.2199, 0.1801],\n",
      "         [0.2005, 0.2095, 0.1897, 0.1907, 0.2096]],\n",
      "\n",
      "        [[0.2240, 0.1940, 0.1940, 0.1940, 0.1940],\n",
      "         [0.2393, 0.1729, 0.1959, 0.1959, 0.1959],\n",
      "         [0.2571, 0.1414, 0.2455, 0.1780, 0.1780],\n",
      "         [0.2491, 0.1516, 0.2397, 0.1761, 0.1835],\n",
      "         [0.1788, 0.2170, 0.1815, 0.2047, 0.2179]],\n",
      "\n",
      "        [[0.1654, 0.2086, 0.2086, 0.2086, 0.2086],\n",
      "         [0.2144, 0.2928, 0.1643, 0.1643, 0.1643],\n",
      "         [0.1990, 0.2207, 0.2159, 0.1822, 0.1822],\n",
      "         [0.1983, 0.2200, 0.2151, 0.1852, 0.1814],\n",
      "         [0.1882, 0.2119, 0.2065, 0.1741, 0.2193]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1466, 0.2133, 0.2133, 0.2133, 0.2133],\n",
      "         [0.5372, 0.1311, 0.1105, 0.1105, 0.1105],\n",
      "         [0.3228, 0.1801, 0.1615, 0.1678, 0.1678],\n",
      "         [0.4774, 0.1614, 0.1318, 0.0878, 0.1416],\n",
      "         [0.2380, 0.1939, 0.1866, 0.1728, 0.2086]],\n",
      "\n",
      "        [[0.4039, 0.1490, 0.1490, 0.1490, 0.1490],\n",
      "         [0.3122, 0.1779, 0.1699, 0.1699, 0.1699],\n",
      "         [0.1355, 0.2208, 0.1842, 0.2298, 0.2298],\n",
      "         [0.2603, 0.1803, 0.2066, 0.1780, 0.1749],\n",
      "         [0.3410, 0.1497, 0.2031, 0.1454, 0.1609]],\n",
      "\n",
      "        [[0.2303, 0.1924, 0.1924, 0.1924, 0.1924],\n",
      "         [0.1977, 0.1969, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2126, 0.2167, 0.1832, 0.1937, 0.1937],\n",
      "         [0.1956, 0.1948, 0.2018, 0.2083, 0.1995],\n",
      "         [0.2512, 0.2638, 0.1704, 0.1148, 0.1998]],\n",
      "\n",
      "        [[0.2046, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1959, 0.1798, 0.2081, 0.2081, 0.2081],\n",
      "         [0.1995, 0.2067, 0.2047, 0.1945, 0.1945],\n",
      "         [0.1994, 0.1985, 0.1987, 0.2032, 0.2001],\n",
      "         [0.2022, 0.2062, 0.2051, 0.1875, 0.1990]],\n",
      "\n",
      "        [[0.2348, 0.1913, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2494, 0.1786, 0.1907, 0.1907, 0.1907],\n",
      "         [0.2080, 0.1867, 0.2239, 0.1907, 0.1907],\n",
      "         [0.1996, 0.1554, 0.2369, 0.2448, 0.1633],\n",
      "         [0.2002, 0.2034, 0.1979, 0.1975, 0.2009]],\n",
      "\n",
      "        [[0.2021, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.2595, 0.2410, 0.1665, 0.1665, 0.1665],\n",
      "         [0.1810, 0.1869, 0.1928, 0.2197, 0.2197],\n",
      "         [0.2045, 0.2031, 0.2017, 0.1947, 0.1960],\n",
      "         [0.2496, 0.2243, 0.2023, 0.1187, 0.2052]],\n",
      "\n",
      "        [[0.2020, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.2091, 0.1780, 0.2043, 0.2043, 0.2043],\n",
      "         [0.2210, 0.1824, 0.1667, 0.2149, 0.2149],\n",
      "         [0.2169, 0.2015, 0.1947, 0.1723, 0.2146],\n",
      "         [0.1882, 0.2002, 0.2061, 0.2287, 0.1768]],\n",
      "\n",
      "        [[0.1848, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1754, 0.2032, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1474, 0.2073, 0.2117, 0.2168, 0.2168],\n",
      "         [0.1734, 0.1976, 0.1992, 0.2288, 0.2010],\n",
      "         [0.1666, 0.1965, 0.1985, 0.2367, 0.2016]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0806, 0.2298, 0.2298, 0.2298, 0.2298],\n",
      "         [0.2016, 0.2003, 0.1994, 0.1994, 0.1994],\n",
      "         [0.3108, 0.1772, 0.2855, 0.1132, 0.1132],\n",
      "         [0.2251, 0.1945, 0.2202, 0.1870, 0.1732],\n",
      "         [0.2290, 0.1939, 0.2233, 0.1853, 0.1686]],\n",
      "\n",
      "        [[0.1603, 0.2099, 0.2099, 0.2099, 0.2099],\n",
      "         [0.2000, 0.1996, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1855, 0.0833, 0.2407, 0.2452, 0.2452],\n",
      "         [0.1838, 0.3303, 0.1519, 0.1842, 0.1499],\n",
      "         [0.1722, 0.3124, 0.1419, 0.1726, 0.2010]],\n",
      "\n",
      "        [[0.2115, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.1934, 0.1612, 0.2151, 0.2151, 0.2151],\n",
      "         [0.2107, 0.2820, 0.1516, 0.1778, 0.1778],\n",
      "         [0.1998, 0.1992, 0.2004, 0.2004, 0.2001],\n",
      "         [0.1938, 0.1784, 0.2128, 0.2127, 0.2024]],\n",
      "\n",
      "        [[0.3149, 0.1713, 0.1713, 0.1713, 0.1713],\n",
      "         [0.3742, 0.0793, 0.1822, 0.1822, 0.1822],\n",
      "         [0.1000, 0.2967, 0.2720, 0.1656, 0.1656],\n",
      "         [0.3059, 0.1371, 0.1462, 0.2000, 0.2108],\n",
      "         [0.1638, 0.2236, 0.2181, 0.1932, 0.2012]],\n",
      "\n",
      "        [[0.2337, 0.1916, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2073, 0.2956, 0.1657, 0.1657, 0.1657],\n",
      "         [0.1959, 0.1818, 0.2114, 0.2055, 0.2055],\n",
      "         [0.1734, 0.1271, 0.2370, 0.2515, 0.2109],\n",
      "         [0.2071, 0.2471, 0.1734, 0.1676, 0.2047]],\n",
      "\n",
      "        [[0.0778, 0.2305, 0.2305, 0.2305, 0.2305],\n",
      "         [0.1659, 0.1921, 0.2140, 0.2140, 0.2140],\n",
      "         [0.2044, 0.2005, 0.1999, 0.1976, 0.1976],\n",
      "         [0.2052, 0.1987, 0.1978, 0.2043, 0.1941],\n",
      "         [0.2051, 0.1979, 0.1968, 0.2041, 0.1961]],\n",
      "\n",
      "        [[0.4314, 0.1422, 0.1422, 0.1422, 0.1422],\n",
      "         [0.1796, 0.2000, 0.2068, 0.2068, 0.2068],\n",
      "         [0.0830, 0.1987, 0.1965, 0.2609, 0.2609],\n",
      "         [0.2390, 0.1989, 0.1994, 0.1749, 0.1878],\n",
      "         [0.3344, 0.1929, 0.1943, 0.1313, 0.1470]],\n",
      "\n",
      "        [[0.0844, 0.2289, 0.2289, 0.2289, 0.2289],\n",
      "         [0.2040, 0.2005, 0.1985, 0.1985, 0.1985],\n",
      "         [0.1499, 0.1842, 0.2493, 0.2083, 0.2083],\n",
      "         [0.0830, 0.1469, 0.3403, 0.2231, 0.2067],\n",
      "         [0.0499, 0.1098, 0.3497, 0.1953, 0.2952]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2862, 0.1784, 0.1784, 0.1784, 0.1784],\n",
      "         [0.1914, 0.1979, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2608, 0.1953, 0.2391, 0.1524, 0.1524],\n",
      "         [0.1845, 0.1967, 0.1881, 0.2228, 0.2078],\n",
      "         [0.2066, 0.1975, 0.2038, 0.1810, 0.2110]],\n",
      "\n",
      "        [[0.1986, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1781, 0.2318, 0.1967, 0.1967, 0.1967],\n",
      "         [0.1129, 0.3823, 0.1478, 0.1785, 0.1785],\n",
      "         [0.1350, 0.3508, 0.1667, 0.1542, 0.1933],\n",
      "         [0.2169, 0.1909, 0.2109, 0.2131, 0.1682]],\n",
      "\n",
      "        [[0.2059, 0.1985, 0.1985, 0.1985, 0.1985],\n",
      "         [0.1374, 0.4079, 0.1515, 0.1515, 0.1515],\n",
      "         [0.1264, 0.2820, 0.3199, 0.1359, 0.1359],\n",
      "         [0.1745, 0.2092, 0.2152, 0.2236, 0.1774],\n",
      "         [0.3768, 0.1827, 0.1630, 0.1400, 0.1376]],\n",
      "\n",
      "        [[0.1141, 0.2215, 0.2215, 0.2215, 0.2215],\n",
      "         [0.3759, 0.1816, 0.1475, 0.1475, 0.1475],\n",
      "         [0.2629, 0.1946, 0.1854, 0.1786, 0.1786],\n",
      "         [0.2642, 0.1826, 0.1721, 0.2168, 0.1643],\n",
      "         [0.2365, 0.1649, 0.1556, 0.1950, 0.2481]],\n",
      "\n",
      "        [[0.2018, 0.1996, 0.1996, 0.1996, 0.1996],\n",
      "         [0.1991, 0.2113, 0.1965, 0.1965, 0.1965],\n",
      "         [0.1825, 0.2509, 0.2254, 0.1706, 0.1706],\n",
      "         [0.1894, 0.2025, 0.1979, 0.2235, 0.1867],\n",
      "         [0.1499, 0.1827, 0.1709, 0.2449, 0.2515]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2479, 0.1915, 0.1869, 0.1869, 0.1869],\n",
      "         [0.2832, 0.1658, 0.2356, 0.1577, 0.1577],\n",
      "         [0.2546, 0.1742, 0.2235, 0.1796, 0.1681],\n",
      "         [0.1955, 0.2514, 0.2131, 0.2464, 0.0936]],\n",
      "\n",
      "        [[0.1830, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2038, 0.2194, 0.1922, 0.1922, 0.1922],\n",
      "         [0.1990, 0.2030, 0.2060, 0.1960, 0.1960],\n",
      "         [0.1979, 0.2011, 0.2034, 0.2021, 0.1955],\n",
      "         [0.1562, 0.1725, 0.1857, 0.1784, 0.3072]],\n",
      "\n",
      "        [[0.2147, 0.1963, 0.1963, 0.1963, 0.1963],\n",
      "         [0.1919, 0.2806, 0.1758, 0.1758, 0.1758],\n",
      "         [0.1929, 0.2471, 0.1955, 0.1822, 0.1822],\n",
      "         [0.2054, 0.1924, 0.2047, 0.1891, 0.2085],\n",
      "         [0.1850, 0.2004, 0.1858, 0.2048, 0.2239]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1772, 0.2057, 0.2057, 0.2057, 0.2057],\n",
      "         [0.1559, 0.2065, 0.2125, 0.2125, 0.2125],\n",
      "         [0.1750, 0.2046, 0.2046, 0.2079, 0.2079],\n",
      "         [0.1086, 0.1710, 0.1709, 0.3702, 0.1792],\n",
      "         [0.1844, 0.2060, 0.2060, 0.2488, 0.1547]],\n",
      "\n",
      "        [[0.1978, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2206, 0.2050, 0.1915, 0.1915, 0.1915],\n",
      "         [0.1963, 0.1988, 0.2027, 0.2011, 0.2011],\n",
      "         [0.2126, 0.2035, 0.1902, 0.1982, 0.1954],\n",
      "         [0.2270, 0.2120, 0.1907, 0.2034, 0.1668]],\n",
      "\n",
      "        [[0.0771, 0.2307, 0.2307, 0.2307, 0.2307],\n",
      "         [0.0867, 0.2431, 0.2234, 0.2234, 0.2234],\n",
      "         [0.1693, 0.2092, 0.2102, 0.2056, 0.2056],\n",
      "         [0.1386, 0.2196, 0.2217, 0.2087, 0.2114],\n",
      "         [0.0763, 0.2372, 0.2428, 0.2093, 0.2344]],\n",
      "\n",
      "        [[0.0583, 0.2354, 0.2354, 0.2354, 0.2354],\n",
      "         [0.0791, 0.1788, 0.2474, 0.2474, 0.2474],\n",
      "         [0.0695, 0.2026, 0.1073, 0.3103, 0.3103],\n",
      "         [0.1541, 0.1999, 0.1713, 0.2531, 0.2217],\n",
      "         [0.0841, 0.2097, 0.1219, 0.4802, 0.1040]],\n",
      "\n",
      "        [[0.0922, 0.2270, 0.2270, 0.2270, 0.2270],\n",
      "         [0.0735, 0.2335, 0.2310, 0.2310, 0.2310],\n",
      "         [0.2391, 0.1962, 0.1716, 0.1966, 0.1966],\n",
      "         [0.2312, 0.2013, 0.1832, 0.1829, 0.2015],\n",
      "         [0.0691, 0.1840, 0.3579, 0.3619, 0.0270]],\n",
      "\n",
      "        [[0.1618, 0.2096, 0.2096, 0.2096, 0.2096],\n",
      "         [0.1861, 0.2013, 0.2042, 0.2042, 0.2042],\n",
      "         [0.1870, 0.2164, 0.1520, 0.2223, 0.2223],\n",
      "         [0.1783, 0.2226, 0.1300, 0.2371, 0.2320],\n",
      "         [0.1948, 0.2356, 0.1486, 0.2487, 0.1722]],\n",
      "\n",
      "        [[0.1811, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.2379, 0.1754, 0.1956, 0.1956, 0.1956],\n",
      "         [0.1210, 0.2885, 0.1674, 0.2116, 0.2116],\n",
      "         [0.1695, 0.2211, 0.1872, 0.2211, 0.2011],\n",
      "         [0.2570, 0.1605, 0.2156, 0.1605, 0.2064]],\n",
      "\n",
      "        [[0.1442, 0.2140, 0.2140, 0.2140, 0.2140],\n",
      "         [0.1520, 0.0530, 0.2650, 0.2650, 0.2650],\n",
      "         [0.2067, 0.1702, 0.1653, 0.2289, 0.2289],\n",
      "         [0.2232, 0.1459, 0.1368, 0.2148, 0.2793],\n",
      "         [0.2572, 0.1412, 0.1288, 0.2436, 0.2292]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0973, 0.2257, 0.2257, 0.2257, 0.2257],\n",
      "         [0.2011, 0.1999, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1182, 0.2232, 0.1589, 0.2498, 0.2498],\n",
      "         [0.1933, 0.2019, 0.1973, 0.2040, 0.2034],\n",
      "         [0.1223, 0.2303, 0.1642, 0.2687, 0.2146]],\n",
      "\n",
      "        [[0.1991, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1977, 0.1129, 0.2298, 0.2298, 0.2298],\n",
      "         [0.2110, 0.1136, 0.1768, 0.2493, 0.2493],\n",
      "         [0.2157, 0.1572, 0.1970, 0.1953, 0.2348],\n",
      "         [0.3004, 0.1304, 0.2366, 0.2311, 0.1015]],\n",
      "\n",
      "        [[0.1547, 0.2113, 0.2113, 0.2113, 0.2113],\n",
      "         [0.1194, 0.1109, 0.2566, 0.2566, 0.2566],\n",
      "         [0.1153, 0.1061, 0.2338, 0.2724, 0.2724],\n",
      "         [0.1828, 0.1799, 0.2100, 0.2108, 0.2164],\n",
      "         [0.1580, 0.1483, 0.2710, 0.2750, 0.1478]],\n",
      "\n",
      "        [[0.2254, 0.1936, 0.1936, 0.1936, 0.1936],\n",
      "         [0.1631, 0.1510, 0.2286, 0.2286, 0.2286],\n",
      "         [0.1277, 0.1037, 0.1354, 0.3166, 0.3166],\n",
      "         [0.1776, 0.1614, 0.1824, 0.2094, 0.2692],\n",
      "         [0.2019, 0.1808, 0.2083, 0.2444, 0.1646]],\n",
      "\n",
      "        [[0.1434, 0.2142, 0.2142, 0.2142, 0.2142],\n",
      "         [0.1205, 0.0959, 0.2612, 0.2612, 0.2612],\n",
      "         [0.1416, 0.1229, 0.2779, 0.2288, 0.2288],\n",
      "         [0.1455, 0.1262, 0.2874, 0.2048, 0.2362],\n",
      "         [0.1520, 0.1253, 0.3830, 0.2417, 0.0980]],\n",
      "\n",
      "        [[0.1470, 0.2132, 0.2132, 0.2132, 0.2132],\n",
      "         [0.1820, 0.2048, 0.2044, 0.2044, 0.2044],\n",
      "         [0.1291, 0.2360, 0.1680, 0.2334, 0.2334],\n",
      "         [0.1385, 0.2391, 0.1758, 0.2099, 0.2367],\n",
      "         [0.1149, 0.3161, 0.1787, 0.2482, 0.1421]],\n",
      "\n",
      "        [[0.1853, 0.2037, 0.2037, 0.2037, 0.2037],\n",
      "         [0.2003, 0.1817, 0.2060, 0.2060, 0.2060],\n",
      "         [0.1963, 0.2191, 0.2042, 0.1902, 0.1902],\n",
      "         [0.2008, 0.2160, 0.2062, 0.1803, 0.1967],\n",
      "         [0.2024, 0.2087, 0.2047, 0.1935, 0.1908]],\n",
      "\n",
      "        [[0.1075, 0.2231, 0.2231, 0.2231, 0.2231],\n",
      "         [0.1427, 0.1448, 0.2375, 0.2375, 0.2375],\n",
      "         [0.1459, 0.1480, 0.2285, 0.2388, 0.2388],\n",
      "         [0.2170, 0.2160, 0.1875, 0.1948, 0.1848],\n",
      "         [0.1661, 0.1684, 0.2515, 0.2255, 0.1885]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1779, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.2217, 0.2215, 0.1856, 0.1856, 0.1856],\n",
      "         [0.1607, 0.1610, 0.2720, 0.2031, 0.2031],\n",
      "         [0.2704, 0.2700, 0.1557, 0.0925, 0.2115],\n",
      "         [0.1770, 0.1771, 0.2211, 0.2727, 0.1520]],\n",
      "\n",
      "        [[0.2239, 0.1940, 0.1940, 0.1940, 0.1940],\n",
      "         [0.1960, 0.2040, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1891, 0.2094, 0.2035, 0.1990, 0.1990],\n",
      "         [0.1572, 0.2262, 0.2043, 0.2236, 0.1887],\n",
      "         [0.1959, 0.2022, 0.2004, 0.2020, 0.1994]],\n",
      "\n",
      "        [[0.1911, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1816, 0.2448, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2154, 0.1694, 0.2020, 0.2066, 0.2066],\n",
      "         [0.2866, 0.1110, 0.2223, 0.1370, 0.2432],\n",
      "         [0.2940, 0.1287, 0.2356, 0.1546, 0.1871]],\n",
      "\n",
      "        [[0.1858, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2009, 0.1925, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1898, 0.2936, 0.1612, 0.1777, 0.1777],\n",
      "         [0.2273, 0.1438, 0.2697, 0.1157, 0.2435],\n",
      "         [0.1789, 0.2245, 0.1644, 0.2502, 0.1821]],\n",
      "\n",
      "        [[0.1977, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1956, 0.2136, 0.1969, 0.1969, 0.1969],\n",
      "         [0.2085, 0.1733, 0.2069, 0.2056, 0.2056],\n",
      "         [0.2019, 0.1151, 0.1974, 0.2919, 0.1936],\n",
      "         [0.1825, 0.2873, 0.1859, 0.1354, 0.2089]],\n",
      "\n",
      "        [[0.1952, 0.2012, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1735, 0.2098, 0.2056, 0.2056, 0.2056],\n",
      "         [0.2702, 0.1719, 0.1969, 0.1805, 0.1805],\n",
      "         [0.1824, 0.2123, 0.2028, 0.1937, 0.2088],\n",
      "         [0.2030, 0.1984, 0.1998, 0.2012, 0.1976]],\n",
      "\n",
      "        [[0.2164, 0.1959, 0.1959, 0.1959, 0.1959],\n",
      "         [0.2181, 0.1478, 0.2114, 0.2114, 0.2114],\n",
      "         [0.2068, 0.1678, 0.2185, 0.2034, 0.2034],\n",
      "         [0.2376, 0.1418, 0.2722, 0.1204, 0.2279],\n",
      "         [0.2057, 0.1949, 0.2086, 0.1917, 0.1992]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2141, 0.2157, 0.1901, 0.1901, 0.1901],\n",
      "         [0.1914, 0.1905, 0.2086, 0.2048, 0.2048],\n",
      "         [0.1867, 0.1831, 0.2745, 0.1031, 0.2527],\n",
      "         [0.2041, 0.1995, 0.3207, 0.1017, 0.1739]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2032, 0.1992, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1926, 0.1897, 0.2059, 0.2059, 0.2059],\n",
      "         [0.1721, 0.1691, 0.2879, 0.1854, 0.1854],\n",
      "         [0.2056, 0.2062, 0.1872, 0.1982, 0.2028],\n",
      "         [0.2037, 0.2043, 0.1863, 0.1967, 0.2090]],\n",
      "\n",
      "        [[0.1976, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1948, 0.2057, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1800, 0.2361, 0.1751, 0.2044, 0.2044],\n",
      "         [0.2081, 0.1504, 0.2151, 0.2477, 0.1787],\n",
      "         [0.1989, 0.2015, 0.1987, 0.1975, 0.2034]],\n",
      "\n",
      "        [[0.2227, 0.1943, 0.1943, 0.1943, 0.1943],\n",
      "         [0.1975, 0.1955, 0.2023, 0.2023, 0.2023],\n",
      "         [0.2037, 0.2061, 0.1942, 0.1980, 0.1980],\n",
      "         [0.2047, 0.2123, 0.1757, 0.2204, 0.1869],\n",
      "         [0.1893, 0.1817, 0.2242, 0.1744, 0.2304]],\n",
      "\n",
      "        [[0.1914, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.1927, 0.2136, 0.1979, 0.1979, 0.1979],\n",
      "         [0.1995, 0.2048, 0.1940, 0.2008, 0.2008],\n",
      "         [0.2111, 0.1737, 0.2593, 0.1550, 0.2008],\n",
      "         [0.1627, 0.2311, 0.1123, 0.2838, 0.2101]],\n",
      "\n",
      "        [[0.1780, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.2013, 0.2061, 0.1975, 0.1975, 0.1975],\n",
      "         [0.2035, 0.2235, 0.1963, 0.1883, 0.1883],\n",
      "         [0.1992, 0.2042, 0.1974, 0.2039, 0.1953],\n",
      "         [0.2105, 0.1749, 0.2259, 0.1765, 0.2121]],\n",
      "\n",
      "        [[0.1827, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.2138, 0.1970, 0.1964, 0.1964, 0.1964],\n",
      "         [0.2596, 0.1569, 0.2761, 0.1537, 0.1537],\n",
      "         [0.2442, 0.1687, 0.2555, 0.1654, 0.1662],\n",
      "         [0.1812, 0.2090, 0.1781, 0.2105, 0.2212]],\n",
      "\n",
      "        [[0.3284, 0.1679, 0.1679, 0.1679, 0.1679],\n",
      "         [0.2058, 0.2142, 0.1933, 0.1933, 0.1933],\n",
      "         [0.2466, 0.3546, 0.1195, 0.1396, 0.1396],\n",
      "         [0.1703, 0.1304, 0.2901, 0.1505, 0.2587],\n",
      "         [0.1990, 0.2231, 0.1583, 0.2098, 0.2098]],\n",
      "\n",
      "        [[0.1994, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1785, 0.2100, 0.2039, 0.2039, 0.2039],\n",
      "         [0.1985, 0.2014, 0.1983, 0.2009, 0.2009],\n",
      "         [0.2016, 0.2005, 0.2016, 0.1956, 0.2007],\n",
      "         [0.2016, 0.2001, 0.2017, 0.1927, 0.2039]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1737, 0.2066, 0.2066, 0.2066, 0.2066],\n",
      "         [0.1757, 0.2082, 0.2054, 0.2054, 0.2054],\n",
      "         [0.1990, 0.2035, 0.1913, 0.2031, 0.2031],\n",
      "         [0.1997, 0.2008, 0.1977, 0.2011, 0.2007],\n",
      "         [0.2057, 0.1835, 0.2510, 0.1789, 0.1810]],\n",
      "\n",
      "        [[0.1993, 0.2002, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2434, 0.1118, 0.2149, 0.2149, 0.2149],\n",
      "         [0.1816, 0.2606, 0.1730, 0.1924, 0.1924],\n",
      "         [0.1828, 0.2198, 0.1783, 0.2308, 0.1883],\n",
      "         [0.2191, 0.1889, 0.2235, 0.1816, 0.1869]],\n",
      "\n",
      "        [[0.1784, 0.2054, 0.2054, 0.2054, 0.2054],\n",
      "         [0.2372, 0.1336, 0.2097, 0.2097, 0.2097],\n",
      "         [0.1335, 0.3117, 0.2346, 0.1601, 0.1601],\n",
      "         [0.1212, 0.3094, 0.2259, 0.1954, 0.1481],\n",
      "         [0.1914, 0.2103, 0.2038, 0.2008, 0.1937]],\n",
      "\n",
      "        [[0.1347, 0.2163, 0.2163, 0.2163, 0.2163],\n",
      "         [0.2290, 0.1896, 0.1938, 0.1938, 0.1938],\n",
      "         [0.2041, 0.1986, 0.1989, 0.1992, 0.1992],\n",
      "         [0.1687, 0.2131, 0.2099, 0.2009, 0.2074],\n",
      "         [0.1560, 0.2095, 0.2056, 0.1945, 0.2343]],\n",
      "\n",
      "        [[0.2126, 0.1969, 0.1969, 0.1969, 0.1969],\n",
      "         [0.1810, 0.1659, 0.2177, 0.2177, 0.2177],\n",
      "         [0.2035, 0.2139, 0.2166, 0.1830, 0.1830],\n",
      "         [0.2090, 0.2360, 0.2435, 0.1502, 0.1614],\n",
      "         [0.1991, 0.2025, 0.2034, 0.1900, 0.2050]],\n",
      "\n",
      "        [[0.1977, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2017, 0.1862, 0.2040, 0.2040, 0.2040],\n",
      "         [0.1728, 0.1991, 0.2895, 0.1693, 0.1693],\n",
      "         [0.1927, 0.1987, 0.2153, 0.2014, 0.1919],\n",
      "         [0.2030, 0.2010, 0.1958, 0.2001, 0.2001]],\n",
      "\n",
      "        [[0.0812, 0.2297, 0.2297, 0.2297, 0.2297],\n",
      "         [0.1629, 0.2360, 0.2004, 0.2004, 0.2004],\n",
      "         [0.0949, 0.3191, 0.2124, 0.1868, 0.1868],\n",
      "         [0.1828, 0.2140, 0.2030, 0.2006, 0.1996],\n",
      "         [0.1200, 0.2517, 0.1963, 0.1857, 0.2463]],\n",
      "\n",
      "        [[0.1780, 0.2055, 0.2055, 0.2055, 0.2055],\n",
      "         [0.2029, 0.2121, 0.1950, 0.1950, 0.1950],\n",
      "         [0.2328, 0.2876, 0.0938, 0.1929, 0.1929],\n",
      "         [0.2109, 0.2195, 0.1773, 0.1889, 0.2034],\n",
      "         [0.2401, 0.2674, 0.1512, 0.1791, 0.1622]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2234, 0.1942, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2001, 0.2341, 0.1886, 0.1886, 0.1886],\n",
      "         [0.1933, 0.2205, 0.2184, 0.1839, 0.1839],\n",
      "         [0.1907, 0.1541, 0.1566, 0.2921, 0.2065],\n",
      "         [0.1897, 0.2497, 0.2446, 0.1093, 0.2067]],\n",
      "\n",
      "        [[0.1990, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1926, 0.2149, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1951, 0.2035, 0.2076, 0.1970, 0.1970],\n",
      "         [0.2029, 0.1973, 0.1947, 0.2034, 0.2017],\n",
      "         [0.1682, 0.2009, 0.2185, 0.1660, 0.2465]],\n",
      "\n",
      "        [[0.1718, 0.2070, 0.2070, 0.2070, 0.2070],\n",
      "         [0.1775, 0.2449, 0.1926, 0.1926, 0.1926],\n",
      "         [0.1647, 0.2224, 0.2574, 0.1778, 0.1778],\n",
      "         [0.1279, 0.2220, 0.2903, 0.2128, 0.1470],\n",
      "         [0.1180, 0.2165, 0.2909, 0.2068, 0.1678]],\n",
      "\n",
      "        [[0.4024, 0.1494, 0.1494, 0.1494, 0.1494],\n",
      "         [0.3162, 0.2100, 0.1579, 0.1579, 0.1579],\n",
      "         [0.3659, 0.2094, 0.1407, 0.1420, 0.1420],\n",
      "         [0.2519, 0.1975, 0.1661, 0.2177, 0.1667],\n",
      "         [0.2612, 0.1515, 0.1028, 0.1885, 0.2960]],\n",
      "\n",
      "        [[0.1692, 0.2077, 0.2077, 0.2077, 0.2077],\n",
      "         [0.1871, 0.1893, 0.2079, 0.2079, 0.2079],\n",
      "         [0.2192, 0.2161, 0.1796, 0.1926, 0.1926],\n",
      "         [0.1677, 0.1710, 0.2204, 0.2407, 0.2003],\n",
      "         [0.1259, 0.1305, 0.2069, 0.2428, 0.2941]],\n",
      "\n",
      "        [[0.2313, 0.1922, 0.1922, 0.1922, 0.1922],\n",
      "         [0.2005, 0.2052, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1935, 0.2410, 0.2207, 0.1724, 0.1724],\n",
      "         [0.1920, 0.2315, 0.2148, 0.1876, 0.1741],\n",
      "         [0.1691, 0.2398, 0.2084, 0.1618, 0.2209]],\n",
      "\n",
      "        [[0.3141, 0.1715, 0.1715, 0.1715, 0.1715],\n",
      "         [0.1423, 0.1502, 0.2358, 0.2358, 0.2358],\n",
      "         [0.2160, 0.2123, 0.2052, 0.1833, 0.1833],\n",
      "         [0.2108, 0.2080, 0.2025, 0.1933, 0.1854],\n",
      "         [0.2143, 0.2016, 0.1790, 0.1453, 0.2597]],\n",
      "\n",
      "        [[0.2255, 0.1936, 0.1936, 0.1936, 0.1936],\n",
      "         [0.2070, 0.2846, 0.1695, 0.1695, 0.1695],\n",
      "         [0.1873, 0.2412, 0.2518, 0.1599, 0.1599],\n",
      "         [0.1716, 0.2255, 0.2363, 0.2221, 0.1445],\n",
      "         [0.1430, 0.1942, 0.2045, 0.1908, 0.2674]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1723, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1830, 0.1764, 0.2135, 0.2135, 0.2135],\n",
      "         [0.1361, 0.1198, 0.2799, 0.2321, 0.2321],\n",
      "         [0.1949, 0.1917, 0.2142, 0.1902, 0.2090],\n",
      "         [0.1616, 0.1493, 0.2522, 0.1438, 0.2931]],\n",
      "\n",
      "        [[0.1955, 0.2011, 0.2011, 0.2011, 0.2011],\n",
      "         [0.1345, 0.1426, 0.2410, 0.2410, 0.2410],\n",
      "         [0.2233, 0.2199, 0.1743, 0.1912, 0.1912],\n",
      "         [0.1130, 0.1208, 0.3279, 0.2181, 0.2202],\n",
      "         [0.1378, 0.1442, 0.2849, 0.2158, 0.2173]],\n",
      "\n",
      "        [[0.1833, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.3475, 0.1370, 0.1718, 0.1718, 0.1718],\n",
      "         [0.2280, 0.1778, 0.2163, 0.1889, 0.1889],\n",
      "         [0.1972, 0.2041, 0.1986, 0.1976, 0.2024],\n",
      "         [0.1893, 0.2185, 0.1951, 0.1911, 0.2059]],\n",
      "\n",
      "        [[0.1653, 0.2087, 0.2087, 0.2087, 0.2087],\n",
      "         [0.1961, 0.2003, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2214, 0.1918, 0.2153, 0.1858, 0.1858],\n",
      "         [0.1616, 0.2287, 0.1729, 0.1898, 0.2470],\n",
      "         [0.2168, 0.1663, 0.2058, 0.1917, 0.2193]],\n",
      "\n",
      "        [[0.2056, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.2238, 0.1606, 0.2052, 0.2052, 0.2052],\n",
      "         [0.2328, 0.1787, 0.1541, 0.2172, 0.2172],\n",
      "         [0.1485, 0.2266, 0.2871, 0.1719, 0.1659],\n",
      "         [0.2157, 0.1542, 0.1278, 0.1920, 0.3102]],\n",
      "\n",
      "        [[0.1980, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2070, 0.1288, 0.2214, 0.2214, 0.2214],\n",
      "         [0.1885, 0.1228, 0.2882, 0.2002, 0.2002],\n",
      "         [0.1998, 0.1956, 0.2040, 0.2003, 0.2004],\n",
      "         [0.1867, 0.1248, 0.2783, 0.1965, 0.2137]],\n",
      "\n",
      "        [[0.1428, 0.2143, 0.2143, 0.2143, 0.2143],\n",
      "         [0.3647, 0.1018, 0.1778, 0.1778, 0.1778],\n",
      "         [0.2467, 0.1546, 0.2193, 0.1897, 0.1897],\n",
      "         [0.1788, 0.2301, 0.1906, 0.1945, 0.2061],\n",
      "         [0.2259, 0.1558, 0.2057, 0.1996, 0.2131]],\n",
      "\n",
      "        [[0.1887, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1711, 0.2448, 0.1947, 0.1947, 0.1947],\n",
      "         [0.2387, 0.1791, 0.1520, 0.2151, 0.2151],\n",
      "         [0.2474, 0.1866, 0.1589, 0.1837, 0.2234],\n",
      "         [0.3172, 0.1851, 0.1361, 0.1795, 0.1821]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1956, 0.2011, 0.2011, 0.2011, 0.2011],\n",
      "         [0.2005, 0.1965, 0.2010, 0.2010, 0.2010],\n",
      "         [0.1954, 0.2724, 0.1563, 0.1880, 0.1880],\n",
      "         [0.1996, 0.1456, 0.2467, 0.2011, 0.2070],\n",
      "         [0.1477, 0.3872, 0.0773, 0.1444, 0.2434]],\n",
      "\n",
      "        [[0.1723, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1955, 0.1993, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2017, 0.2000, 0.2006, 0.1989, 0.1989],\n",
      "         [0.1825, 0.1973, 0.1919, 0.2210, 0.2073],\n",
      "         [0.2418, 0.1980, 0.2125, 0.1481, 0.1996]],\n",
      "\n",
      "        [[0.2026, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1710, 0.2511, 0.1926, 0.1926, 0.1926],\n",
      "         [0.2169, 0.1610, 0.2268, 0.1977, 0.1977],\n",
      "         [0.1724, 0.2430, 0.1637, 0.2290, 0.1918],\n",
      "         [0.2036, 0.1909, 0.2056, 0.1930, 0.2069]],\n",
      "\n",
      "        [[0.1700, 0.2075, 0.2075, 0.2075, 0.2075],\n",
      "         [0.1216, 0.2163, 0.2207, 0.2207, 0.2207],\n",
      "         [0.2405, 0.1859, 0.2050, 0.1843, 0.1843],\n",
      "         [0.1683, 0.2169, 0.1970, 0.1989, 0.2189],\n",
      "         [0.2073, 0.1959, 0.2002, 0.1997, 0.1969]],\n",
      "\n",
      "        [[0.1975, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2010, 0.2056, 0.1978, 0.1978, 0.1978],\n",
      "         [0.2002, 0.1994, 0.1990, 0.2007, 0.2007],\n",
      "         [0.2112, 0.2020, 0.1977, 0.1711, 0.2181],\n",
      "         [0.1940, 0.1983, 0.2004, 0.2151, 0.1922]],\n",
      "\n",
      "        [[0.2056, 0.1986, 0.1986, 0.1986, 0.1986],\n",
      "         [0.2076, 0.1768, 0.2052, 0.2052, 0.2052],\n",
      "         [0.2024, 0.1666, 0.2318, 0.1996, 0.1996],\n",
      "         [0.2065, 0.1533, 0.2541, 0.1841, 0.2021],\n",
      "         [0.1792, 0.2718, 0.1341, 0.2104, 0.2046]],\n",
      "\n",
      "        [[0.1385, 0.2154, 0.2154, 0.2154, 0.2154],\n",
      "         [0.1667, 0.2435, 0.1966, 0.1966, 0.1966],\n",
      "         [0.2641, 0.1078, 0.2704, 0.1789, 0.1789],\n",
      "         [0.1696, 0.2492, 0.1679, 0.2127, 0.2005],\n",
      "         [0.1362, 0.2650, 0.1338, 0.2015, 0.2635]],\n",
      "\n",
      "        [[0.1725, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1819, 0.2090, 0.2030, 0.2030, 0.2030],\n",
      "         [0.2090, 0.1844, 0.2281, 0.1893, 0.1893],\n",
      "         [0.1808, 0.2250, 0.1550, 0.2242, 0.2150],\n",
      "         [0.1958, 0.2320, 0.1739, 0.2314, 0.1669]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2715, 0.1821, 0.1821, 0.1821, 0.1821],\n",
      "         [0.2180, 0.1824, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2237, 0.1569, 0.2432, 0.1882, 0.1882],\n",
      "         [0.2048, 0.1931, 0.2076, 0.1954, 0.1990],\n",
      "         [0.2496, 0.1454, 0.2835, 0.1622, 0.1593]],\n",
      "\n",
      "        [[0.2174, 0.1956, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2048, 0.1933, 0.2006, 0.2006, 0.2006],\n",
      "         [0.2029, 0.1955, 0.2012, 0.2002, 0.2002],\n",
      "         [0.2014, 0.1832, 0.1970, 0.2237, 0.1947],\n",
      "         [0.2111, 0.1784, 0.2031, 0.2544, 0.1529]],\n",
      "\n",
      "        [[0.2302, 0.1924, 0.1924, 0.1924, 0.1924],\n",
      "         [0.2521, 0.1479, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1966, 0.2024, 0.2029, 0.1991, 0.1991],\n",
      "         [0.2060, 0.1978, 0.1971, 0.1967, 0.2024],\n",
      "         [0.3569, 0.1946, 0.1838, 0.1782, 0.0865]],\n",
      "\n",
      "        [[0.0987, 0.2253, 0.2253, 0.2253, 0.2253],\n",
      "         [0.1801, 0.2063, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1877, 0.2028, 0.2057, 0.2019, 0.2019],\n",
      "         [0.2295, 0.1883, 0.1816, 0.2100, 0.1906],\n",
      "         [0.1470, 0.2240, 0.2419, 0.1777, 0.2094]],\n",
      "\n",
      "        [[0.2254, 0.1937, 0.1937, 0.1937, 0.1937],\n",
      "         [0.2118, 0.1900, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2306, 0.1810, 0.1854, 0.2015, 0.2015],\n",
      "         [0.2232, 0.1885, 0.1917, 0.1934, 0.2032],\n",
      "         [0.2473, 0.1824, 0.1880, 0.1910, 0.1914]],\n",
      "\n",
      "        [[0.1544, 0.2114, 0.2114, 0.2114, 0.2114],\n",
      "         [0.1857, 0.1714, 0.2143, 0.2143, 0.2143],\n",
      "         [0.1684, 0.1540, 0.2830, 0.1973, 0.1973],\n",
      "         [0.2282, 0.2430, 0.1579, 0.1670, 0.2039],\n",
      "         [0.1201, 0.0979, 0.3963, 0.3304, 0.0553]],\n",
      "\n",
      "        [[0.1120, 0.2220, 0.2220, 0.2220, 0.2220],\n",
      "         [0.1141, 0.1463, 0.2465, 0.2465, 0.2465],\n",
      "         [0.2032, 0.2015, 0.1992, 0.1981, 0.1981],\n",
      "         [0.1774, 0.1906, 0.2111, 0.1992, 0.2217],\n",
      "         [0.1333, 0.1687, 0.2355, 0.1949, 0.2676]],\n",
      "\n",
      "        [[0.2071, 0.1982, 0.1982, 0.1982, 0.1982],\n",
      "         [0.2026, 0.2029, 0.1982, 0.1982, 0.1982],\n",
      "         [0.1943, 0.1939, 0.2131, 0.1994, 0.1994],\n",
      "         [0.2053, 0.2055, 0.1965, 0.1898, 0.2028],\n",
      "         [0.2241, 0.2247, 0.2009, 0.1844, 0.1659]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1979, 0.1942, 0.2026, 0.2026, 0.2026],\n",
      "         [0.1786, 0.1729, 0.2761, 0.1862, 0.1862],\n",
      "         [0.2129, 0.2172, 0.1629, 0.1994, 0.2076],\n",
      "         [0.2090, 0.2130, 0.1616, 0.1962, 0.2202]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1992, 0.2022, 0.1995, 0.1995, 0.1995],\n",
      "         [0.2020, 0.1816, 0.2174, 0.1995, 0.1995],\n",
      "         [0.1986, 0.1888, 0.2056, 0.2097, 0.1974],\n",
      "         [0.1776, 0.2392, 0.1448, 0.1289, 0.3093]],\n",
      "\n",
      "        [[0.1548, 0.2113, 0.2113, 0.2113, 0.2113],\n",
      "         [0.1814, 0.2150, 0.2012, 0.2012, 0.2012],\n",
      "         [0.2188, 0.1868, 0.1971, 0.1987, 0.1987],\n",
      "         [0.1816, 0.2239, 0.2085, 0.1796, 0.2064],\n",
      "         [0.0986, 0.1993, 0.1569, 0.0949, 0.4503]],\n",
      "\n",
      "        [[0.2940, 0.1765, 0.1765, 0.1765, 0.1765],\n",
      "         [0.2090, 0.1936, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2412, 0.1700, 0.2020, 0.1934, 0.1934],\n",
      "         [0.2095, 0.1947, 0.2018, 0.1940, 0.2000],\n",
      "         [0.2377, 0.1833, 0.2084, 0.1811, 0.1894]],\n",
      "\n",
      "        [[0.2009, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1896, 0.2440, 0.1888, 0.1888, 0.1888],\n",
      "         [0.2136, 0.1620, 0.1952, 0.2146, 0.2146],\n",
      "         [0.2008, 0.1983, 0.2000, 0.2000, 0.2009],\n",
      "         [0.1697, 0.2520, 0.1930, 0.1922, 0.1932]],\n",
      "\n",
      "        [[0.2072, 0.1982, 0.1982, 0.1982, 0.1982],\n",
      "         [0.1995, 0.1920, 0.2028, 0.2028, 0.2028],\n",
      "         [0.2015, 0.2027, 0.1940, 0.2009, 0.2009],\n",
      "         [0.1776, 0.1683, 0.2451, 0.2272, 0.1818],\n",
      "         [0.1884, 0.2148, 0.0856, 0.1031, 0.4081]],\n",
      "\n",
      "        [[0.1377, 0.2156, 0.2156, 0.2156, 0.2156],\n",
      "         [0.2247, 0.2199, 0.1851, 0.1851, 0.1851],\n",
      "         [0.2181, 0.2146, 0.1910, 0.1881, 0.1881],\n",
      "         [0.1942, 0.1958, 0.2078, 0.1927, 0.2095],\n",
      "         [0.2000, 0.2000, 0.1999, 0.2000, 0.2001]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2083, 0.1879, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2061, 0.1953, 0.1937, 0.2024, 0.2024],\n",
      "         [0.2327, 0.1911, 0.1857, 0.1726, 0.2179],\n",
      "         [0.2167, 0.1738, 0.1683, 0.1551, 0.2861]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1748, 0.2063, 0.2063, 0.2063, 0.2063],\n",
      "         [0.2130, 0.1917, 0.1984, 0.1984, 0.1984],\n",
      "         [0.1493, 0.2611, 0.1548, 0.2174, 0.2174],\n",
      "         [0.2553, 0.1697, 0.2487, 0.1323, 0.1940],\n",
      "         [0.1608, 0.2475, 0.1653, 0.3220, 0.1044]],\n",
      "\n",
      "        [[0.1979, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2004, 0.2130, 0.1955, 0.1955, 0.1955],\n",
      "         [0.1992, 0.1928, 0.2043, 0.2018, 0.2018],\n",
      "         [0.1986, 0.1923, 0.2037, 0.2042, 0.2012],\n",
      "         [0.2053, 0.2264, 0.1904, 0.1890, 0.1888]],\n",
      "\n",
      "        [[0.2034, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2028, 0.1979, 0.1998, 0.1998, 0.1998],\n",
      "         [0.2025, 0.2003, 0.1949, 0.2011, 0.2011],\n",
      "         [0.2016, 0.2006, 0.1983, 0.1985, 0.2010],\n",
      "         [0.2024, 0.1996, 0.1931, 0.1937, 0.2111]],\n",
      "\n",
      "        [[0.2024, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2034, 0.1945, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2000, 0.2227, 0.1640, 0.2066, 0.2066],\n",
      "         [0.2023, 0.1943, 0.2179, 0.1856, 0.1998],\n",
      "         [0.1934, 0.2216, 0.1505, 0.2588, 0.1758]],\n",
      "\n",
      "        [[0.2011, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2004, 0.1989, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2018, 0.1569, 0.2503, 0.1955, 0.1955],\n",
      "         [0.2019, 0.1556, 0.2521, 0.1950, 0.1954],\n",
      "         [0.2004, 0.2399, 0.1719, 0.2053, 0.1824]],\n",
      "\n",
      "        [[0.2113, 0.1972, 0.1972, 0.1972, 0.1972],\n",
      "         [0.1839, 0.2106, 0.2018, 0.2018, 0.2018],\n",
      "         [0.2191, 0.1956, 0.1799, 0.2027, 0.2027],\n",
      "         [0.1764, 0.2053, 0.2296, 0.1929, 0.1957],\n",
      "         [0.3051, 0.1885, 0.1323, 0.2297, 0.1445]],\n",
      "\n",
      "        [[0.1893, 0.2027, 0.2027, 0.2027, 0.2027],\n",
      "         [0.2088, 0.1915, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2079, 0.1958, 0.1926, 0.2018, 0.2018],\n",
      "         [0.2120, 0.1857, 0.1791, 0.2247, 0.1984],\n",
      "         [0.2496, 0.1660, 0.1483, 0.2983, 0.1378]],\n",
      "\n",
      "        [[0.1983, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.2043, 0.1949, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1989, 0.2004, 0.2014, 0.1996, 0.1996],\n",
      "         [0.2080, 0.1960, 0.1884, 0.2048, 0.2028],\n",
      "         [0.1721, 0.2171, 0.2531, 0.1827, 0.1751]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2349, 0.1913, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2118, 0.1977, 0.1968, 0.1968, 0.1968],\n",
      "         [0.1459, 0.1861, 0.2898, 0.1891, 0.1891],\n",
      "         [0.2090, 0.2026, 0.1913, 0.1950, 0.2021],\n",
      "         [0.2062, 0.2007, 0.1912, 0.1944, 0.2075]],\n",
      "\n",
      "        [[0.2304, 0.1924, 0.1924, 0.1924, 0.1924],\n",
      "         [0.1907, 0.2072, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1881, 0.2049, 0.2105, 0.1983, 0.1983],\n",
      "         [0.2117, 0.2009, 0.1976, 0.1849, 0.2050],\n",
      "         [0.1538, 0.1861, 0.1977, 0.2519, 0.2105]],\n",
      "\n",
      "        [[0.2482, 0.1879, 0.1879, 0.1879, 0.1879],\n",
      "         [0.2145, 0.1937, 0.1973, 0.1973, 0.1973],\n",
      "         [0.2563, 0.1561, 0.2457, 0.1709, 0.1709],\n",
      "         [0.2995, 0.1310, 0.2793, 0.1379, 0.1524],\n",
      "         [0.2491, 0.1523, 0.2389, 0.1570, 0.2027]],\n",
      "\n",
      "        [[0.1244, 0.2189, 0.2189, 0.2189, 0.2189],\n",
      "         [0.1393, 0.2047, 0.2187, 0.2187, 0.2187],\n",
      "         [0.1403, 0.2066, 0.2114, 0.2209, 0.2209],\n",
      "         [0.0715, 0.1737, 0.1830, 0.3694, 0.2024],\n",
      "         [0.0885, 0.1740, 0.1811, 0.3094, 0.2470]],\n",
      "\n",
      "        [[0.2015, 0.1996, 0.1996, 0.1996, 0.1996],\n",
      "         [0.2057, 0.2208, 0.1912, 0.1912, 0.1912],\n",
      "         [0.1635, 0.1244, 0.2789, 0.2166, 0.2166],\n",
      "         [0.2049, 0.2153, 0.1860, 0.1990, 0.1947],\n",
      "         [0.2149, 0.2579, 0.1505, 0.1931, 0.1835]],\n",
      "\n",
      "        [[0.1894, 0.2026, 0.2026, 0.2026, 0.2026],\n",
      "         [0.2282, 0.2581, 0.1712, 0.1712, 0.1712],\n",
      "         [0.1930, 0.1882, 0.2094, 0.2047, 0.2047],\n",
      "         [0.1694, 0.1562, 0.2208, 0.2486, 0.2049],\n",
      "         [0.1785, 0.1667, 0.2232, 0.2468, 0.1848]],\n",
      "\n",
      "        [[0.1600, 0.2100, 0.2100, 0.2100, 0.2100],\n",
      "         [0.2157, 0.1968, 0.1958, 0.1958, 0.1958],\n",
      "         [0.1644, 0.1948, 0.2473, 0.1967, 0.1967],\n",
      "         [0.0852, 0.1448, 0.3055, 0.3151, 0.1493],\n",
      "         [0.1186, 0.1714, 0.2876, 0.2938, 0.1287]],\n",
      "\n",
      "        [[0.1988, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1843, 0.2080, 0.2026, 0.2026, 0.2026],\n",
      "         [0.1922, 0.2228, 0.1535, 0.2158, 0.2158],\n",
      "         [0.2011, 0.2606, 0.1356, 0.1563, 0.2464],\n",
      "         [0.2191, 0.2603, 0.1686, 0.1853, 0.1667]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1660, 0.2085, 0.2085, 0.2085, 0.2085],\n",
      "         [0.1690, 0.1880, 0.2143, 0.2143, 0.2143],\n",
      "         [0.1990, 0.2036, 0.1785, 0.2094, 0.2094],\n",
      "         [0.1917, 0.2078, 0.1305, 0.2406, 0.2294],\n",
      "         [0.1994, 0.1812, 0.3146, 0.1523, 0.1525]],\n",
      "\n",
      "        [[0.2189, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.2124, 0.1659, 0.2072, 0.2072, 0.2072],\n",
      "         [0.2222, 0.1151, 0.2467, 0.2080, 0.2080],\n",
      "         [0.2332, 0.1116, 0.2621, 0.1766, 0.2165],\n",
      "         [0.1705, 0.2519, 0.1603, 0.1975, 0.2198]],\n",
      "\n",
      "        [[0.1816, 0.2046, 0.2046, 0.2046, 0.2046],\n",
      "         [0.2158, 0.1718, 0.2041, 0.2041, 0.2041],\n",
      "         [0.2179, 0.1684, 0.2045, 0.2046, 0.2046],\n",
      "         [0.1874, 0.2463, 0.2005, 0.1655, 0.2003],\n",
      "         [0.1767, 0.2946, 0.2005, 0.1401, 0.1881]],\n",
      "\n",
      "        [[0.1026, 0.2244, 0.2244, 0.2244, 0.2244],\n",
      "         [0.2668, 0.1815, 0.1839, 0.1839, 0.1839],\n",
      "         [0.1277, 0.2212, 0.2169, 0.2171, 0.2171],\n",
      "         [0.1993, 0.2003, 0.2002, 0.2000, 0.2002],\n",
      "         [0.1289, 0.2211, 0.2169, 0.1939, 0.2392]],\n",
      "\n",
      "        [[0.2002, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2013, 0.1956, 0.2011, 0.2011, 0.2011],\n",
      "         [0.2152, 0.0649, 0.3090, 0.2054, 0.2054],\n",
      "         [0.1790, 0.0805, 0.2279, 0.3391, 0.1735],\n",
      "         [0.1899, 0.3070, 0.1643, 0.1294, 0.2095]],\n",
      "\n",
      "        [[0.2024, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1868, 0.1897, 0.2078, 0.2078, 0.2078],\n",
      "         [0.1967, 0.1977, 0.1993, 0.2032, 0.2032],\n",
      "         [0.1952, 0.1977, 0.2020, 0.1924, 0.2128],\n",
      "         [0.1962, 0.1931, 0.1880, 0.1997, 0.2230]],\n",
      "\n",
      "        [[0.1573, 0.2107, 0.2107, 0.2107, 0.2107],\n",
      "         [0.3529, 0.1502, 0.1656, 0.1656, 0.1656],\n",
      "         [0.2441, 0.1846, 0.1900, 0.1906, 0.1906],\n",
      "         [0.1715, 0.2103, 0.2059, 0.2069, 0.2054],\n",
      "         [0.1862, 0.2053, 0.2032, 0.2037, 0.2016]],\n",
      "\n",
      "        [[0.1624, 0.2094, 0.2094, 0.2094, 0.2094],\n",
      "         [0.2062, 0.1985, 0.1984, 0.1984, 0.1984],\n",
      "         [0.2394, 0.2055, 0.1453, 0.2049, 0.2049],\n",
      "         [0.3061, 0.2217, 0.1068, 0.1450, 0.2204],\n",
      "         [0.2269, 0.2120, 0.1819, 0.1939, 0.1853]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2387, 0.1903, 0.1903, 0.1903, 0.1903],\n",
      "         [0.2010, 0.1988, 0.2001, 0.2001, 0.2001],\n",
      "         [0.2307, 0.1593, 0.2126, 0.1987, 0.1987],\n",
      "         [0.1954, 0.2224, 0.2011, 0.1752, 0.2059],\n",
      "         [0.2257, 0.1251, 0.1982, 0.3713, 0.0796]],\n",
      "\n",
      "        [[0.1999, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1841, 0.2335, 0.1941, 0.1941, 0.1941],\n",
      "         [0.1079, 0.1551, 0.5030, 0.1170, 0.1170],\n",
      "         [0.1684, 0.1863, 0.2584, 0.2147, 0.1723],\n",
      "         [0.1685, 0.1851, 0.2516, 0.2115, 0.1833]],\n",
      "\n",
      "        [[0.2147, 0.1963, 0.1963, 0.1963, 0.1963],\n",
      "         [0.2120, 0.2130, 0.1917, 0.1917, 0.1917],\n",
      "         [0.2037, 0.2043, 0.2107, 0.1906, 0.1906],\n",
      "         [0.1963, 0.1945, 0.1765, 0.1903, 0.2423],\n",
      "         [0.2037, 0.2025, 0.1908, 0.1998, 0.2032]],\n",
      "\n",
      "        [[0.2114, 0.1971, 0.1971, 0.1971, 0.1971],\n",
      "         [0.1375, 0.3578, 0.1682, 0.1682, 0.1682],\n",
      "         [0.0955, 0.4317, 0.2100, 0.1314, 0.1314],\n",
      "         [0.1478, 0.2378, 0.1894, 0.2616, 0.1634],\n",
      "         [0.1452, 0.2364, 0.1873, 0.2607, 0.1704]],\n",
      "\n",
      "        [[0.2165, 0.1959, 0.1959, 0.1959, 0.1959],\n",
      "         [0.2333, 0.2661, 0.1669, 0.1669, 0.1669],\n",
      "         [0.2031, 0.2063, 0.2004, 0.1951, 0.1951],\n",
      "         [0.1994, 0.1903, 0.2073, 0.1785, 0.2245],\n",
      "         [0.1953, 0.2164, 0.1793, 0.2492, 0.1599]],\n",
      "\n",
      "        [[0.2226, 0.1943, 0.1943, 0.1943, 0.1943],\n",
      "         [0.2170, 0.1824, 0.2002, 0.2002, 0.2002],\n",
      "         [0.2187, 0.1688, 0.2245, 0.1940, 0.1940],\n",
      "         [0.1997, 0.1360, 0.2075, 0.2896, 0.1671],\n",
      "         [0.1960, 0.2089, 0.1947, 0.1842, 0.2163]],\n",
      "\n",
      "        [[0.2453, 0.1887, 0.1887, 0.1887, 0.1887],\n",
      "         [0.2242, 0.2636, 0.1707, 0.1707, 0.1707],\n",
      "         [0.1988, 0.1975, 0.2014, 0.2011, 0.2011],\n",
      "         [0.2003, 0.2005, 0.2000, 0.1992, 0.2000],\n",
      "         [0.2174, 0.2298, 0.1961, 0.1524, 0.2043]],\n",
      "\n",
      "        [[0.3470, 0.1632, 0.1632, 0.1632, 0.1632],\n",
      "         [0.2501, 0.2232, 0.1756, 0.1756, 0.1756],\n",
      "         [0.2805, 0.2261, 0.2069, 0.1433, 0.1433],\n",
      "         [0.1947, 0.1983, 0.1998, 0.2010, 0.2062],\n",
      "         [0.2059, 0.1988, 0.1959, 0.1938, 0.2057]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0726, 0.2318, 0.2318, 0.2318, 0.2318],\n",
      "         [0.0796, 0.1755, 0.2483, 0.2483, 0.2483],\n",
      "         [0.2135, 0.1978, 0.2062, 0.1913, 0.1913],\n",
      "         [0.1287, 0.2120, 0.1619, 0.2333, 0.2640],\n",
      "         [0.1934, 0.2030, 0.1978, 0.2050, 0.2009]],\n",
      "\n",
      "        [[0.0463, 0.2384, 0.2384, 0.2384, 0.2384],\n",
      "         [0.1738, 0.1688, 0.2191, 0.2191, 0.2191],\n",
      "         [0.1560, 0.1490, 0.2466, 0.2241, 0.2241],\n",
      "         [0.1107, 0.1000, 0.3043, 0.2387, 0.2463],\n",
      "         [0.2453, 0.2613, 0.1303, 0.1516, 0.2115]],\n",
      "\n",
      "        [[0.1984, 0.2004, 0.2004, 0.2004, 0.2004],\n",
      "         [0.1420, 0.1830, 0.2250, 0.2250, 0.2250],\n",
      "         [0.2547, 0.2171, 0.1470, 0.1906, 0.1906],\n",
      "         [0.2879, 0.2330, 0.1390, 0.1440, 0.1961],\n",
      "         [0.2459, 0.2147, 0.1542, 0.1577, 0.2275]],\n",
      "\n",
      "        [[0.2240, 0.1940, 0.1940, 0.1940, 0.1940],\n",
      "         [0.2440, 0.1809, 0.1917, 0.1917, 0.1917],\n",
      "         [0.1666, 0.2070, 0.2294, 0.1985, 0.1985],\n",
      "         [0.1608, 0.2162, 0.2487, 0.1702, 0.2041],\n",
      "         [0.1732, 0.2106, 0.2310, 0.1798, 0.2053]],\n",
      "\n",
      "        [[0.1678, 0.2080, 0.2080, 0.2080, 0.2080],\n",
      "         [0.1975, 0.1939, 0.2029, 0.2029, 0.2029],\n",
      "         [0.1314, 0.0881, 0.3124, 0.2341, 0.2341],\n",
      "         [0.1737, 0.1512, 0.2345, 0.2285, 0.2121],\n",
      "         [0.2005, 0.2011, 0.1992, 0.1993, 0.1999]],\n",
      "\n",
      "        [[0.0893, 0.2277, 0.2277, 0.2277, 0.2277],\n",
      "         [0.0514, 0.1847, 0.2546, 0.2546, 0.2546],\n",
      "         [0.1171, 0.1999, 0.2259, 0.2285, 0.2285],\n",
      "         [0.1543, 0.2006, 0.2130, 0.2179, 0.2143],\n",
      "         [0.2343, 0.1828, 0.1727, 0.1691, 0.2410]],\n",
      "\n",
      "        [[0.2373, 0.1907, 0.1907, 0.1907, 0.1907],\n",
      "         [0.2399, 0.1904, 0.1899, 0.1899, 0.1899],\n",
      "         [0.1949, 0.2039, 0.1930, 0.2041, 0.2041],\n",
      "         [0.1866, 0.2015, 0.1835, 0.2267, 0.2017],\n",
      "         [0.2110, 0.2010, 0.2133, 0.1864, 0.1883]],\n",
      "\n",
      "        [[0.1218, 0.2196, 0.2196, 0.2196, 0.2196],\n",
      "         [0.1854, 0.2064, 0.2028, 0.2028, 0.2028],\n",
      "         [0.2438, 0.1935, 0.1609, 0.2009, 0.2009],\n",
      "         [0.2860, 0.1902, 0.1374, 0.1830, 0.2033],\n",
      "         [0.2038, 0.2004, 0.1978, 0.2001, 0.1978]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3321, 0.1670, 0.1670, 0.1670, 0.1670],\n",
      "         [0.2781, 0.2866, 0.1451, 0.1451, 0.1451],\n",
      "         [0.2003, 0.2003, 0.2000, 0.1997, 0.1997],\n",
      "         [0.1324, 0.1289, 0.1816, 0.3226, 0.2345],\n",
      "         [0.2299, 0.2327, 0.1993, 0.1537, 0.1844]],\n",
      "\n",
      "        [[0.2213, 0.1947, 0.1947, 0.1947, 0.1947],\n",
      "         [0.3514, 0.1614, 0.1624, 0.1624, 0.1624],\n",
      "         [0.3619, 0.1331, 0.2368, 0.1341, 0.1341],\n",
      "         [0.1737, 0.2189, 0.1916, 0.1973, 0.2185],\n",
      "         [0.2156, 0.1877, 0.2033, 0.1998, 0.1936]],\n",
      "\n",
      "        [[0.1814, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1981, 0.1949, 0.2023, 0.2023, 0.2023],\n",
      "         [0.1922, 0.2144, 0.2593, 0.1671, 0.1671],\n",
      "         [0.1991, 0.1824, 0.1567, 0.2390, 0.2227],\n",
      "         [0.1948, 0.1710, 0.1363, 0.2556, 0.2424]],\n",
      "\n",
      "        [[0.1915, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2585, 0.2517, 0.1632, 0.1632, 0.1632],\n",
      "         [0.2434, 0.2381, 0.1874, 0.1656, 0.1656],\n",
      "         [0.2294, 0.2261, 0.1937, 0.1721, 0.1787],\n",
      "         [0.2103, 0.2085, 0.1901, 0.1772, 0.2139]],\n",
      "\n",
      "        [[0.1860, 0.2035, 0.2035, 0.2035, 0.2035],\n",
      "         [0.2175, 0.1904, 0.1974, 0.1974, 0.1974],\n",
      "         [0.2144, 0.1652, 0.2658, 0.1773, 0.1773],\n",
      "         [0.1829, 0.1233, 0.2533, 0.3034, 0.1372],\n",
      "         [0.1983, 0.1912, 0.2043, 0.2077, 0.1985]],\n",
      "\n",
      "        [[0.1922, 0.2020, 0.2020, 0.2020, 0.2020],\n",
      "         [0.2032, 0.3122, 0.1615, 0.1615, 0.1615],\n",
      "         [0.2010, 0.1939, 0.1951, 0.2050, 0.2050],\n",
      "         [0.1514, 0.2921, 0.2593, 0.1907, 0.1066],\n",
      "         [0.2276, 0.1326, 0.1463, 0.1883, 0.3052]],\n",
      "\n",
      "        [[0.1298, 0.2175, 0.2175, 0.2175, 0.2175],\n",
      "         [0.0928, 0.1660, 0.2471, 0.2471, 0.2471],\n",
      "         [0.2189, 0.1981, 0.2128, 0.1851, 0.1851],\n",
      "         [0.1515, 0.1895, 0.1614, 0.2768, 0.2208],\n",
      "         [0.1588, 0.1993, 0.1693, 0.2931, 0.1794]],\n",
      "\n",
      "        [[0.3480, 0.1630, 0.1630, 0.1630, 0.1630],\n",
      "         [0.2379, 0.2315, 0.1769, 0.1769, 0.1769],\n",
      "         [0.2621, 0.2484, 0.1958, 0.1468, 0.1468],\n",
      "         [0.1785, 0.1812, 0.1935, 0.2375, 0.2094],\n",
      "         [0.2421, 0.2326, 0.1948, 0.1120, 0.2185]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2084, 0.1979, 0.1979, 0.1979, 0.1979],\n",
      "         [0.2008, 0.2282, 0.1903, 0.1903, 0.1903],\n",
      "         [0.1948, 0.2650, 0.1979, 0.1711, 0.1711],\n",
      "         [0.1993, 0.2023, 0.1994, 0.2010, 0.1980],\n",
      "         [0.1818, 0.2600, 0.1852, 0.2245, 0.1484]],\n",
      "\n",
      "        [[0.2266, 0.1933, 0.1933, 0.1933, 0.1933],\n",
      "         [0.2988, 0.1959, 0.1684, 0.1684, 0.1684],\n",
      "         [0.0930, 0.1586, 0.3644, 0.1919, 0.1919],\n",
      "         [0.2385, 0.2025, 0.1568, 0.2112, 0.1910],\n",
      "         [0.4291, 0.1863, 0.0508, 0.2312, 0.1026]],\n",
      "\n",
      "        [[0.2225, 0.1944, 0.1944, 0.1944, 0.1944],\n",
      "         [0.2031, 0.2002, 0.1989, 0.1989, 0.1989],\n",
      "         [0.1954, 0.1969, 0.2124, 0.1976, 0.1976],\n",
      "         [0.1891, 0.1913, 0.2147, 0.2126, 0.1923],\n",
      "         [0.2744, 0.2577, 0.1380, 0.1458, 0.1841]],\n",
      "\n",
      "        [[0.1809, 0.2048, 0.2048, 0.2048, 0.2048],\n",
      "         [0.2141, 0.2152, 0.1902, 0.1902, 0.1902],\n",
      "         [0.1756, 0.1731, 0.1577, 0.2468, 0.2468],\n",
      "         [0.1772, 0.1794, 0.1944, 0.3169, 0.1321],\n",
      "         [0.2233, 0.2207, 0.2054, 0.1323, 0.2183]],\n",
      "\n",
      "        [[0.2619, 0.1845, 0.1845, 0.1845, 0.1845],\n",
      "         [0.2485, 0.2092, 0.1808, 0.1808, 0.1808],\n",
      "         [0.1777, 0.1977, 0.1917, 0.2165, 0.2165],\n",
      "         [0.1842, 0.1941, 0.1912, 0.2275, 0.2030],\n",
      "         [0.2674, 0.2204, 0.2332, 0.1229, 0.1561]],\n",
      "\n",
      "        [[0.2081, 0.1980, 0.1980, 0.1980, 0.1980],\n",
      "         [0.1999, 0.1863, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1972, 0.2371, 0.1940, 0.1858, 0.1858],\n",
      "         [0.1998, 0.1820, 0.2015, 0.2108, 0.2059],\n",
      "         [0.1829, 0.3845, 0.1711, 0.1194, 0.1421]],\n",
      "\n",
      "        [[0.1888, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1970, 0.1621, 0.2136, 0.2136, 0.2136],\n",
      "         [0.2020, 0.2653, 0.1722, 0.1803, 0.1803],\n",
      "         [0.1808, 0.1390, 0.2109, 0.2675, 0.2017],\n",
      "         [0.1854, 0.1576, 0.2040, 0.2364, 0.2167]],\n",
      "\n",
      "        [[0.2001, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2057, 0.1804, 0.2046, 0.2046, 0.2046],\n",
      "         [0.1755, 0.1975, 0.2742, 0.1764, 0.1764],\n",
      "         [0.1704, 0.1882, 0.2481, 0.2221, 0.1711],\n",
      "         [0.1404, 0.1627, 0.2449, 0.2079, 0.2441]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1542, 0.2115, 0.2115, 0.2115, 0.2115],\n",
      "         [0.1295, 0.1489, 0.2405, 0.2405, 0.2405],\n",
      "         [0.1650, 0.1766, 0.2113, 0.2236, 0.2236],\n",
      "         [0.1530, 0.1641, 0.1972, 0.2766, 0.2090],\n",
      "         [0.2692, 0.2374, 0.1707, 0.0929, 0.2298]],\n",
      "\n",
      "        [[0.1129, 0.2218, 0.2218, 0.2218, 0.2218],\n",
      "         [0.1342, 0.2310, 0.2116, 0.2116, 0.2116],\n",
      "         [0.2131, 0.1946, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1663, 0.2189, 0.2094, 0.1960, 0.2094],\n",
      "         [0.2051, 0.1965, 0.1978, 0.1999, 0.2007]],\n",
      "\n",
      "        [[0.2331, 0.1917, 0.1917, 0.1917, 0.1917],\n",
      "         [0.1849, 0.1943, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1951, 0.1986, 0.2000, 0.2032, 0.2032],\n",
      "         [0.1936, 0.1975, 0.1990, 0.2075, 0.2024],\n",
      "         [0.2142, 0.2049, 0.2014, 0.1834, 0.1962]],\n",
      "\n",
      "        [[0.2565, 0.1859, 0.1859, 0.1859, 0.1859],\n",
      "         [0.1693, 0.2070, 0.2079, 0.2079, 0.2079],\n",
      "         [0.2595, 0.1790, 0.2064, 0.1776, 0.1776],\n",
      "         [0.1789, 0.2003, 0.1918, 0.2282, 0.2008],\n",
      "         [0.2212, 0.1935, 0.2037, 0.1657, 0.2159]],\n",
      "\n",
      "        [[0.2074, 0.1981, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1902, 0.2047, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1649, 0.2283, 0.1791, 0.2138, 0.2138],\n",
      "         [0.1643, 0.2529, 0.1833, 0.1676, 0.2319],\n",
      "         [0.2134, 0.1534, 0.1962, 0.2102, 0.2269]],\n",
      "\n",
      "        [[0.2013, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1737, 0.1438, 0.2275, 0.2275, 0.2275],\n",
      "         [0.1996, 0.1973, 0.1971, 0.2030, 0.2030],\n",
      "         [0.1874, 0.1501, 0.1475, 0.2577, 0.2573],\n",
      "         [0.1937, 0.2243, 0.2269, 0.1569, 0.1982]],\n",
      "\n",
      "        [[0.1975, 0.2006, 0.2006, 0.2006, 0.2006],\n",
      "         [0.1480, 0.2158, 0.2121, 0.2121, 0.2121],\n",
      "         [0.1930, 0.2035, 0.1975, 0.2030, 0.2030],\n",
      "         [0.1597, 0.1991, 0.1758, 0.2684, 0.1970],\n",
      "         [0.1622, 0.2026, 0.1786, 0.2741, 0.1825]],\n",
      "\n",
      "        [[0.1765, 0.2059, 0.2059, 0.2059, 0.2059],\n",
      "         [0.2029, 0.2069, 0.1967, 0.1967, 0.1967],\n",
      "         [0.2130, 0.2419, 0.1966, 0.1742, 0.1742],\n",
      "         [0.2089, 0.2166, 0.2042, 0.1730, 0.1973],\n",
      "         [0.2085, 0.2175, 0.2031, 0.1676, 0.2033]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0964, 0.2259, 0.2259, 0.2259, 0.2259],\n",
      "         [0.1592, 0.2250, 0.2053, 0.2053, 0.2053],\n",
      "         [0.2193, 0.1891, 0.1982, 0.1967, 0.1967],\n",
      "         [0.2018, 0.1993, 0.2001, 0.1990, 0.1999],\n",
      "         [0.1687, 0.2348, 0.2115, 0.2433, 0.1416]],\n",
      "\n",
      "        [[0.1355, 0.2161, 0.2161, 0.2161, 0.2161],\n",
      "         [0.1378, 0.1634, 0.2329, 0.2329, 0.2329],\n",
      "         [0.2240, 0.2141, 0.1721, 0.1949, 0.1949],\n",
      "         [0.2202, 0.2132, 0.1824, 0.1847, 0.1994],\n",
      "         [0.1351, 0.1531, 0.2801, 0.2670, 0.1646]],\n",
      "\n",
      "        [[0.1812, 0.2047, 0.2047, 0.2047, 0.2047],\n",
      "         [0.1541, 0.2350, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2872, 0.1751, 0.1235, 0.2071, 0.2071],\n",
      "         [0.2803, 0.1936, 0.1491, 0.1575, 0.2195],\n",
      "         [0.1218, 0.1952, 0.2724, 0.2540, 0.1566]],\n",
      "\n",
      "        [[0.2059, 0.1985, 0.1985, 0.1985, 0.1985],\n",
      "         [0.2000, 0.2263, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2040, 0.2388, 0.1721, 0.1926, 0.1926],\n",
      "         [0.2064, 0.1661, 0.2607, 0.1435, 0.2233],\n",
      "         [0.1996, 0.2009, 0.1982, 0.2018, 0.1996]],\n",
      "\n",
      "        [[0.1861, 0.2035, 0.2035, 0.2035, 0.2035],\n",
      "         [0.1875, 0.1820, 0.2102, 0.2102, 0.2102],\n",
      "         [0.1920, 0.1903, 0.2206, 0.1986, 0.1986],\n",
      "         [0.1808, 0.1787, 0.2171, 0.2343, 0.1890],\n",
      "         [0.1724, 0.1693, 0.2293, 0.2582, 0.1709]],\n",
      "\n",
      "        [[0.1047, 0.2238, 0.2238, 0.2238, 0.2238],\n",
      "         [0.1336, 0.2226, 0.2146, 0.2146, 0.2146],\n",
      "         [0.2037, 0.1990, 0.1985, 0.1994, 0.1994],\n",
      "         [0.1985, 0.2000, 0.2001, 0.2015, 0.1999],\n",
      "         [0.0992, 0.2027, 0.2187, 0.4153, 0.0640]],\n",
      "\n",
      "        [[0.1914, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1962, 0.1685, 0.2118, 0.2118, 0.2118],\n",
      "         [0.2030, 0.1744, 0.1843, 0.2191, 0.2191],\n",
      "         [0.2132, 0.1762, 0.1888, 0.1873, 0.2345],\n",
      "         [0.2022, 0.1974, 0.1991, 0.1989, 0.2024]],\n",
      "\n",
      "        [[0.1990, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.1977, 0.2086, 0.1979, 0.1979, 0.1979],\n",
      "         [0.2120, 0.1799, 0.1856, 0.2113, 0.2113],\n",
      "         [0.2116, 0.1922, 0.1957, 0.1893, 0.2112],\n",
      "         [0.1953, 0.1997, 0.1989, 0.2005, 0.2056]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2306, 0.1924, 0.1924, 0.1924, 0.1924],\n",
      "         [0.2012, 0.2031, 0.1986, 0.1986, 0.1986],\n",
      "         [0.1996, 0.2064, 0.2142, 0.1899, 0.1899],\n",
      "         [0.2005, 0.2177, 0.2383, 0.1659, 0.1776],\n",
      "         [0.2008, 0.1971, 0.1930, 0.2097, 0.1994]],\n",
      "\n",
      "        [[0.2938, 0.1765, 0.1765, 0.1765, 0.1765],\n",
      "         [0.1960, 0.2002, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2575, 0.1905, 0.1970, 0.1775, 0.1775],\n",
      "         [0.2231, 0.1930, 0.1962, 0.2012, 0.1865],\n",
      "         [0.1589, 0.1953, 0.1909, 0.1842, 0.2707]],\n",
      "\n",
      "        [[0.2100, 0.1975, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1798, 0.2323, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1706, 0.2366, 0.2120, 0.1904, 0.1904],\n",
      "         [0.1374, 0.2413, 0.1997, 0.2556, 0.1660],\n",
      "         [0.1601, 0.2055, 0.1890, 0.2108, 0.2347]],\n",
      "\n",
      "        [[0.2932, 0.1767, 0.1767, 0.1767, 0.1767],\n",
      "         [0.1889, 0.2024, 0.2029, 0.2029, 0.2029],\n",
      "         [0.2255, 0.1955, 0.1899, 0.1946, 0.1946],\n",
      "         [0.1882, 0.2036, 0.2070, 0.1971, 0.2042],\n",
      "         [0.2545, 0.1834, 0.1714, 0.2101, 0.1806]],\n",
      "\n",
      "        [[0.1997, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1999, 0.1999, 0.2000, 0.2000, 0.2000],\n",
      "         [0.1955, 0.1939, 0.1914, 0.2096, 0.2096],\n",
      "         [0.2000, 0.1983, 0.1956, 0.1908, 0.2153],\n",
      "         [0.1938, 0.1957, 0.1987, 0.2044, 0.2074]],\n",
      "\n",
      "        [[0.1889, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1916, 0.2096, 0.1996, 0.1996, 0.1996],\n",
      "         [0.1931, 0.2049, 0.2050, 0.1985, 0.1985],\n",
      "         [0.1817, 0.2060, 0.2062, 0.2136, 0.1925],\n",
      "         [0.2225, 0.1733, 0.1729, 0.1613, 0.2700]],\n",
      "\n",
      "        [[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1986, 0.2044, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1998, 0.2125, 0.1863, 0.2008, 0.2008],\n",
      "         [0.1994, 0.1814, 0.2220, 0.1993, 0.1979],\n",
      "         [0.1996, 0.2104, 0.1881, 0.1997, 0.2022]],\n",
      "\n",
      "        [[0.2205, 0.1949, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1821, 0.2079, 0.2033, 0.2033, 0.2033],\n",
      "         [0.1942, 0.2028, 0.2002, 0.2014, 0.2014],\n",
      "         [0.1704, 0.2014, 0.1917, 0.2407, 0.1958],\n",
      "         [0.1584, 0.1846, 0.1764, 0.2173, 0.2633]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1453, 0.2137, 0.2137, 0.2137, 0.2137],\n",
      "         [0.1280, 0.2815, 0.1968, 0.1968, 0.1968],\n",
      "         [0.1674, 0.2322, 0.2002, 0.2001, 0.2001],\n",
      "         [0.1000, 0.2531, 0.1661, 0.3148, 0.1660],\n",
      "         [0.3244, 0.1180, 0.1867, 0.0930, 0.2779]],\n",
      "\n",
      "        [[0.1621, 0.2095, 0.2095, 0.2095, 0.2095],\n",
      "         [0.1837, 0.2188, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2242, 0.1624, 0.2271, 0.1932, 0.1932],\n",
      "         [0.1692, 0.2407, 0.1668, 0.2242, 0.1991],\n",
      "         [0.1945, 0.2033, 0.1942, 0.2015, 0.2064]],\n",
      "\n",
      "        [[0.2040, 0.1990, 0.1990, 0.1990, 0.1990],\n",
      "         [0.2100, 0.1977, 0.1974, 0.1974, 0.1974],\n",
      "         [0.1783, 0.2192, 0.1620, 0.2202, 0.2202],\n",
      "         [0.1954, 0.2527, 0.1734, 0.1245, 0.2541],\n",
      "         [0.2091, 0.2339, 0.1985, 0.1717, 0.1868]],\n",
      "\n",
      "        [[0.1241, 0.2190, 0.2190, 0.2190, 0.2190],\n",
      "         [0.1697, 0.1953, 0.2117, 0.2117, 0.2117],\n",
      "         [0.1451, 0.1894, 0.2237, 0.2209, 0.2209],\n",
      "         [0.1230, 0.2057, 0.2836, 0.1111, 0.2766],\n",
      "         [0.1684, 0.2170, 0.2542, 0.1602, 0.2002]],\n",
      "\n",
      "        [[0.2310, 0.1922, 0.1922, 0.1922, 0.1922],\n",
      "         [0.2137, 0.2265, 0.1866, 0.1866, 0.1866],\n",
      "         [0.2231, 0.2396, 0.1588, 0.1893, 0.1893],\n",
      "         [0.2063, 0.2160, 0.1653, 0.2271, 0.1853],\n",
      "         [0.1967, 0.1913, 0.2244, 0.1857, 0.2019]],\n",
      "\n",
      "        [[0.1957, 0.2011, 0.2011, 0.2011, 0.2011],\n",
      "         [0.1759, 0.1910, 0.2110, 0.2110, 0.2110],\n",
      "         [0.1893, 0.1974, 0.1978, 0.2077, 0.2077],\n",
      "         [0.1756, 0.1853, 0.1857, 0.2555, 0.1979],\n",
      "         [0.2229, 0.2113, 0.2108, 0.1536, 0.2015]],\n",
      "\n",
      "        [[0.1393, 0.2152, 0.2152, 0.2152, 0.2152],\n",
      "         [0.2424, 0.2004, 0.1857, 0.1857, 0.1857],\n",
      "         [0.2008, 0.1998, 0.2004, 0.1994, 0.1994],\n",
      "         [0.1431, 0.2361, 0.1774, 0.1550, 0.2884],\n",
      "         [0.1540, 0.2326, 0.1839, 0.1645, 0.2650]],\n",
      "\n",
      "        [[0.1945, 0.2014, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2067, 0.1891, 0.2014, 0.2014, 0.2014],\n",
      "         [0.2589, 0.1366, 0.1741, 0.2152, 0.2152],\n",
      "         [0.2997, 0.1655, 0.2072, 0.0752, 0.2524],\n",
      "         [0.3218, 0.1999, 0.2393, 0.1061, 0.1329]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1640, 0.2090, 0.2090, 0.2090, 0.2090],\n",
      "         [0.1850, 0.2088, 0.2021, 0.2021, 0.2021],\n",
      "         [0.1717, 0.2098, 0.2211, 0.1987, 0.1987],\n",
      "         [0.2704, 0.1720, 0.1529, 0.2101, 0.1946],\n",
      "         [0.1403, 0.2198, 0.2471, 0.1802, 0.2126]],\n",
      "\n",
      "        [[0.1172, 0.2207, 0.2207, 0.2207, 0.2207],\n",
      "         [0.2534, 0.1792, 0.1891, 0.1891, 0.1891],\n",
      "         [0.1991, 0.2002, 0.2005, 0.2001, 0.2001],\n",
      "         [0.2036, 0.1988, 0.1975, 0.2006, 0.1995],\n",
      "         [0.1587, 0.2070, 0.2225, 0.1867, 0.2250]],\n",
      "\n",
      "        [[0.2143, 0.1964, 0.1964, 0.1964, 0.1964],\n",
      "         [0.1483, 0.2332, 0.2061, 0.2061, 0.2061],\n",
      "         [0.2348, 0.1703, 0.2232, 0.1859, 0.1859],\n",
      "         [0.1562, 0.2648, 0.1698, 0.1799, 0.2293],\n",
      "         [0.2198, 0.1612, 0.2093, 0.2023, 0.2074]],\n",
      "\n",
      "        [[0.1705, 0.2074, 0.2074, 0.2074, 0.2074],\n",
      "         [0.1335, 0.1678, 0.2329, 0.2329, 0.2329],\n",
      "         [0.1777, 0.1951, 0.1815, 0.2229, 0.2229],\n",
      "         [0.1875, 0.2178, 0.1939, 0.1307, 0.2701],\n",
      "         [0.2061, 0.2178, 0.2087, 0.1805, 0.1869]],\n",
      "\n",
      "        [[0.2005, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2074, 0.1845, 0.2027, 0.2027, 0.2027],\n",
      "         [0.2076, 0.2623, 0.0952, 0.2174, 0.2174],\n",
      "         [0.2051, 0.2597, 0.0933, 0.2271, 0.2149],\n",
      "         [0.1974, 0.1835, 0.2520, 0.1912, 0.1759]],\n",
      "\n",
      "        [[0.2189, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.1996, 0.1977, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2005, 0.2043, 0.1990, 0.1982, 0.1982],\n",
      "         [0.2088, 0.2606, 0.1910, 0.1578, 0.1819],\n",
      "         [0.1705, 0.1253, 0.1930, 0.2518, 0.2594]],\n",
      "\n",
      "        [[0.1731, 0.2067, 0.2067, 0.2067, 0.2067],\n",
      "         [0.2326, 0.1728, 0.1982, 0.1982, 0.1982],\n",
      "         [0.2510, 0.1377, 0.2483, 0.1815, 0.1815],\n",
      "         [0.1720, 0.2802, 0.1736, 0.1504, 0.2238],\n",
      "         [0.2042, 0.1852, 0.2038, 0.2097, 0.1970]],\n",
      "\n",
      "        [[0.1989, 0.2003, 0.2003, 0.2003, 0.2003],\n",
      "         [0.2087, 0.1602, 0.2104, 0.2104, 0.2104],\n",
      "         [0.2169, 0.1857, 0.1617, 0.2179, 0.2179],\n",
      "         [0.2895, 0.1846, 0.1237, 0.1088, 0.2935],\n",
      "         [0.2633, 0.2113, 0.1737, 0.1632, 0.1885]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2581, 0.1855, 0.1855, 0.1855, 0.1855],\n",
      "         [0.1683, 0.1861, 0.2152, 0.2152, 0.2152],\n",
      "         [0.2628, 0.2087, 0.2291, 0.1497, 0.1497],\n",
      "         [0.1902, 0.1959, 0.1936, 0.2159, 0.2045],\n",
      "         [0.2233, 0.1718, 0.1910, 0.0729, 0.3409]],\n",
      "\n",
      "        [[0.1829, 0.2043, 0.2043, 0.2043, 0.2043],\n",
      "         [0.1908, 0.3161, 0.1644, 0.1644, 0.1644],\n",
      "         [0.1906, 0.2677, 0.1971, 0.1723, 0.1723],\n",
      "         [0.2000, 0.1998, 0.2000, 0.2001, 0.2001],\n",
      "         [0.1395, 0.2661, 0.1487, 0.1083, 0.3375]],\n",
      "\n",
      "        [[0.3312, 0.1672, 0.1672, 0.1672, 0.1672],\n",
      "         [0.3033, 0.1940, 0.1676, 0.1676, 0.1676],\n",
      "         [0.3675, 0.1818, 0.1618, 0.1444, 0.1444],\n",
      "         [0.3803, 0.1765, 0.1554, 0.1504, 0.1374],\n",
      "         [0.3417, 0.1657, 0.1469, 0.1425, 0.2032]],\n",
      "\n",
      "        [[0.2561, 0.1860, 0.1860, 0.1860, 0.1860],\n",
      "         [0.3381, 0.2325, 0.1431, 0.1431, 0.1431],\n",
      "         [0.3073, 0.1575, 0.4027, 0.0663, 0.0663],\n",
      "         [0.2802, 0.1765, 0.3378, 0.1085, 0.0970],\n",
      "         [0.2241, 0.1423, 0.2694, 0.0881, 0.2760]],\n",
      "\n",
      "        [[0.2606, 0.1849, 0.1849, 0.1849, 0.1849],\n",
      "         [0.2942, 0.2548, 0.1503, 0.1503, 0.1503],\n",
      "         [0.1785, 0.1836, 0.2309, 0.2035, 0.2035],\n",
      "         [0.2247, 0.2179, 0.1699, 0.1926, 0.1949],\n",
      "         [0.2334, 0.2017, 0.0615, 0.1120, 0.3914]],\n",
      "\n",
      "        [[0.2485, 0.1879, 0.1879, 0.1879, 0.1879],\n",
      "         [0.1395, 0.1953, 0.2217, 0.2217, 0.2217],\n",
      "         [0.2208, 0.1959, 0.2089, 0.1872, 0.1872],\n",
      "         [0.2598, 0.1833, 0.2210, 0.1753, 0.1606],\n",
      "         [0.2560, 0.1279, 0.1856, 0.1171, 0.3135]],\n",
      "\n",
      "        [[0.1137, 0.2216, 0.2216, 0.2216, 0.2216],\n",
      "         [0.0999, 0.3485, 0.1838, 0.1838, 0.1838],\n",
      "         [0.2542, 0.1423, 0.2203, 0.1916, 0.1916],\n",
      "         [0.2900, 0.1154, 0.2310, 0.1787, 0.1850],\n",
      "         [0.2405, 0.1434, 0.2117, 0.1832, 0.2212]],\n",
      "\n",
      "        [[0.2942, 0.1764, 0.1764, 0.1764, 0.1764],\n",
      "         [0.2653, 0.3150, 0.1399, 0.1399, 0.1399],\n",
      "         [0.2322, 0.2770, 0.2499, 0.1205, 0.1205],\n",
      "         [0.2201, 0.2738, 0.2410, 0.1673, 0.0977],\n",
      "         [0.2031, 0.2337, 0.2153, 0.1702, 0.1778]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2220, 0.1945, 0.1945, 0.1945, 0.1945],\n",
      "         [0.2064, 0.2075, 0.1954, 0.1954, 0.1954],\n",
      "         [0.2565, 0.2695, 0.1552, 0.1594, 0.1594],\n",
      "         [0.1388, 0.1329, 0.2168, 0.2999, 0.2116],\n",
      "         [0.1503, 0.1439, 0.2333, 0.3215, 0.1510]],\n",
      "\n",
      "        [[0.1863, 0.2034, 0.2034, 0.2034, 0.2034],\n",
      "         [0.2146, 0.2175, 0.1893, 0.1893, 0.1893],\n",
      "         [0.2073, 0.2085, 0.1916, 0.1963, 0.1963],\n",
      "         [0.2087, 0.2100, 0.1933, 0.1901, 0.1979],\n",
      "         [0.2220, 0.2266, 0.1697, 0.1601, 0.2216]],\n",
      "\n",
      "        [[0.2005, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1948, 0.2176, 0.1959, 0.1959, 0.1959],\n",
      "         [0.1900, 0.2060, 0.2224, 0.1908, 0.1908],\n",
      "         [0.1948, 0.2018, 0.2087, 0.1996, 0.1951],\n",
      "         [0.1954, 0.2017, 0.2078, 0.1997, 0.1954]],\n",
      "\n",
      "        [[0.1723, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.2330, 0.2025, 0.1882, 0.1882, 0.1882],\n",
      "         [0.1696, 0.2027, 0.1827, 0.2225, 0.2225],\n",
      "         [0.2398, 0.1990, 0.2219, 0.1588, 0.1805],\n",
      "         [0.2300, 0.1922, 0.2134, 0.1546, 0.2097]],\n",
      "\n",
      "        [[0.1914, 0.2022, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1984, 0.2003, 0.2004, 0.2004, 0.2004],\n",
      "         [0.2069, 0.1954, 0.2079, 0.1949, 0.1949],\n",
      "         [0.2007, 0.1957, 0.2011, 0.2069, 0.1955],\n",
      "         [0.1990, 0.1965, 0.1993, 0.2021, 0.2031]],\n",
      "\n",
      "        [[0.2024, 0.1994, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1997, 0.2156, 0.1949, 0.1949, 0.1949],\n",
      "         [0.1981, 0.2304, 0.1938, 0.1888, 0.1888],\n",
      "         [0.2000, 0.1997, 0.2001, 0.2000, 0.2001],\n",
      "         [0.1875, 0.2315, 0.1818, 0.1913, 0.2078]],\n",
      "\n",
      "        [[0.1800, 0.2050, 0.2050, 0.2050, 0.2050],\n",
      "         [0.2484, 0.1568, 0.1983, 0.1983, 0.1983],\n",
      "         [0.1983, 0.2027, 0.1982, 0.2004, 0.2004],\n",
      "         [0.2206, 0.1866, 0.2215, 0.1680, 0.2033],\n",
      "         [0.2570, 0.1414, 0.2608, 0.0971, 0.2437]],\n",
      "\n",
      "        [[0.2184, 0.1954, 0.1954, 0.1954, 0.1954],\n",
      "         [0.2118, 0.2090, 0.1931, 0.1931, 0.1931],\n",
      "         [0.1829, 0.1877, 0.1923, 0.2186, 0.2186],\n",
      "         [0.1978, 0.1985, 0.1991, 0.2023, 0.2023],\n",
      "         [0.2067, 0.2035, 0.2006, 0.1861, 0.2030]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2352, 0.1912, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2831, 0.2019, 0.1717, 0.1717, 0.1717],\n",
      "         [0.2144, 0.1982, 0.2058, 0.1908, 0.1908],\n",
      "         [0.2427, 0.1831, 0.2097, 0.2045, 0.1600],\n",
      "         [0.2233, 0.1938, 0.2075, 0.2049, 0.1705]],\n",
      "\n",
      "        [[0.2269, 0.1933, 0.1933, 0.1933, 0.1933],\n",
      "         [0.2399, 0.2732, 0.1623, 0.1623, 0.1623],\n",
      "         [0.1757, 0.1518, 0.1277, 0.2724, 0.2724],\n",
      "         [0.2008, 0.1996, 0.1981, 0.1970, 0.2045],\n",
      "         [0.1606, 0.1968, 0.2505, 0.3007, 0.0914]],\n",
      "\n",
      "        [[0.2570, 0.1857, 0.1857, 0.1857, 0.1857],\n",
      "         [0.1985, 0.1845, 0.2057, 0.2057, 0.2057],\n",
      "         [0.1993, 0.1917, 0.2028, 0.2031, 0.2031],\n",
      "         [0.2071, 0.2853, 0.1797, 0.1509, 0.1770],\n",
      "         [0.2129, 0.3788, 0.1649, 0.1204, 0.1230]],\n",
      "\n",
      "        [[0.1792, 0.2052, 0.2052, 0.2052, 0.2052],\n",
      "         [0.2047, 0.1886, 0.2022, 0.2022, 0.2022],\n",
      "         [0.1421, 0.3171, 0.2215, 0.1597, 0.1597],\n",
      "         [0.2391, 0.1500, 0.1847, 0.2028, 0.2234],\n",
      "         [0.1345, 0.2977, 0.2087, 0.1781, 0.1809]],\n",
      "\n",
      "        [[0.3422, 0.1645, 0.1645, 0.1645, 0.1645],\n",
      "         [0.2710, 0.2922, 0.1456, 0.1456, 0.1456],\n",
      "         [0.1612, 0.1527, 0.1798, 0.2532, 0.2532],\n",
      "         [0.2292, 0.2409, 0.2076, 0.1701, 0.1521],\n",
      "         [0.2237, 0.2392, 0.1956, 0.1494, 0.1920]],\n",
      "\n",
      "        [[0.2305, 0.1924, 0.1924, 0.1924, 0.1924],\n",
      "         [0.1818, 0.1863, 0.2106, 0.2106, 0.2106],\n",
      "         [0.2009, 0.2006, 0.2009, 0.1988, 0.1988],\n",
      "         [0.2172, 0.2032, 0.2163, 0.2183, 0.1449],\n",
      "         [0.2441, 0.2107, 0.2418, 0.2470, 0.0564]],\n",
      "\n",
      "        [[0.1846, 0.2038, 0.2038, 0.2038, 0.2038],\n",
      "         [0.2502, 0.1831, 0.1889, 0.1889, 0.1889],\n",
      "         [0.1756, 0.2125, 0.1949, 0.2085, 0.2085],\n",
      "         [0.2106, 0.1947, 0.2018, 0.1967, 0.1962],\n",
      "         [0.2124, 0.1924, 0.2012, 0.1949, 0.1992]],\n",
      "\n",
      "        [[0.2350, 0.1912, 0.1912, 0.1912, 0.1912],\n",
      "         [0.2899, 0.2310, 0.1597, 0.1597, 0.1597],\n",
      "         [0.1419, 0.1714, 0.2205, 0.2331, 0.2331],\n",
      "         [0.2359, 0.2143, 0.1885, 0.1780, 0.1832],\n",
      "         [0.2081, 0.2033, 0.1970, 0.1943, 0.1973]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1446, 0.2139, 0.2139, 0.2139, 0.2139],\n",
      "         [0.1699, 0.1869, 0.2144, 0.2144, 0.2144],\n",
      "         [0.2001, 0.2001, 0.1999, 0.2000, 0.2000],\n",
      "         [0.1965, 0.1993, 0.2077, 0.1931, 0.2033],\n",
      "         [0.1645, 0.1855, 0.2657, 0.1412, 0.2431]],\n",
      "\n",
      "        [[0.1941, 0.2015, 0.2015, 0.2015, 0.2015],\n",
      "         [0.1580, 0.1360, 0.2354, 0.2354, 0.2354],\n",
      "         [0.1759, 0.1631, 0.2306, 0.2152, 0.2152],\n",
      "         [0.1899, 0.1829, 0.2171, 0.2002, 0.2098],\n",
      "         [0.1787, 0.1583, 0.2759, 0.2120, 0.1751]],\n",
      "\n",
      "        [[0.1962, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.2000, 0.2007, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1976, 0.1630, 0.2233, 0.2080, 0.2080],\n",
      "         [0.2003, 0.1958, 0.2032, 0.1992, 0.2015],\n",
      "         [0.1869, 0.2250, 0.1661, 0.1955, 0.2264]],\n",
      "\n",
      "        [[0.1765, 0.2059, 0.2059, 0.2059, 0.2059],\n",
      "         [0.1520, 0.1604, 0.2292, 0.2292, 0.2292],\n",
      "         [0.1309, 0.1431, 0.2054, 0.2603, 0.2603],\n",
      "         [0.2050, 0.2039, 0.1992, 0.1958, 0.1961],\n",
      "         [0.1736, 0.1790, 0.2022, 0.2211, 0.2242]],\n",
      "\n",
      "        [[0.1243, 0.2189, 0.2189, 0.2189, 0.2189],\n",
      "         [0.1449, 0.1352, 0.2400, 0.2400, 0.2400],\n",
      "         [0.1754, 0.1690, 0.1964, 0.2296, 0.2296],\n",
      "         [0.1637, 0.1525, 0.2035, 0.2057, 0.2747],\n",
      "         [0.1767, 0.1650, 0.2180, 0.2202, 0.2201]],\n",
      "\n",
      "        [[0.2177, 0.1956, 0.1956, 0.1956, 0.1956],\n",
      "         [0.1526, 0.1716, 0.2253, 0.2253, 0.2253],\n",
      "         [0.1727, 0.1847, 0.2114, 0.2156, 0.2156],\n",
      "         [0.1115, 0.1478, 0.2613, 0.1955, 0.2839],\n",
      "         [0.2410, 0.2126, 0.1651, 0.1878, 0.1935]],\n",
      "\n",
      "        [[0.2189, 0.1953, 0.1953, 0.1953, 0.1953],\n",
      "         [0.2183, 0.2370, 0.1815, 0.1815, 0.1815],\n",
      "         [0.1889, 0.1835, 0.2240, 0.2018, 0.2018],\n",
      "         [0.2247, 0.2374, 0.1631, 0.1762, 0.1985],\n",
      "         [0.2297, 0.2418, 0.1704, 0.1831, 0.1751]],\n",
      "\n",
      "        [[0.1237, 0.2191, 0.2191, 0.2191, 0.2191],\n",
      "         [0.1365, 0.1760, 0.2292, 0.2292, 0.2292],\n",
      "         [0.1450, 0.1746, 0.2570, 0.2117, 0.2117],\n",
      "         [0.2052, 0.2017, 0.1944, 0.2006, 0.1980],\n",
      "         [0.1735, 0.1946, 0.2472, 0.2014, 0.1833]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2202, 0.1950, 0.1950, 0.1950, 0.1950],\n",
      "         [0.2295, 0.1187, 0.2173, 0.2173, 0.2173],\n",
      "         [0.2564, 0.0939, 0.1780, 0.2358, 0.2358],\n",
      "         [0.3404, 0.0733, 0.1949, 0.0918, 0.2996],\n",
      "         [0.1706, 0.2446, 0.1945, 0.2321, 0.1581]],\n",
      "\n",
      "        [[0.1513, 0.2122, 0.2122, 0.2122, 0.2122],\n",
      "         [0.1638, 0.1409, 0.2318, 0.2318, 0.2318],\n",
      "         [0.1634, 0.1476, 0.2759, 0.2066, 0.2066],\n",
      "         [0.1610, 0.1376, 0.3605, 0.1100, 0.2310],\n",
      "         [0.1989, 0.1883, 0.2634, 0.1741, 0.1754]],\n",
      "\n",
      "        [[0.1350, 0.2162, 0.2162, 0.2162, 0.2162],\n",
      "         [0.0828, 0.1490, 0.2561, 0.2561, 0.2561],\n",
      "         [0.2083, 0.2013, 0.2001, 0.1951, 0.1951],\n",
      "         [0.2348, 0.1840, 0.1763, 0.2580, 0.1470],\n",
      "         [0.1919, 0.2095, 0.2127, 0.1855, 0.2003]],\n",
      "\n",
      "        [[0.4045, 0.1489, 0.1489, 0.1489, 0.1489],\n",
      "         [0.0606, 0.1183, 0.2737, 0.2737, 0.2737],\n",
      "         [0.1318, 0.1720, 0.2159, 0.2401, 0.2401],\n",
      "         [0.1486, 0.1852, 0.2235, 0.1987, 0.2440],\n",
      "         [0.2040, 0.1998, 0.1963, 0.1985, 0.2014]],\n",
      "\n",
      "        [[0.0904, 0.2274, 0.2274, 0.2274, 0.2274],\n",
      "         [0.0690, 0.1362, 0.2649, 0.2649, 0.2649],\n",
      "         [0.0782, 0.1295, 0.3682, 0.2121, 0.2121],\n",
      "         [0.1301, 0.1733, 0.3134, 0.1540, 0.2292],\n",
      "         [0.1719, 0.1986, 0.2677, 0.1871, 0.1747]],\n",
      "\n",
      "        [[0.1117, 0.2221, 0.2221, 0.2221, 0.2221],\n",
      "         [0.1290, 0.1609, 0.2367, 0.2367, 0.2367],\n",
      "         [0.1587, 0.1890, 0.1393, 0.2565, 0.2565],\n",
      "         [0.1723, 0.2237, 0.1418, 0.1090, 0.3531],\n",
      "         [0.2003, 0.1897, 0.2086, 0.2203, 0.1811]],\n",
      "\n",
      "        [[0.5643, 0.1089, 0.1089, 0.1089, 0.1089],\n",
      "         [0.1054, 0.1947, 0.2333, 0.2333, 0.2333],\n",
      "         [0.1985, 0.2002, 0.2000, 0.2007, 0.2007],\n",
      "         [0.1186, 0.2176, 0.2047, 0.1990, 0.2601],\n",
      "         [0.2793, 0.1694, 0.1782, 0.1824, 0.1907]],\n",
      "\n",
      "        [[0.1245, 0.2189, 0.2189, 0.2189, 0.2189],\n",
      "         [0.1882, 0.1339, 0.2260, 0.2260, 0.2260],\n",
      "         [0.1990, 0.2650, 0.1948, 0.1706, 0.1706],\n",
      "         [0.1847, 0.2329, 0.1816, 0.2376, 0.1631],\n",
      "         [0.1856, 0.2107, 0.1838, 0.2131, 0.2068]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2292, 0.1927, 0.1927, 0.1927, 0.1927],\n",
      "         [0.2385, 0.2150, 0.1822, 0.1822, 0.1822],\n",
      "         [0.2934, 0.2207, 0.2058, 0.1401, 0.1401],\n",
      "         [0.1040, 0.1472, 0.1603, 0.3320, 0.2564],\n",
      "         [0.1920, 0.1960, 0.1969, 0.2057, 0.2095]],\n",
      "\n",
      "        [[0.2047, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.2109, 0.2448, 0.1814, 0.1814, 0.1814],\n",
      "         [0.2044, 0.2124, 0.1899, 0.1966, 0.1966],\n",
      "         [0.1858, 0.1659, 0.2302, 0.2100, 0.2081],\n",
      "         [0.2006, 0.2042, 0.1940, 0.1968, 0.2044]],\n",
      "\n",
      "        [[0.2177, 0.1956, 0.1956, 0.1956, 0.1956],\n",
      "         [0.2148, 0.1740, 0.2038, 0.2038, 0.2038],\n",
      "         [0.1798, 0.2332, 0.2034, 0.1918, 0.1918],\n",
      "         [0.3254, 0.0994, 0.1852, 0.1478, 0.2421],\n",
      "         [0.2850, 0.1481, 0.2088, 0.1844, 0.1737]],\n",
      "\n",
      "        [[0.1710, 0.2072, 0.2072, 0.2072, 0.2072],\n",
      "         [0.1977, 0.1916, 0.2036, 0.2036, 0.2036],\n",
      "         [0.1936, 0.1847, 0.2171, 0.2023, 0.2023],\n",
      "         [0.1991, 0.1973, 0.2034, 0.1995, 0.2007],\n",
      "         [0.2015, 0.2114, 0.1792, 0.1992, 0.2086]],\n",
      "\n",
      "        [[0.1872, 0.2032, 0.2032, 0.2032, 0.2032],\n",
      "         [0.1986, 0.1934, 0.2027, 0.2027, 0.2027],\n",
      "         [0.2019, 0.1941, 0.1876, 0.2082, 0.2082],\n",
      "         [0.2070, 0.1992, 0.1928, 0.1877, 0.2133],\n",
      "         [0.2132, 0.2027, 0.1941, 0.1873, 0.2027]],\n",
      "\n",
      "        [[0.1633, 0.2092, 0.2092, 0.2092, 0.2092],\n",
      "         [0.2004, 0.2003, 0.1998, 0.1998, 0.1998],\n",
      "         [0.1901, 0.1929, 0.2021, 0.2074, 0.2074],\n",
      "         [0.1912, 0.1939, 0.2031, 0.2035, 0.2082],\n",
      "         [0.2018, 0.2005, 0.1960, 0.1958, 0.2059]],\n",
      "\n",
      "        [[0.2104, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.2107, 0.1693, 0.2067, 0.2067, 0.2067],\n",
      "         [0.1957, 0.2165, 0.1928, 0.1975, 0.1975],\n",
      "         [0.2015, 0.1953, 0.2024, 0.2000, 0.2009],\n",
      "         [0.1837, 0.2141, 0.1796, 0.1906, 0.2320]],\n",
      "\n",
      "        [[0.2259, 0.1935, 0.1935, 0.1935, 0.1935],\n",
      "         [0.1833, 0.1947, 0.2073, 0.2073, 0.2073],\n",
      "         [0.1635, 0.1802, 0.2579, 0.1992, 0.1992],\n",
      "         [0.1346, 0.1627, 0.3273, 0.1776, 0.1978],\n",
      "         [0.1472, 0.1744, 0.3262, 0.1887, 0.1636]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0704, 0.2324, 0.2324, 0.2324, 0.2324],\n",
      "         [0.1183, 0.1631, 0.2395, 0.2395, 0.2395],\n",
      "         [0.2018, 0.2001, 0.2020, 0.1980, 0.1980],\n",
      "         [0.2483, 0.1836, 0.2565, 0.1836, 0.1281],\n",
      "         [0.1420, 0.1957, 0.1372, 0.1957, 0.3294]],\n",
      "\n",
      "        [[0.1185, 0.2204, 0.2204, 0.2204, 0.2204],\n",
      "         [0.1636, 0.1730, 0.2211, 0.2211, 0.2211],\n",
      "         [0.1915, 0.1943, 0.2003, 0.2070, 0.2070],\n",
      "         [0.1983, 0.1996, 0.2024, 0.1942, 0.2055],\n",
      "         [0.1865, 0.1979, 0.2241, 0.1546, 0.2369]],\n",
      "\n",
      "        [[0.1684, 0.2079, 0.2079, 0.2079, 0.2079],\n",
      "         [0.1805, 0.1385, 0.2270, 0.2270, 0.2270],\n",
      "         [0.1961, 0.1849, 0.2065, 0.2063, 0.2063],\n",
      "         [0.2069, 0.2299, 0.1886, 0.1857, 0.1889],\n",
      "         [0.2075, 0.2286, 0.1905, 0.1879, 0.1854]],\n",
      "\n",
      "        [[0.2261, 0.1935, 0.1935, 0.1935, 0.1935],\n",
      "         [0.2373, 0.1253, 0.2125, 0.2125, 0.2125],\n",
      "         [0.2572, 0.1138, 0.1823, 0.2234, 0.2234],\n",
      "         [0.1599, 0.2951, 0.2071, 0.1602, 0.1777],\n",
      "         [0.2562, 0.1284, 0.1914, 0.2557, 0.1683]],\n",
      "\n",
      "        [[0.1205, 0.2199, 0.2199, 0.2199, 0.2199],\n",
      "         [0.1906, 0.1934, 0.2053, 0.2053, 0.2053],\n",
      "         [0.1703, 0.1807, 0.1896, 0.2297, 0.2297],\n",
      "         [0.1531, 0.1689, 0.1828, 0.2442, 0.2510],\n",
      "         [0.1471, 0.1716, 0.1941, 0.3052, 0.1820]],\n",
      "\n",
      "        [[0.1020, 0.2245, 0.2245, 0.2245, 0.2245],\n",
      "         [0.0887, 0.0985, 0.2709, 0.2709, 0.2709],\n",
      "         [0.1360, 0.1466, 0.1114, 0.3030, 0.3030],\n",
      "         [0.1764, 0.1847, 0.1562, 0.1951, 0.2875],\n",
      "         [0.1843, 0.1896, 0.1710, 0.1962, 0.2590]],\n",
      "\n",
      "        [[0.1901, 0.2025, 0.2025, 0.2025, 0.2025],\n",
      "         [0.1917, 0.2762, 0.1773, 0.1773, 0.1773],\n",
      "         [0.2016, 0.1835, 0.2035, 0.2057, 0.2057],\n",
      "         [0.1752, 0.3646, 0.1627, 0.1476, 0.1498],\n",
      "         [0.2074, 0.1779, 0.2106, 0.2150, 0.1891]],\n",
      "\n",
      "        [[0.0864, 0.2284, 0.2284, 0.2284, 0.2284],\n",
      "         [0.1047, 0.1269, 0.2561, 0.2561, 0.2561],\n",
      "         [0.1369, 0.1524, 0.2600, 0.2254, 0.2254],\n",
      "         [0.2556, 0.2373, 0.1637, 0.1625, 0.1808],\n",
      "         [0.1261, 0.1410, 0.2457, 0.2484, 0.2389]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1934, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.1849, 0.2461, 0.1897, 0.1897, 0.1897],\n",
      "         [0.1562, 0.2839, 0.2302, 0.1648, 0.1648],\n",
      "         [0.2389, 0.1585, 0.1831, 0.1893, 0.2302],\n",
      "         [0.1599, 0.2460, 0.2114, 0.2041, 0.1786]],\n",
      "\n",
      "        [[0.2163, 0.1959, 0.1959, 0.1959, 0.1959],\n",
      "         [0.2015, 0.1757, 0.2076, 0.2076, 0.2076],\n",
      "         [0.1826, 0.3528, 0.1479, 0.1584, 0.1584],\n",
      "         [0.1925, 0.2303, 0.1817, 0.2104, 0.1851],\n",
      "         [0.1955, 0.2058, 0.1923, 0.2005, 0.2058]],\n",
      "\n",
      "        [[0.1917, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2599, 0.2247, 0.1718, 0.1718, 0.1718],\n",
      "         [0.2506, 0.2061, 0.2557, 0.1438, 0.1438],\n",
      "         [0.2083, 0.2006, 0.2091, 0.1947, 0.1873],\n",
      "         [0.2105, 0.1893, 0.2129, 0.1738, 0.2135]],\n",
      "\n",
      "        [[0.2347, 0.1913, 0.1913, 0.1913, 0.1913],\n",
      "         [0.2116, 0.2065, 0.1940, 0.1940, 0.1940],\n",
      "         [0.1795, 0.1919, 0.1733, 0.2276, 0.2276],\n",
      "         [0.2225, 0.1992, 0.2358, 0.1925, 0.1500],\n",
      "         [0.2008, 0.1907, 0.2063, 0.1877, 0.2145]],\n",
      "\n",
      "        [[0.1598, 0.2101, 0.2101, 0.2101, 0.2101],\n",
      "         [0.1709, 0.1547, 0.2248, 0.2248, 0.2248],\n",
      "         [0.2757, 0.3765, 0.1143, 0.1167, 0.1167],\n",
      "         [0.2407, 0.2876, 0.1455, 0.1790, 0.1472],\n",
      "         [0.1976, 0.1943, 0.2071, 0.2031, 0.1979]],\n",
      "\n",
      "        [[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1954, 0.2516, 0.1843, 0.1843, 0.1843],\n",
      "         [0.1926, 0.2348, 0.2045, 0.1840, 0.1840],\n",
      "         [0.2136, 0.1847, 0.2044, 0.1764, 0.2209],\n",
      "         [0.1642, 0.2156, 0.1784, 0.2349, 0.2069]],\n",
      "\n",
      "        [[0.2371, 0.1907, 0.1907, 0.1907, 0.1907],\n",
      "         [0.2475, 0.2075, 0.1817, 0.1817, 0.1817],\n",
      "         [0.1980, 0.1879, 0.2529, 0.1806, 0.1806],\n",
      "         [0.1903, 0.1727, 0.2987, 0.1777, 0.1606],\n",
      "         [0.1806, 0.1513, 0.4113, 0.1594, 0.0975]],\n",
      "\n",
      "        [[0.2074, 0.1981, 0.1981, 0.1981, 0.1981],\n",
      "         [0.1955, 0.1924, 0.2040, 0.2040, 0.2040],\n",
      "         [0.2264, 0.2526, 0.1850, 0.1680, 0.1680],\n",
      "         [0.2096, 0.2205, 0.1910, 0.1962, 0.1827],\n",
      "         [0.2091, 0.2312, 0.1738, 0.1835, 0.2024]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1489, 0.2128, 0.2128, 0.2128, 0.2128],\n",
      "         [0.1078, 0.3536, 0.1795, 0.1795, 0.1795],\n",
      "         [0.1401, 0.2805, 0.2017, 0.1888, 0.1888],\n",
      "         [0.1410, 0.2523, 0.1914, 0.2342, 0.1810],\n",
      "         [0.2260, 0.1713, 0.1954, 0.1775, 0.2298]],\n",
      "\n",
      "        [[0.1481, 0.2130, 0.2130, 0.2130, 0.2130],\n",
      "         [0.1768, 0.1545, 0.2229, 0.2229, 0.2229],\n",
      "         [0.1896, 0.1847, 0.2288, 0.1984, 0.1984],\n",
      "         [0.1188, 0.1041, 0.3043, 0.3236, 0.1490],\n",
      "         [0.1403, 0.1286, 0.2605, 0.2712, 0.1994]],\n",
      "\n",
      "        [[0.1996, 0.2001, 0.2001, 0.2001, 0.2001],\n",
      "         [0.1892, 0.2595, 0.1838, 0.1838, 0.1838],\n",
      "         [0.1690, 0.2118, 0.2884, 0.1655, 0.1655],\n",
      "         [0.2374, 0.1946, 0.1482, 0.1780, 0.2418],\n",
      "         [0.1911, 0.1997, 0.2121, 0.2036, 0.1935]],\n",
      "\n",
      "        [[0.1624, 0.2094, 0.2094, 0.2094, 0.2094],\n",
      "         [0.2465, 0.2349, 0.1729, 0.1729, 0.1729],\n",
      "         [0.1853, 0.1890, 0.1964, 0.2147, 0.2147],\n",
      "         [0.1877, 0.1921, 0.2007, 0.1971, 0.2224],\n",
      "         [0.2033, 0.2010, 0.1968, 0.1986, 0.2003]],\n",
      "\n",
      "        [[0.2875, 0.1781, 0.1781, 0.1781, 0.1781],\n",
      "         [0.3044, 0.1934, 0.1674, 0.1674, 0.1674],\n",
      "         [0.2707, 0.1321, 0.3871, 0.1051, 0.1051],\n",
      "         [0.1985, 0.1100, 0.2665, 0.3338, 0.0911],\n",
      "         [0.1985, 0.1842, 0.2061, 0.2120, 0.1991]],\n",
      "\n",
      "        [[0.1790, 0.2052, 0.2052, 0.2052, 0.2052],\n",
      "         [0.0854, 0.3177, 0.1990, 0.1990, 0.1990],\n",
      "         [0.1394, 0.2165, 0.2740, 0.1851, 0.1851],\n",
      "         [0.0696, 0.1743, 0.2848, 0.3457, 0.1257],\n",
      "         [0.1960, 0.2007, 0.2033, 0.2043, 0.1958]],\n",
      "\n",
      "        [[0.1916, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2112, 0.2094, 0.1931, 0.1931, 0.1931],\n",
      "         [0.1880, 0.1857, 0.2958, 0.1652, 0.1652],\n",
      "         [0.1995, 0.2007, 0.1603, 0.2273, 0.2123],\n",
      "         [0.1990, 0.1997, 0.1758, 0.2144, 0.2111]],\n",
      "\n",
      "        [[0.2252, 0.1937, 0.1937, 0.1937, 0.1937],\n",
      "         [0.1943, 0.2440, 0.1872, 0.1872, 0.1872],\n",
      "         [0.2282, 0.1237, 0.1432, 0.2524, 0.2524],\n",
      "         [0.2404, 0.1329, 0.1531, 0.2086, 0.2650],\n",
      "         [0.2165, 0.1903, 0.1963, 0.2099, 0.1870]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1925, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.1867, 0.2157, 0.1992, 0.1992, 0.1992],\n",
      "         [0.1620, 0.2340, 0.2221, 0.1910, 0.1910],\n",
      "         [0.2857, 0.1590, 0.1728, 0.1627, 0.2198],\n",
      "         [0.1345, 0.2437, 0.2241, 0.2382, 0.1594]],\n",
      "\n",
      "        [[0.2008, 0.1998, 0.1998, 0.1998, 0.1998],\n",
      "         [0.2025, 0.1848, 0.2042, 0.2042, 0.2042],\n",
      "         [0.2085, 0.2299, 0.1483, 0.2066, 0.2066],\n",
      "         [0.2015, 0.2051, 0.1894, 0.2028, 0.2012],\n",
      "         [0.1974, 0.2038, 0.1764, 0.1996, 0.2228]],\n",
      "\n",
      "        [[0.3353, 0.1662, 0.1662, 0.1662, 0.1662],\n",
      "         [0.1567, 0.1857, 0.2192, 0.2192, 0.2192],\n",
      "         [0.3037, 0.2031, 0.2194, 0.1369, 0.1369],\n",
      "         [0.2091, 0.1951, 0.1977, 0.2157, 0.1823],\n",
      "         [0.2037, 0.1854, 0.1888, 0.2124, 0.2097]],\n",
      "\n",
      "        [[0.2761, 0.1810, 0.1810, 0.1810, 0.1810],\n",
      "         [0.1750, 0.2015, 0.2078, 0.2078, 0.2078],\n",
      "         [0.2688, 0.1933, 0.1781, 0.1799, 0.1799],\n",
      "         [0.2200, 0.1937, 0.1877, 0.2101, 0.1884],\n",
      "         [0.2606, 0.1571, 0.1385, 0.2168, 0.2270]],\n",
      "\n",
      "        [[0.2100, 0.1975, 0.1975, 0.1975, 0.1975],\n",
      "         [0.1820, 0.1924, 0.2086, 0.2086, 0.2086],\n",
      "         [0.2285, 0.2160, 0.1574, 0.1990, 0.1990],\n",
      "         [0.2512, 0.2184, 0.0995, 0.2527, 0.1782],\n",
      "         [0.2110, 0.2011, 0.1534, 0.2115, 0.2230]],\n",
      "\n",
      "        [[0.1663, 0.2084, 0.2084, 0.2084, 0.2084],\n",
      "         [0.1853, 0.2039, 0.2036, 0.2036, 0.2036],\n",
      "         [0.2419, 0.1819, 0.2110, 0.1826, 0.1826],\n",
      "         [0.2003, 0.1999, 0.2001, 0.2000, 0.1999],\n",
      "         [0.2091, 0.1799, 0.1945, 0.1871, 0.2294]],\n",
      "\n",
      "        [[0.3317, 0.1671, 0.1671, 0.1671, 0.1671],\n",
      "         [0.1353, 0.1715, 0.2311, 0.2311, 0.2311],\n",
      "         [0.4220, 0.2222, 0.1578, 0.0990, 0.0990],\n",
      "         [0.1864, 0.1984, 0.2051, 0.1954, 0.2147],\n",
      "         [0.2420, 0.1896, 0.1664, 0.2011, 0.2010]],\n",
      "\n",
      "        [[0.2097, 0.1976, 0.1976, 0.1976, 0.1976],\n",
      "         [0.1883, 0.2179, 0.1979, 0.1979, 0.1979],\n",
      "         [0.1551, 0.2600, 0.2149, 0.1850, 0.1850],\n",
      "         [0.2171, 0.1959, 0.2035, 0.1738, 0.2096],\n",
      "         [0.1647, 0.1978, 0.1849, 0.2447, 0.2079]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2453, 0.1887, 0.1887, 0.1887, 0.1887],\n",
      "         [0.2177, 0.1918, 0.1968, 0.1968, 0.1968],\n",
      "         [0.1904, 0.1997, 0.2143, 0.1978, 0.1978],\n",
      "         [0.2964, 0.1657, 0.0705, 0.2810, 0.1864],\n",
      "         [0.0865, 0.1650, 0.4258, 0.0918, 0.2309]],\n",
      "\n",
      "        [[0.2573, 0.1857, 0.1857, 0.1857, 0.1857],\n",
      "         [0.2014, 0.2005, 0.1994, 0.1994, 0.1994],\n",
      "         [0.2032, 0.1992, 0.2098, 0.1940, 0.1940],\n",
      "         [0.2176, 0.1968, 0.2560, 0.1575, 0.1720],\n",
      "         [0.1882, 0.2079, 0.1603, 0.2591, 0.1846]],\n",
      "\n",
      "        [[0.1551, 0.2112, 0.2112, 0.2112, 0.2112],\n",
      "         [0.2095, 0.2027, 0.1959, 0.1959, 0.1959],\n",
      "         [0.1626, 0.1845, 0.2320, 0.2105, 0.2105],\n",
      "         [0.1669, 0.1931, 0.2517, 0.1633, 0.2249],\n",
      "         [0.1808, 0.2135, 0.2887, 0.1764, 0.1406]],\n",
      "\n",
      "        [[0.0971, 0.2257, 0.2257, 0.2257, 0.2257],\n",
      "         [0.2056, 0.2064, 0.1960, 0.1960, 0.1960],\n",
      "         [0.1254, 0.1168, 0.1803, 0.2887, 0.2887],\n",
      "         [0.1749, 0.1695, 0.2054, 0.1974, 0.2529],\n",
      "         [0.1484, 0.1380, 0.2157, 0.1966, 0.3014]],\n",
      "\n",
      "        [[0.1959, 0.2010, 0.2010, 0.2010, 0.2010],\n",
      "         [0.2017, 0.2006, 0.1992, 0.1992, 0.1992],\n",
      "         [0.2990, 0.2259, 0.1576, 0.1588, 0.1588],\n",
      "         [0.2110, 0.1972, 0.1809, 0.2297, 0.1812],\n",
      "         [0.1677, 0.2151, 0.2961, 0.1224, 0.1987]],\n",
      "\n",
      "        [[0.2103, 0.1974, 0.1974, 0.1974, 0.1974],\n",
      "         [0.3296, 0.1570, 0.1711, 0.1711, 0.1711],\n",
      "         [0.2235, 0.1928, 0.1914, 0.1961, 0.1961],\n",
      "         [0.2732, 0.1667, 0.1628, 0.2207, 0.1765],\n",
      "         [0.0828, 0.2565, 0.2708, 0.1349, 0.2550]],\n",
      "\n",
      "        [[0.1933, 0.2017, 0.2017, 0.2017, 0.2017],\n",
      "         [0.2255, 0.1344, 0.2134, 0.2134, 0.2134],\n",
      "         [0.1684, 0.2554, 0.2241, 0.1761, 0.1761],\n",
      "         [0.1827, 0.2284, 0.2129, 0.1888, 0.1871],\n",
      "         [0.1674, 0.2452, 0.2175, 0.1771, 0.1929]],\n",
      "\n",
      "        [[0.2474, 0.1882, 0.1882, 0.1882, 0.1882],\n",
      "         [0.1571, 0.1698, 0.2243, 0.2243, 0.2243],\n",
      "         [0.1341, 0.1555, 0.1824, 0.2640, 0.2640],\n",
      "         [0.1566, 0.1772, 0.2025, 0.1877, 0.2760],\n",
      "         [0.1913, 0.2127, 0.2384, 0.2234, 0.1342]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1192, 0.2202, 0.2202, 0.2202, 0.2202],\n",
      "         [0.1700, 0.1194, 0.2369, 0.2369, 0.2369],\n",
      "         [0.1984, 0.1522, 0.1406, 0.2544, 0.2544],\n",
      "         [0.1975, 0.2113, 0.2156, 0.1902, 0.1854],\n",
      "         [0.2085, 0.1725, 0.1630, 0.2319, 0.2241]],\n",
      "\n",
      "        [[0.1626, 0.2094, 0.2094, 0.2094, 0.2094],\n",
      "         [0.1282, 0.1733, 0.2328, 0.2328, 0.2328],\n",
      "         [0.2033, 0.1819, 0.2886, 0.1631, 0.1631],\n",
      "         [0.2091, 0.2362, 0.1424, 0.1460, 0.2662],\n",
      "         [0.2570, 0.3680, 0.0830, 0.0893, 0.2027]],\n",
      "\n",
      "        [[0.0803, 0.2299, 0.2299, 0.2299, 0.2299],\n",
      "         [0.0730, 0.1111, 0.2720, 0.2720, 0.2720],\n",
      "         [0.1830, 0.1956, 0.1711, 0.2252, 0.2252],\n",
      "         [0.1735, 0.2010, 0.1493, 0.2012, 0.2750],\n",
      "         [0.1642, 0.2099, 0.1279, 0.2102, 0.2878]],\n",
      "\n",
      "        [[0.1004, 0.2249, 0.2249, 0.2249, 0.2249],\n",
      "         [0.1221, 0.0847, 0.2644, 0.2644, 0.2644],\n",
      "         [0.1333, 0.0853, 0.0968, 0.3423, 0.3423],\n",
      "         [0.1988, 0.2428, 0.2294, 0.1986, 0.1304],\n",
      "         [0.2601, 0.1589, 0.1828, 0.2609, 0.1373]],\n",
      "\n",
      "        [[0.0460, 0.2385, 0.2385, 0.2385, 0.2385],\n",
      "         [0.0767, 0.1349, 0.2628, 0.2628, 0.2628],\n",
      "         [0.1974, 0.1996, 0.1988, 0.2021, 0.2021],\n",
      "         [0.1730, 0.2107, 0.1971, 0.1534, 0.2659],\n",
      "         [0.1564, 0.3460, 0.2643, 0.0962, 0.1370]],\n",
      "\n",
      "        [[0.1949, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2106, 0.1470, 0.2141, 0.2141, 0.2141],\n",
      "         [0.2591, 0.1506, 0.0591, 0.2656, 0.2656],\n",
      "         [0.3013, 0.1989, 0.0973, 0.0953, 0.3071],\n",
      "         [0.3083, 0.1761, 0.0671, 0.0653, 0.3831]],\n",
      "\n",
      "        [[0.1999, 0.2000, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2364, 0.1947, 0.1896, 0.1896, 0.1896],\n",
      "         [0.1843, 0.2188, 0.1491, 0.2239, 0.2239],\n",
      "         [0.2120, 0.1382, 0.3594, 0.1601, 0.1304],\n",
      "         [0.1940, 0.2277, 0.1593, 0.2155, 0.2036]],\n",
      "\n",
      "        [[0.0697, 0.2326, 0.2326, 0.2326, 0.2326],\n",
      "         [0.0750, 0.0329, 0.2974, 0.2974, 0.2974],\n",
      "         [0.2053, 0.2214, 0.2115, 0.1809, 0.1809],\n",
      "         [0.2026, 0.2150, 0.2074, 0.1916, 0.1834],\n",
      "         [0.2033, 0.1518, 0.1810, 0.2674, 0.1965]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2264, 0.1934, 0.1934, 0.1934, 0.1934],\n",
      "         [0.2164, 0.1840, 0.1999, 0.1999, 0.1999],\n",
      "         [0.2066, 0.1907, 0.2053, 0.1987, 0.1987],\n",
      "         [0.2372, 0.1856, 0.2325, 0.1344, 0.2103],\n",
      "         [0.1688, 0.2175, 0.1723, 0.3039, 0.1375]],\n",
      "\n",
      "        [[0.2013, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.2003, 0.1990, 0.2002, 0.2002, 0.2002],\n",
      "         [0.1852, 0.1556, 0.2933, 0.1830, 0.1830],\n",
      "         [0.1941, 0.1688, 0.2808, 0.1641, 0.1922],\n",
      "         [0.1995, 0.2070, 0.1809, 0.2085, 0.2041]],\n",
      "\n",
      "        [[0.1755, 0.2061, 0.2061, 0.2061, 0.2061],\n",
      "         [0.2578, 0.1790, 0.1877, 0.1877, 0.1877],\n",
      "         [0.2423, 0.1850, 0.1894, 0.1916, 0.1916],\n",
      "         [0.1843, 0.2034, 0.2017, 0.2098, 0.2008],\n",
      "         [0.1545, 0.2098, 0.2043, 0.2307, 0.2007]],\n",
      "\n",
      "        [[0.1948, 0.2013, 0.2013, 0.2013, 0.2013],\n",
      "         [0.2182, 0.2183, 0.1878, 0.1878, 0.1878],\n",
      "         [0.2001, 0.2001, 0.2021, 0.1989, 0.1989],\n",
      "         [0.1998, 0.1998, 0.2212, 0.1912, 0.1879],\n",
      "         [0.2014, 0.2014, 0.1972, 0.2033, 0.1966]],\n",
      "\n",
      "        [[0.1980, 0.2005, 0.2005, 0.2005, 0.2005],\n",
      "         [0.2001, 0.1954, 0.2015, 0.2015, 0.2015],\n",
      "         [0.2233, 0.1920, 0.1186, 0.2331, 0.2331],\n",
      "         [0.2308, 0.2040, 0.1376, 0.1887, 0.2390],\n",
      "         [0.1990, 0.2001, 0.2037, 0.2008, 0.1963]],\n",
      "\n",
      "        [[0.2035, 0.1991, 0.1991, 0.1991, 0.1991],\n",
      "         [0.2447, 0.1603, 0.1983, 0.1983, 0.1983],\n",
      "         [0.2349, 0.1550, 0.2282, 0.1910, 0.1910],\n",
      "         [0.2675, 0.1416, 0.2560, 0.1399, 0.1950],\n",
      "         [0.1451, 0.2721, 0.1516, 0.2755, 0.1558]],\n",
      "\n",
      "        [[0.1859, 0.2035, 0.2035, 0.2035, 0.2035],\n",
      "         [0.2068, 0.1910, 0.2007, 0.2007, 0.2007],\n",
      "         [0.2051, 0.1932, 0.2006, 0.2005, 0.2005],\n",
      "         [0.1920, 0.2112, 0.1988, 0.1990, 0.1990],\n",
      "         [0.1909, 0.2164, 0.1998, 0.2001, 0.1928]],\n",
      "\n",
      "        [[0.2380, 0.1905, 0.1905, 0.1905, 0.1905],\n",
      "         [0.2137, 0.2115, 0.1916, 0.1916, 0.1916],\n",
      "         [0.2020, 0.2009, 0.2162, 0.1905, 0.1905],\n",
      "         [0.1951, 0.1964, 0.1796, 0.2194, 0.2096],\n",
      "         [0.2047, 0.2027, 0.2304, 0.1731, 0.1890]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1440, 0.2140, 0.2140, 0.2140, 0.2140],\n",
      "         [0.0618, 0.1229, 0.2717, 0.2717, 0.2717],\n",
      "         [0.1234, 0.1700, 0.2142, 0.2462, 0.2462],\n",
      "         [0.3109, 0.2321, 0.1880, 0.1034, 0.1656],\n",
      "         [0.0415, 0.0853, 0.1435, 0.6265, 0.1032]],\n",
      "\n",
      "        [[0.0777, 0.2306, 0.2306, 0.2306, 0.2306],\n",
      "         [0.2407, 0.2071, 0.1841, 0.1841, 0.1841],\n",
      "         [0.1405, 0.1797, 0.2441, 0.2178, 0.2178],\n",
      "         [0.1426, 0.1759, 0.2285, 0.2458, 0.2073],\n",
      "         [0.1580, 0.1860, 0.2278, 0.2412, 0.1870]],\n",
      "\n",
      "        [[0.1335, 0.2166, 0.2166, 0.2166, 0.2166],\n",
      "         [0.2243, 0.2178, 0.1860, 0.1860, 0.1860],\n",
      "         [0.1402, 0.1525, 0.2317, 0.2378, 0.2378],\n",
      "         [0.2131, 0.2086, 0.1877, 0.2041, 0.1865],\n",
      "         [0.2027, 0.2015, 0.1959, 0.2004, 0.1995]],\n",
      "\n",
      "        [[0.1244, 0.2189, 0.2189, 0.2189, 0.2189],\n",
      "         [0.2191, 0.2069, 0.1913, 0.1913, 0.1913],\n",
      "         [0.1396, 0.1743, 0.2142, 0.2360, 0.2360],\n",
      "         [0.2314, 0.2111, 0.1939, 0.1774, 0.1862],\n",
      "         [0.1410, 0.1825, 0.2320, 0.2977, 0.1468]],\n",
      "\n",
      "        [[0.1889, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.2055, 0.1908, 0.2012, 0.2012, 0.2012],\n",
      "         [0.1748, 0.2455, 0.1950, 0.1923, 0.1923],\n",
      "         [0.1804, 0.2854, 0.2091, 0.1199, 0.2052],\n",
      "         [0.1996, 0.2690, 0.2197, 0.1530, 0.1587]],\n",
      "\n",
      "        [[0.3030, 0.1743, 0.1743, 0.1743, 0.1743],\n",
      "         [0.0961, 0.1340, 0.2566, 0.2566, 0.2566],\n",
      "         [0.1164, 0.1544, 0.1930, 0.2681, 0.2681],\n",
      "         [0.1580, 0.1805, 0.2006, 0.2266, 0.2343],\n",
      "         [0.1752, 0.1950, 0.2121, 0.2338, 0.1839]],\n",
      "\n",
      "        [[0.1023, 0.2244, 0.2244, 0.2244, 0.2244],\n",
      "         [0.2157, 0.1979, 0.1955, 0.1955, 0.1955],\n",
      "         [0.0481, 0.2025, 0.2496, 0.2499, 0.2499],\n",
      "         [0.3331, 0.1783, 0.1628, 0.1631, 0.1627],\n",
      "         [0.2094, 0.1936, 0.1914, 0.1914, 0.2142]],\n",
      "\n",
      "        [[0.1857, 0.2036, 0.2036, 0.2036, 0.2036],\n",
      "         [0.1541, 0.1764, 0.2232, 0.2232, 0.2232],\n",
      "         [0.1243, 0.1610, 0.2097, 0.2525, 0.2525],\n",
      "         [0.2399, 0.2113, 0.1856, 0.1939, 0.1694],\n",
      "         [0.1821, 0.1966, 0.2127, 0.2071, 0.2016]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.0889, 0.2278, 0.2278, 0.2278, 0.2278],\n",
      "         [0.0906, 0.1877, 0.2406, 0.2406, 0.2406],\n",
      "         [0.2723, 0.1957, 0.1822, 0.1749, 0.1749],\n",
      "         [0.2928, 0.1926, 0.1758, 0.1718, 0.1670],\n",
      "         [0.1309, 0.2031, 0.2235, 0.2289, 0.2137]],\n",
      "\n",
      "        [[0.1229, 0.2193, 0.2193, 0.2193, 0.2193],\n",
      "         [0.1270, 0.1201, 0.2510, 0.2510, 0.2510],\n",
      "         [0.3321, 0.3632, 0.0810, 0.1118, 0.1118],\n",
      "         [0.2811, 0.2970, 0.1179, 0.1602, 0.1438],\n",
      "         [0.1878, 0.1861, 0.2169, 0.2061, 0.2031]],\n",
      "\n",
      "        [[0.2433, 0.1892, 0.1892, 0.1892, 0.1892],\n",
      "         [0.2062, 0.2030, 0.1969, 0.1969, 0.1969],\n",
      "         [0.3011, 0.2490, 0.1017, 0.1741, 0.1741],\n",
      "         [0.2475, 0.2257, 0.1462, 0.1909, 0.1897],\n",
      "         [0.2286, 0.2163, 0.1667, 0.1956, 0.1928]],\n",
      "\n",
      "        [[0.1970, 0.2007, 0.2007, 0.2007, 0.2007],\n",
      "         [0.1981, 0.2425, 0.1865, 0.1865, 0.1865],\n",
      "         [0.1951, 0.2194, 0.2086, 0.1884, 0.1884],\n",
      "         [0.1798, 0.3155, 0.2478, 0.1049, 0.1520],\n",
      "         [0.1948, 0.2235, 0.2107, 0.1707, 0.2004]],\n",
      "\n",
      "        [[0.2170, 0.1958, 0.1958, 0.1958, 0.1958],\n",
      "         [0.2042, 0.2024, 0.1978, 0.1978, 0.1978],\n",
      "         [0.2300, 0.2180, 0.1705, 0.1908, 0.1908],\n",
      "         [0.2670, 0.2387, 0.1430, 0.1705, 0.1807],\n",
      "         [0.2079, 0.2043, 0.1889, 0.1941, 0.2049]],\n",
      "\n",
      "        [[0.0994, 0.2252, 0.2252, 0.2252, 0.2252],\n",
      "         [0.0686, 0.1551, 0.2588, 0.2588, 0.2588],\n",
      "         [0.3100, 0.1973, 0.1954, 0.1486, 0.1486],\n",
      "         [0.2359, 0.1955, 0.1947, 0.2002, 0.1737],\n",
      "         [0.1264, 0.2097, 0.2119, 0.1964, 0.2556]],\n",
      "\n",
      "        [[0.1923, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.2003, 0.1996, 0.2000, 0.2000, 0.2000],\n",
      "         [0.2246, 0.1835, 0.1809, 0.2055, 0.2055],\n",
      "         [0.2595, 0.1705, 0.1655, 0.1888, 0.2157],\n",
      "         [0.1926, 0.2063, 0.2073, 0.2029, 0.1910]],\n",
      "\n",
      "        [[0.1411, 0.2147, 0.2147, 0.2147, 0.2147],\n",
      "         [0.2451, 0.2130, 0.1806, 0.1806, 0.1806],\n",
      "         [0.3204, 0.2293, 0.1402, 0.1550, 0.1550],\n",
      "         [0.2480, 0.2151, 0.1744, 0.1804, 0.1820],\n",
      "         [0.2714, 0.2212, 0.1638, 0.1719, 0.1717]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.2048, 0.1988, 0.1988, 0.1988, 0.1988],\n",
      "         [0.1225, 0.4110, 0.1555, 0.1555, 0.1555],\n",
      "         [0.1177, 0.3476, 0.2435, 0.1456, 0.1456],\n",
      "         [0.1436, 0.2794, 0.2245, 0.1888, 0.1637],\n",
      "         [0.1770, 0.2215, 0.2058, 0.1941, 0.2015]],\n",
      "\n",
      "        [[0.2221, 0.1945, 0.1945, 0.1945, 0.1945],\n",
      "         [0.2585, 0.2173, 0.1747, 0.1747, 0.1747],\n",
      "         [0.1715, 0.1916, 0.1968, 0.2201, 0.2201],\n",
      "         [0.2659, 0.2158, 0.2051, 0.1470, 0.1661],\n",
      "         [0.2198, 0.1732, 0.1634, 0.1117, 0.3319]],\n",
      "\n",
      "        [[0.3579, 0.1605, 0.1605, 0.1605, 0.1605],\n",
      "         [0.3416, 0.0993, 0.1864, 0.1864, 0.1864],\n",
      "         [0.2949, 0.0849, 0.3000, 0.1601, 0.1601],\n",
      "         [0.2544, 0.1233, 0.2570, 0.1869, 0.1783],\n",
      "         [0.2480, 0.0707, 0.2523, 0.1454, 0.2837]],\n",
      "\n",
      "        [[0.2231, 0.1942, 0.1942, 0.1942, 0.1942],\n",
      "         [0.2114, 0.1814, 0.2024, 0.2024, 0.2024],\n",
      "         [0.2121, 0.1547, 0.2453, 0.1940, 0.1940],\n",
      "         [0.2004, 0.1909, 0.2049, 0.2062, 0.1976],\n",
      "         [0.1766, 0.0931, 0.2372, 0.2580, 0.2351]],\n",
      "\n",
      "        [[0.2440, 0.1890, 0.1890, 0.1890, 0.1890],\n",
      "         [0.2160, 0.2749, 0.1697, 0.1697, 0.1697],\n",
      "         [0.1995, 0.1981, 0.2006, 0.2009, 0.2009],\n",
      "         [0.1936, 0.2008, 0.1884, 0.2306, 0.1866],\n",
      "         [0.1374, 0.1758, 0.1145, 0.4473, 0.1251]],\n",
      "\n",
      "        [[0.1722, 0.2069, 0.2069, 0.2069, 0.2069],\n",
      "         [0.1472, 0.1982, 0.2182, 0.2182, 0.2182],\n",
      "         [0.1446, 0.2136, 0.1572, 0.2423, 0.2423],\n",
      "         [0.1542, 0.2048, 0.1639, 0.2525, 0.2245],\n",
      "         [0.1220, 0.1720, 0.1313, 0.2216, 0.3532]],\n",
      "\n",
      "        [[0.1924, 0.2019, 0.2019, 0.2019, 0.2019],\n",
      "         [0.2092, 0.1367, 0.2180, 0.2180, 0.2180],\n",
      "         [0.1303, 0.0655, 0.5254, 0.1394, 0.1394],\n",
      "         [0.1921, 0.1750, 0.2322, 0.2068, 0.1939],\n",
      "         [0.1929, 0.1720, 0.2432, 0.2110, 0.1808]],\n",
      "\n",
      "        [[0.1889, 0.2028, 0.2028, 0.2028, 0.2028],\n",
      "         [0.1766, 0.1759, 0.2158, 0.2158, 0.2158],\n",
      "         [0.1990, 0.1990, 0.2011, 0.2004, 0.2004],\n",
      "         [0.1922, 0.1920, 0.2061, 0.2082, 0.2015],\n",
      "         [0.1881, 0.1878, 0.2065, 0.2093, 0.2082]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1839, 0.2040, 0.2040, 0.2040, 0.2040],\n",
      "         [0.1715, 0.2415, 0.1957, 0.1957, 0.1957],\n",
      "         [0.3185, 0.1113, 0.1459, 0.2122, 0.2122],\n",
      "         [0.2343, 0.1638, 0.1796, 0.2182, 0.2041],\n",
      "         [0.1923, 0.2062, 0.2025, 0.1950, 0.2041]],\n",
      "\n",
      "        [[0.1682, 0.2080, 0.2080, 0.2080, 0.2080],\n",
      "         [0.2356, 0.1522, 0.2040, 0.2040, 0.2040],\n",
      "         [0.1816, 0.2549, 0.1574, 0.2031, 0.2031],\n",
      "         [0.1931, 0.2351, 0.1777, 0.1880, 0.2061],\n",
      "         [0.2166, 0.1584, 0.2472, 0.2261, 0.1517]],\n",
      "\n",
      "        [[0.2068, 0.1983, 0.1983, 0.1983, 0.1983],\n",
      "         [0.2067, 0.2207, 0.1909, 0.1909, 0.1909],\n",
      "         [0.2021, 0.2040, 0.1945, 0.1997, 0.1997],\n",
      "         [0.2257, 0.2784, 0.0980, 0.2229, 0.1749],\n",
      "         [0.2166, 0.2365, 0.1529, 0.2155, 0.1785]],\n",
      "\n",
      "        [[0.2004, 0.1999, 0.1999, 0.1999, 0.1999],\n",
      "         [0.1932, 0.2226, 0.1947, 0.1947, 0.1947],\n",
      "         [0.1993, 0.2043, 0.1971, 0.1996, 0.1996],\n",
      "         [0.1793, 0.2889, 0.1449, 0.2028, 0.1842],\n",
      "         [0.1955, 0.2403, 0.1782, 0.2062, 0.1798]],\n",
      "\n",
      "        [[0.1916, 0.2021, 0.2021, 0.2021, 0.2021],\n",
      "         [0.2073, 0.1887, 0.2013, 0.2013, 0.2013],\n",
      "         [0.1684, 0.2663, 0.1771, 0.1941, 0.1941],\n",
      "         [0.1821, 0.2486, 0.1884, 0.1802, 0.2006],\n",
      "         [0.2002, 0.1995, 0.2001, 0.2002, 0.1999]],\n",
      "\n",
      "        [[0.2580, 0.1855, 0.1855, 0.1855, 0.1855],\n",
      "         [0.1260, 0.2495, 0.2081, 0.2081, 0.2081],\n",
      "         [0.1908, 0.2084, 0.1936, 0.2036, 0.2036],\n",
      "         [0.2070, 0.1931, 0.2046, 0.1985, 0.1967],\n",
      "         [0.3091, 0.1358, 0.2695, 0.1884, 0.0972]],\n",
      "\n",
      "        [[0.2395, 0.1901, 0.1901, 0.1901, 0.1901],\n",
      "         [0.2408, 0.2050, 0.1848, 0.1848, 0.1848],\n",
      "         [0.1890, 0.1946, 0.2197, 0.1983, 0.1983],\n",
      "         [0.3159, 0.2199, 0.0491, 0.2410, 0.1741],\n",
      "         [0.2617, 0.1898, 0.0502, 0.2059, 0.2924]],\n",
      "\n",
      "        [[0.1901, 0.2025, 0.2025, 0.2025, 0.2025],\n",
      "         [0.2013, 0.2005, 0.1994, 0.1994, 0.1994],\n",
      "         [0.1926, 0.1975, 0.2004, 0.2047, 0.2047],\n",
      "         [0.2496, 0.2110, 0.1919, 0.1812, 0.1663],\n",
      "         [0.1960, 0.1998, 0.2021, 0.2034, 0.1987]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.1762, 0.2060, 0.2060, 0.2060, 0.2060],\n",
      "         [0.2156, 0.1854, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1655, 0.2391, 0.1963, 0.1996, 0.1996],\n",
      "         [0.2192, 0.1871, 0.2037, 0.1878, 0.2022],\n",
      "         [0.1668, 0.2187, 0.1892, 0.2173, 0.2079]],\n",
      "\n",
      "        [[0.2011, 0.1997, 0.1997, 0.1997, 0.1997],\n",
      "         [0.1952, 0.2021, 0.2009, 0.2009, 0.2009],\n",
      "         [0.2475, 0.2047, 0.1247, 0.2116, 0.2116],\n",
      "         [0.2480, 0.2081, 0.1318, 0.1975, 0.2146],\n",
      "         [0.2773, 0.2250, 0.1306, 0.2115, 0.1555]],\n",
      "\n",
      "        [[0.1550, 0.2113, 0.2113, 0.2113, 0.2113],\n",
      "         [0.1665, 0.1381, 0.2318, 0.2318, 0.2318],\n",
      "         [0.1845, 0.1573, 0.1686, 0.2448, 0.2448],\n",
      "         [0.1996, 0.1722, 0.1836, 0.1851, 0.2595],\n",
      "         [0.2211, 0.1903, 0.2031, 0.2048, 0.1806]],\n",
      "\n",
      "        [[0.1470, 0.2132, 0.2132, 0.2132, 0.2132],\n",
      "         [0.1764, 0.1501, 0.2245, 0.2245, 0.2245],\n",
      "         [0.1840, 0.1543, 0.1829, 0.2394, 0.2394],\n",
      "         [0.1998, 0.1775, 0.1991, 0.1850, 0.2386],\n",
      "         [0.2257, 0.1662, 0.2235, 0.1849, 0.1996]],\n",
      "\n",
      "        [[0.1583, 0.2104, 0.2104, 0.2104, 0.2104],\n",
      "         [0.1894, 0.1947, 0.2053, 0.2053, 0.2053],\n",
      "         [0.1805, 0.1910, 0.2028, 0.2129, 0.2129],\n",
      "         [0.1875, 0.2031, 0.2212, 0.1512, 0.2371],\n",
      "         [0.2021, 0.2312, 0.2669, 0.1408, 0.1590]],\n",
      "\n",
      "        [[0.1576, 0.2106, 0.2106, 0.2106, 0.2106],\n",
      "         [0.2139, 0.1876, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1206, 0.3323, 0.1343, 0.2065, 0.2065],\n",
      "         [0.1930, 0.2115, 0.1949, 0.1980, 0.2026],\n",
      "         [0.1552, 0.3080, 0.1669, 0.1878, 0.1821]],\n",
      "\n",
      "        [[0.2089, 0.1978, 0.1978, 0.1978, 0.1978],\n",
      "         [0.1988, 0.2272, 0.1913, 0.1913, 0.1913],\n",
      "         [0.1971, 0.2256, 0.1982, 0.1895, 0.1895],\n",
      "         [0.2000, 0.2167, 0.2007, 0.1871, 0.1954],\n",
      "         [0.2080, 0.2862, 0.2107, 0.1594, 0.1358]],\n",
      "\n",
      "        [[0.1832, 0.2042, 0.2042, 0.2042, 0.2042],\n",
      "         [0.1560, 0.1336, 0.2368, 0.2368, 0.2368],\n",
      "         [0.1775, 0.1580, 0.1798, 0.2423, 0.2423],\n",
      "         [0.1940, 0.1832, 0.1953, 0.2012, 0.2263],\n",
      "         [0.2104, 0.1870, 0.2132, 0.2266, 0.1629]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Softmax for each column across one row tensor([[[0.3330, 0.1667, 0.1667, 0.1667, 0.1667],\n",
      "         [0.2346, 0.2139, 0.1838, 0.1838, 0.1838],\n",
      "         [0.2003, 0.2002, 0.1998, 0.1999, 0.1999],\n",
      "         [0.1420, 0.1732, 0.2781, 0.1663, 0.2404],\n",
      "         [0.2605, 0.2246, 0.1578, 0.2316, 0.1255]],\n",
      "\n",
      "        [[0.4798, 0.1301, 0.1301, 0.1301, 0.1301],\n",
      "         [0.2088, 0.1972, 0.1980, 0.1980, 0.1980],\n",
      "         [0.3919, 0.1258, 0.2117, 0.1353, 0.1353],\n",
      "         [0.2066, 0.1947, 0.2000, 0.2034, 0.1954],\n",
      "         [0.1791, 0.2128, 0.1966, 0.1874, 0.2240]],\n",
      "\n",
      "        [[0.1905, 0.2024, 0.2024, 0.2024, 0.2024],\n",
      "         [0.2234, 0.2024, 0.1914, 0.1914, 0.1914],\n",
      "         [0.2043, 0.1990, 0.2048, 0.1960, 0.1960],\n",
      "         [0.1563, 0.1254, 0.1591, 0.4484, 0.1107],\n",
      "         [0.1632, 0.1441, 0.1649, 0.2970, 0.2308]],\n",
      "\n",
      "        [[0.2445, 0.1889, 0.1889, 0.1889, 0.1889],\n",
      "         [0.1716, 0.2106, 0.2059, 0.2059, 0.2059],\n",
      "         [0.1476, 0.2299, 0.1843, 0.2191, 0.2191],\n",
      "         [0.2379, 0.1599, 0.1950, 0.2401, 0.1670],\n",
      "         [0.1993, 0.2014, 0.2004, 0.1993, 0.1996]],\n",
      "\n",
      "        [[0.2137, 0.1966, 0.1966, 0.1966, 0.1966],\n",
      "         [0.1941, 0.1987, 0.2024, 0.2024, 0.2024],\n",
      "         [0.2065, 0.1815, 0.2843, 0.1638, 0.1638],\n",
      "         [0.1892, 0.1567, 0.3017, 0.2174, 0.1349],\n",
      "         [0.2004, 0.1975, 0.2077, 0.2025, 0.1919]],\n",
      "\n",
      "        [[0.2019, 0.1995, 0.1995, 0.1995, 0.1995],\n",
      "         [0.1682, 0.2720, 0.1866, 0.1866, 0.1866],\n",
      "         [0.1835, 0.2260, 0.2066, 0.1920, 0.1920],\n",
      "         [0.1683, 0.2318, 0.2020, 0.2174, 0.1804],\n",
      "         [0.2151, 0.1386, 0.1675, 0.1514, 0.3275]],\n",
      "\n",
      "        [[0.3497, 0.1626, 0.1626, 0.1626, 0.1626],\n",
      "         [0.4224, 0.2056, 0.1240, 0.1240, 0.1240],\n",
      "         [0.1859, 0.1973, 0.2054, 0.2057, 0.2057],\n",
      "         [0.2796, 0.1554, 0.1048, 0.3574, 0.1028],\n",
      "         [0.2165, 0.1420, 0.1071, 0.2582, 0.2762]],\n",
      "\n",
      "        [[0.3053, 0.1737, 0.1737, 0.1737, 0.1737],\n",
      "         [0.2116, 0.1957, 0.1976, 0.1976, 0.1976],\n",
      "         [0.2522, 0.1903, 0.1633, 0.1971, 0.1971],\n",
      "         [0.0468, 0.1111, 0.1777, 0.5647, 0.0997],\n",
      "         [0.1761, 0.1926, 0.2022, 0.2280, 0.2010]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#get embeddings for the first batch\n",
    "for batch in train_data_loader:\n",
    "    pin_ids, action_ids, target_pin_ids = batch\n",
    "    print(pin_ids.shape)\n",
    "    print(action_ids.shape)\n",
    "    print(target_pin_ids.shape)\n",
    "    print(target_pin_ids)\n",
    "    embedding_decoder(pin_ids,action_ids)\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-25 22:27:12.452102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-24 22:27:12.452679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-23 22:27:12.452684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-22 22:27:12.452686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-21 22:27:12.452688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user  pin  action                  timestamp\n",
       "0    1  709       0 2023-07-25 22:27:12.452102\n",
       "1    1  534       0 2023-07-24 22:27:12.452679\n",
       "2    1  299       0 2023-07-23 22:27:12.452684\n",
       "3    1  357       0 2023-07-22 22:27:12.452686\n",
       "4    1  834       1 2023-07-21 22:27:12.452688"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user']\n",
    "        pin_ids = torch.tensor(self.df.iloc[idx]['pin_train'])\n",
    "        action_ids = torch.tensor(self.df.iloc[idx]['action_train'])\n",
    "        target_pin_ids = torch.tensor(self.df.iloc[idx]['pin_target'])\n",
    "        target_action_ids = torch.tensor(self.df.iloc[idx]['action_target'])\n",
    "        return user, pin_ids, action_ids, target_pin_ids, target_action_ids\n",
    "       \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[240, 986, 1000, 675, 995]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.452702, 2023-07-13 22:27:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[532, 164, 688, 779, 19]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-12 22:27:12.458356, 2023-07-13 22:27:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[994, 180, 298, 761, 125]</td>\n",
       "      <td>[1, 0, 2, 0, 0]</td>\n",
       "      <td>[2023-07-16 22:27:12.460196, 2023-07-17 22:27:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[631, 739, 799, 345, 886]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.461940, 2023-07-13 22:27:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[440, 333, 37, 381, 461]</td>\n",
       "      <td>[0, 2, 1, 0, 1]</td>\n",
       "      <td>[2023-07-17 22:27:12.463753, 2023-07-18 22:27:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                         pin           action  \\\n",
       "0     1  [240, 986, 1000, 675, 995]  [1, 0, 1, 2, 2]   \n",
       "1     2    [532, 164, 688, 779, 19]  [2, 2, 0, 1, 0]   \n",
       "2     3   [994, 180, 298, 761, 125]  [1, 0, 2, 0, 0]   \n",
       "3     4   [631, 739, 799, 345, 886]  [2, 0, 2, 1, 2]   \n",
       "4     5    [440, 333, 37, 381, 461]  [0, 2, 1, 0, 1]   \n",
       "\n",
       "                                           timestamp  \n",
       "0  [2023-07-12 22:27:12.452702, 2023-07-13 22:27:...  \n",
       "1  [2023-07-12 22:27:12.458356, 2023-07-13 22:27:...  \n",
       "2  [2023-07-16 22:27:12.460196, 2023-07-17 22:27:...  \n",
       "3  [2023-07-12 22:27:12.461940, 2023-07-13 22:27:...  \n",
       "4  [2023-07-17 22:27:12.463753, 2023-07-18 22:27:...  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = X_train.merge(y_train, on='user', how='inner')\n",
    "merged_test = X_test.merge(y_test, on='user', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.rename(columns={'pin_x': 'pin_train', 'action_x': 'action_train', 'timestamp_x': 'timestamp_train', 'pin_y':'pin_target', 'action_y': 'action_target', 'timestamp_y': 'timestamp_target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin_train</th>\n",
       "      <th>action_train</th>\n",
       "      <th>timestamp_train</th>\n",
       "      <th>pin_target</th>\n",
       "      <th>action_target</th>\n",
       "      <th>timestamp_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[240, 986, 1000, 675, 995]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.452702, 2023-07-13 22:27:...</td>\n",
       "      <td>[0, 0, 0, 0, 863]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 2023-07-11 22:27:12.452703]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[532, 164, 688, 779, 19]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-12 22:27:12.458356, 2023-07-13 22:27:...</td>\n",
       "      <td>[0, 0, 353, 781, 665]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-09 22:27:12.458360, 2023-07-10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[631, 739, 799, 345, 886]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.461940, 2023-07-13 22:27:...</td>\n",
       "      <td>[0, 0, 989, 979, 272]</td>\n",
       "      <td>[0, 0, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 2023-07-09 22:27:12.461944, 2023-07-10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>[706, 443, 386, 104, 939]</td>\n",
       "      <td>[0, 1, 2, 2, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.484793, 2023-07-13 22:27:...</td>\n",
       "      <td>[0, 0, 0, 27, 511]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-10 22:27:12.484796, 2023-07-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>[559, 987, 673, 126, 355]</td>\n",
       "      <td>[1, 0, 2, 0, 2]</td>\n",
       "      <td>[2023-07-12 22:27:12.489565, 2023-07-13 22:27:...</td>\n",
       "      <td>[651, 124, 476, 866, 443]</td>\n",
       "      <td>[2, 0, 2, 1, 0]</td>\n",
       "      <td>[2023-07-07 22:27:12.489573, 2023-07-08 22:27:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                   pin_train     action_train  \\\n",
       "0     1  [240, 986, 1000, 675, 995]  [1, 0, 1, 2, 2]   \n",
       "1     2    [532, 164, 688, 779, 19]  [2, 2, 0, 1, 0]   \n",
       "2     4   [631, 739, 799, 345, 886]  [2, 0, 2, 1, 2]   \n",
       "3    17   [706, 443, 386, 104, 939]  [0, 1, 2, 2, 2]   \n",
       "4    20   [559, 987, 673, 126, 355]  [1, 0, 2, 0, 2]   \n",
       "\n",
       "                                     timestamp_train  \\\n",
       "0  [2023-07-12 22:27:12.452702, 2023-07-13 22:27:...   \n",
       "1  [2023-07-12 22:27:12.458356, 2023-07-13 22:27:...   \n",
       "2  [2023-07-12 22:27:12.461940, 2023-07-13 22:27:...   \n",
       "3  [2023-07-12 22:27:12.484793, 2023-07-13 22:27:...   \n",
       "4  [2023-07-12 22:27:12.489565, 2023-07-13 22:27:...   \n",
       "\n",
       "                  pin_target    action_target  \\\n",
       "0          [0, 0, 0, 0, 863]  [0, 0, 0, 0, 1]   \n",
       "1      [0, 0, 353, 781, 665]  [0, 0, 0, 1, 0]   \n",
       "2      [0, 0, 989, 979, 272]  [0, 0, 0, 2, 1]   \n",
       "3         [0, 0, 0, 27, 511]  [0, 0, 0, 0, 1]   \n",
       "4  [651, 124, 476, 866, 443]  [2, 0, 2, 1, 0]   \n",
       "\n",
       "                                    timestamp_target  \n",
       "0           [0, 0, 0, 0, 2023-07-11 22:27:12.452703]  \n",
       "1  [0, 0, 2023-07-09 22:27:12.458360, 2023-07-10 ...  \n",
       "2  [0, 0, 2023-07-09 22:27:12.461944, 2023-07-10 ...  \n",
       "3  [0, 0, 0, 2023-07-10 22:27:12.484796, 2023-07-...  \n",
       "4  [2023-07-07 22:27:12.489573, 2023-07-08 22:27:...  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecommenderDataset(merged_train)\n",
    "test_dataset = RecommenderDataset(merged_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n",
      "tensor([0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7])\n",
      "tensor([1, 3, 3, 2, 5, 4, 4, 1])\n",
      "tensor([ 1,  4,  7,  9, 14, 18, 22])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([ 0,  1,  4,  7,  9, 14, 18, 22])\n",
      "tensor([[0, 4],\n",
      "        [1, 2],\n",
      "        [2, 2],\n",
      "        [3, 3],\n",
      "        [4, 0],\n",
      "        [5, 1],\n",
      "        [6, 1],\n",
      "        [7, 4]])\n"
     ]
    }
   ],
   "source": [
    "#get embeddings for the first batch\n",
    "for batch in train_data_loader:\n",
    "\n",
    "    pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "    #print(user)\n",
    "    #print(pin_ids.shape)\n",
    "    #print(action_ids.shape)\n",
    "    print(target_pin_ids.shape)\n",
    "    #print(target_pin_ids)\n",
    "\n",
    "    #user_embeddings = embedding_decoder(pin_ids,action_ids)\n",
    "    #print(user_embeddings.shape)\n",
    "\n",
    "    #for each user select the last embedding\n",
    "    #user_embeddings = user_embeddings[:,-1,:]\n",
    "    # Create a mask of non-zero elements\n",
    "    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "    # Get the indices of non-zero elements\n",
    "    indices = mask.nonzero() #shape [num_non_zero_elements, 2]\n",
    "\n",
    "    #print(mask)\n",
    "    #print(indices.shape)\n",
    "    #print(indices)  \n",
    "    # Get the unique row indices and the counts of non-zero elements in each row\n",
    "    unique_rows, counts = indices[:, 0].unique(return_counts=True) #shape [num_unique_rows, 1] #this is pulling the unique row indices and the counts of non-zero elements in each row\n",
    "    print(indices[:,0])\n",
    "    #print(unique_rows.size(0))\n",
    "    print(counts)\n",
    "    print(counts.cumsum(0)[:-1])\n",
    "    ## Get the starting indices of non-zero elements in each row in the 'indices' tensor\n",
    "    starts = torch.cat((torch.tensor([0]), counts.cumsum(0)[:-1]))\n",
    "#\n",
    "    ## Generate a random number for each row in the range of the number of non-zero elements in the row\n",
    "    rand_num = torch.randint(counts.min(), size=(unique_rows.size(0),)).to(target_pin_ids.device)\n",
    "    print(rand_num)\n",
    "    ## Add the starting indices to the random numbers to get the indices of the selected elements in the 'indices' tensor\n",
    "    selected_indices = starts + rand_num\n",
    "    print(selected_indices)\n",
    "    ## Get the indices of the selected elements in the 'input_tensor'\n",
    "    selected_elements_indices = indices[selected_indices]\n",
    "    print(selected_elements_indices)\n",
    "#\n",
    "    ## Get the selected elements\n",
    "    #selected_elements =target_pin_ids[selected_elements_indices[:, 0], selected_elements_indices[:, 1]]\n",
    "    #print(selected_elements.unsqueeze(1))\n",
    "\n",
    "    #for every user embedding multiply all the pin embeddings\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0, 863],\n",
      "        [  0,   0, 353, 781, 665],\n",
      "        [  0,   0, 989, 979, 272],\n",
      "        [  0,   0,   0,  27, 511],\n",
      "        [651, 124, 476, 866, 443],\n",
      "        [  0, 955, 345, 310,  63],\n",
      "        [  0, 665, 288, 740, 103],\n",
      "        [  0,   0,   0,   0, 841]])\n",
      "tensor([863, 353, 781, 665, 989, 979, 272,  27, 511, 651, 124, 476, 866, 443,\n",
      "        955, 345, 310,  63, 665, 288, 740, 103, 841])\n",
      "tensor([[4],\n",
      "        [3],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4]])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'random_choice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[296], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m unique_pins \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(target_pin_ids, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m user,pin \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(target_action_ids):\n\u001b[0;32m---> 42\u001b[0m     negative_pin \u001b[39m=\u001b[39m  [torch\u001b[39m.\u001b[39mrandom_choice(unique_pins[user]) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n\u001b[1;32m     47\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[296], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m unique_pins \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(target_pin_ids, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m user,pin \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(target_action_ids):\n\u001b[0;32m---> 42\u001b[0m     negative_pin \u001b[39m=\u001b[39m  [torch\u001b[39m.\u001b[39;49mrandom_choice(unique_pins[user]) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n\u001b[1;32m     47\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'random_choice'"
     ]
    }
   ],
   "source": [
    "for batch in train_data_loader:\n",
    "\n",
    "    pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "    #print(user)\n",
    "    #print(pin_ids.shape)\n",
    "    #print(action_ids.shape)\n",
    "    #print(target_pin_ids.shape)\n",
    "    #print(target_pin_ids)\n",
    "\n",
    "    #user_embeddings = embedding_decoder(pin_ids,action_ids)\n",
    "    #print(user_embeddings.shape)\n",
    "\n",
    "    #for each user select the last embedding\n",
    "    #user_embeddings = user_embeddings[:,-1,:]\n",
    "    # Create a mask of non-zero elements\n",
    "    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "    print(target_pin_ids)\n",
    "    non_zero_elements = target_pin_ids[mask]\n",
    "    print(non_zero_elements)\n",
    "    indices = torch.multinomial(mask.float(), 1)\n",
    "    indices[indices < 0] = 0\n",
    "    print(indices)\n",
    "    print(indices.shape)\n",
    "    #print(indices.squeeze())\n",
    "    print(indices.squeeze().shape)\n",
    "    #print(torch.arange(indices.size(0)))\n",
    "    # Reshape to 8x1\n",
    "    print(torch.arange(target_pin_ids.shape[0]))\n",
    "    random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "    random_numbers = random_numbers.view(-1, 1)\n",
    "\n",
    "    #print(random_numbers)   \n",
    "    #embedded = pin_embeddings(random_numbers)\n",
    "    #print(embedded.shape)\n",
    "    #print(embedded.transpose(2,1).shape)\n",
    "    # Get unique pins seen by user\n",
    "    #negative sampling for each user\n",
    "    # Find where the matrix has zeros\n",
    "    unique_pins = torch.unique(target_pin_ids, dim=1)\n",
    "\n",
    "       \n",
    "\n",
    "  \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1])\n",
      "tensor(0.0001, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pins_vocab, pin_embedding_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "\n",
    "    def forward(self, pin_ids):\n",
    "        return self.pin_embeddings(pin_ids)\n",
    "\n",
    "# Outside your loop\n",
    "model = MyModel(pins_vocab, pin_embedding_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for batch in train_data_loader:\n",
    "\n",
    "    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "\n",
    "    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "    indices = torch.multinomial(mask.float(), 1)\n",
    "    indices[indices < 0] = 0\n",
    "\n",
    "    random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "    random_numbers = random_numbers.view(-1, 1)\n",
    "\n",
    "    embedded = model(random_numbers)\n",
    "\n",
    "    dot_product = torch.bmm(embedded, embedded.transpose(2,1))\n",
    "\n",
    "    sigmoid = torch.sigmoid(dot_product)\n",
    "    #print(sigmoid.shape)\n",
    "    #squeeze the sigmoid\n",
    "    preds = sigmoid.squeeze(1)\n",
    "    #print(preds)\n",
    "    print(preds.shape)\n",
    "\n",
    "\n",
    "    negative_pin_embeddings = self.pin_embedding(torch.randint(0,  pin_embedding_shape, (8,)))\n",
    "        negative_pin_dot_product = torch.bmm(user_embeddings, negative_pin_embeddings.transpose(2,1))\n",
    "        negative_pin_sigmoid = torch.sigmoid(negative_pin_dot_product)\n",
    "\n",
    "        positiv_target = torch.ones_like(sigmoid)\n",
    "        negative_target = torch.zeros_like(negative_pin_sigmoid)\n",
    "\n",
    "\n",
    "\n",
    "    target = torch.ones_like(preds) # or torch.zeros_like(preds) if you want the model to output values close to 0\n",
    "    loss = F.binary_cross_entropy(preds, target)\n",
    "    print(loss)\n",
    "\n",
    "    optimizer.zero_grad() #zero out the gradients\n",
    "    loss.backward() #calculate the gradients\n",
    "    optimizer.step() #update the weights\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "tensor([525, 903, 207, 753, 366, 929,  34, 566])\n",
      "torch.Size([8, 12])\n"
     ]
    }
   ],
   "source": [
    "pin_embedding = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "pin_embedding_shape = pin_embedding.weight.shape[0]\n",
    "print(pin_embedding_shape)\n",
    "\n",
    "#sample negative pin embeddings for batch size 8\n",
    "negative_indices = torch.randint(0,  pin_embedding_shape, (8,))\n",
    "print(negative_indices)\n",
    "negative_pin_embeddings = pin_embedding(negative_indices)\n",
    "print(negative_pin_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | user_embedding | TransferDecoder | 936 K \n",
      "1 | pin_embedding  | Embedding       | 12.0 K\n",
      "---------------------------------------------------\n",
      "948 K     Trainable params\n",
      "0         Non-trainable params\n",
      "948 K     Total params\n",
      "3.794     Total estimated model params size (MB)\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099cdd8a29c64f428b7c898754cf8ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user shape 8\n",
      "  user_embeddings  torch.Size([8, 5, 12])\n",
      "  user_embeddings  torch.Size([8, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  dot_product  torch.Size([8, 1])\n",
      "1001\n",
      "tensor([346, 584, 164, 575, 126, 192, 609, 834])\n",
      "  negative_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "user shape 8\n",
      "  user_embeddings  torch.Size([8, 5, 12])\n",
      "  user_embeddings  torch.Size([8, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  dot_product  torch.Size([8, 1])\n",
      "1001\n",
      "tensor([467, 839, 917, 177, 870, 202, 325, 990])\n",
      "  negative_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "user shape 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_embeddings  torch.Size([8, 5, 12])\n",
      "  user_embeddings  torch.Size([8, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  dot_product  torch.Size([8, 1])\n",
      "1001\n",
      "tensor([ 55,  91, 820, 377, 220, 222, 885, 583])\n",
      "  negative_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "user shape 7\n",
      "  user_embeddings  torch.Size([7, 5, 12])\n",
      "  user_embeddings  torch.Size([7, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  dot_product  torch.Size([7, 1])\n",
      "1001\n",
      "tensor([ 79, 456, 807, 944, 249, 875, 820])\n",
      "  negative_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "class RecommenderSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size, pin_embedding_dim, action_embedding_dim, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.user_embedding = TransferDecoder(num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size)\n",
    "        self.pin_embedding = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "        \n",
    "    def forward(self, user ,pin_ids, action_ids,target_pin_ids):\n",
    "        print(f'user shape {user.shape[0]}')\n",
    "         #positive pins sampled from the target pins\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "    \n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "    \n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids)\n",
    "        print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "        #extract the last embedding for each user\n",
    "        user_embeddings = user_embeddings[:,-1,:]\n",
    "        print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "\n",
    "        \n",
    "        user_embeddings = user_embeddings.unsqueeze(1)\n",
    "        print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "       \n",
    "\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers)\n",
    "        print(f'  positive_pin_embeddings  {positive_pin_embeddings.shape}')\n",
    "    \n",
    "        dot_product = torch.bmm(user_embeddings, positive_pin_embeddings.transpose(2,1))\n",
    "        dot_product = dot_product.squeeze(1)\n",
    "        print(f'  dot_product  {dot_product.shape}')\n",
    "        sigmoid = torch.sigmoid(dot_product)\n",
    "        #negative pins sampled randomly\n",
    "        \n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0]\n",
    "        print(pin_embedding_shape)\n",
    "\n",
    "        #sample negative pin embeddings for batch size 8\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],))\n",
    "        print(negative_indices)\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices)\n",
    "        \n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1)\n",
    "        print(f'  negative_pin_embeddings  {negative_pin_embeddings.shape}')\n",
    "        print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "\n",
    "        negative_pin_dot_product = torch.bmm(user_embeddings, negative_pin_embeddings.transpose(2,1))\n",
    "        negative_pin_dot_product = negative_pin_dot_product.squeeze(1)\n",
    "        negative_pin_sigmoid = torch.sigmoid(negative_pin_dot_product)\n",
    "\n",
    "        total_sigmoid = torch.cat([sigmoid, negative_pin_sigmoid], dim=1)\n",
    "        \n",
    "        targets = torch.ones_like(sigmoid)\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_sigmoid)], dim=1)\n",
    "\n",
    "\n",
    "        preds = total_sigmoid.squeeze(1)\n",
    "        \n",
    "        return preds, targets\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "        preds, targets  = self.forward(user,pin_ids, action_ids, target_pin_ids)\n",
    "        #targets = torch.ones_like(preds)\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size, pin_embedding_dim, action_embedding_dim, learning_rate=0.0001)\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "trainer.fit(model, train_data_loader, test_data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positve_pins = torch.ones(8,1)\n",
    "negative_pins = torch.zeros(16,1)\n",
    "combined_prediction = torch.cat([positve_pins, negative_pins], dim=0)\n",
    "combined_prediction.shape\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
