{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 19:07:04.807290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling  \n",
    "from transformers import Trainer, TrainingArguments \n",
    "import torch  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer \n",
    "import torch.nn.functional as F \n",
    "import os\n",
    "#os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline,  AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoConfig, EarlyStoppingCallback, TrainerCallback\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create the list of pins, users and actions\n",
    "pins = list(range(0,1000))\n",
    "users = list(range(1,101))\n",
    "actions = ['click', 'closeup', 'save']\n",
    "\n",
    "# Creating empty DataFrame\n",
    "df = pd.DataFrame(columns=['user', 'pin', 'action', 'timestamp'])\n",
    "\n",
    "# Populating the DataFrame\n",
    "for user in users:\n",
    "    num_pins = np.random.randint(3, 21)  # user engages with min 3 pins and max 20 pins\n",
    "    engaged_pins = np.random.choice(pins, num_pins, replace=False)  # engaged pins for this user\n",
    "    engaged_actions = np.random.choice(actions, num_pins)  # actions for this user\n",
    "    timestamps = [datetime.now() - timedelta(days=x) for x in range(num_pins)]  # random timestamps for user engagement\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        'user': user,\n",
    "        'pin': engaged_pins,\n",
    "        'action': engaged_actions,\n",
    "        'timestamp': timestamps\n",
    "    })\n",
    "    \n",
    "    df = pd.concat([df, temp_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['pin'])\n",
    "min(df['pin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>708</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-29 19:04:36.448816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>533</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-28 19:04:36.448826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-27 19:04:36.448828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-26 19:04:36.448830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>833</td>\n",
       "      <td>closeup</td>\n",
       "      <td>2023-07-25 19:04:36.448831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user  pin   action                  timestamp\n",
       "0    1  708    click 2023-07-29 19:04:36.448816\n",
       "1    1  533    click 2023-07-28 19:04:36.448826\n",
       "2    1  298    click 2023-07-27 19:04:36.448828\n",
       "3    1  356    click 2023-07-26 19:04:36.448830\n",
       "4    1  833  closeup 2023-07-25 19:04:36.448831"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map actions to integers\n",
    "action_to_int = {'click': 0, 'closeup': 1, 'save': 2}\n",
    "df['action'] = df['action'].map(action_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split users into train and test users\n",
    "train_users, test_users = train_test_split(users, test_size=0.2)\n",
    "\n",
    "train_df = df[df['user'].isin(train_users)]\n",
    "test_df = df[df['user'].isin(test_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the max and min timestamp in train_df\n",
    "#cutoff_times is 2 weeks prior to the max timestamp\n",
    "max_timestamp = train_df['timestamp'].max()\n",
    "min_timestamp = train_df['timestamp'].min()\n",
    "cut_off_train = max_timestamp - timedelta(days=14)\n",
    "\n",
    "# Create X_train and y_train\n",
    "X_train = train_df[train_df['timestamp'] > cut_off_train]\n",
    "y_train = train_df[(train_df['timestamp'] <= cut_off_train)]\n",
    "\n",
    "# Create X_test and y_test\n",
    "X_test = test_df[test_df['timestamp'] > cut_off_train]\n",
    "y_test = test_df[(test_df['timestamp'] <= cut_off_train)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>862</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-15 19:04:36.448845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-15 19:04:36.453075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-14 19:04:36.453077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-13 19:04:36.453078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-15 19:04:36.455612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  pin  action                  timestamp\n",
       "14    1  862       1 2023-07-15 19:04:36.448845\n",
       "14    2  664       0 2023-07-15 19:04:36.453075\n",
       "15    2  780       1 2023-07-14 19:04:36.453077\n",
       "16    2  352       0 2023-07-13 19:04:36.453078\n",
       "14    4  271       1 2023-07-15 19:04:36.455612"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#if the list is empty add 5 0's or list is less then length 5 add 0 to make it 5 and it if greater then 5 trim it\n",
    "\n",
    "def pad_or_trim(lst):\n",
    "    if len(lst) == 0:\n",
    "        return [0,0,0,0,0]\n",
    "    elif len(lst) < 5:\n",
    "        return [0]*(5-len(lst)) + lst\n",
    "    else:\n",
    "        return lst[:5]\n",
    "\n",
    "X_train = X_train.sort_values(by=['timestamp'])\n",
    "X_train = X_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_train['pin'] = X_train['pin'].apply(lambda x: pad_or_trim(x))\n",
    "X_train['action'] = X_train['action'].apply(pad_or_trim)\n",
    "X_train['timestamp'] = X_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_train = y_train.sort_values(by=['timestamp'])\n",
    "y_train = y_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_train['pin'] = y_train['pin'].apply(pad_or_trim)\n",
    "y_train['action'] = y_train['action'].apply(pad_or_trim)\n",
    "y_train['timestamp'] = y_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "\n",
    "X_test = X_test.sort_values(by=['timestamp'])   \n",
    "X_test = X_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_test['pin'] = X_test['pin'].apply(pad_or_trim)\n",
    "X_test['action'] = X_test['action'].apply(pad_or_trim)\n",
    "X_test['timestamp'] = X_test['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_test = y_test.sort_values(by=['timestamp'])\n",
    "y_test = y_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_test['pin'] = y_test['pin'].apply(pad_or_trim)\n",
    "y_test['action'] = y_test['action'].apply(pad_or_trim)\n",
    "y_test['timestamp'] = y_test['timestamp'].apply(pad_or_trim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.453074, 2023-07-17 19:04:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                       pin           action  \\\n",
       "1     2  [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "\n",
       "                                           timestamp  \n",
       "1  [2023-07-16 19:04:36.453074, 2023-07-17 19:04:...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 352, 780, 664]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-13 19:04:36.453078, 2023-07-14 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                    pin           action  \\\n",
       "1     2  [0, 0, 352, 780, 664]  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                           timestamp  \n",
       "1  [0, 0, 2023-07-13 19:04:36.453078, 2023-07-14 ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ALL THE PARAMETERS FOR THE MODEL\n",
    "hidden_size = 12 #hidden size of transformer and the final output coming from the transformer\n",
    "pin_embedding_dim = 12 #dimension of the pin embedding\n",
    "action_embedding_dim = 3 #dimension of the action embedding\n",
    "num_attention_heads = 12 #number of attention heads in transformer that is concatenated together to form the final attention head of dimension 768\n",
    "pins_vocab = 1001 #number of pins\n",
    "actions_vocab = 3 #number of actions\n",
    "num_layers = 12 #number of transformer layers in the model which is a replication of the same transformer layer whose input is the output of the previous layer and the output is the input to the next layer\n",
    "dropout = 0.1\n",
    "max_length = 5 #maximum length of the input sequence\n",
    "batch_size = 8 #batch size for training\n",
    "epochs = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Embeddings(nn.Module):\n",
    "  \"\"\"\n",
    "  Creates a single Dense Embedding for each token --> Token Embedding + Positional Embedding\n",
    "  \"\"\"\n",
    "  def __init__(self, pin_embeddings,actions_vocab, action_embedding_dim, hidden_size):\n",
    "    super().__init__()\n",
    "    self.pin_embedding = nn.Embedding.from_pretrained(pin_embeddings)\n",
    "    self.linear1 = nn.Linear(384, hidden_size)\n",
    "    self.pin_embedding.weight.requires_grad = True\n",
    "   \n",
    "        \n",
    "    self.action_type_embedding = nn.Embedding(actions_vocab, action_embedding_dim)\n",
    "    self.linear = nn.Linear(pin_embedding_dim+action_embedding_dim, hidden_size)\n",
    "    self.position_embedding = nn.Embedding(5, hidden_size)\n",
    "    self.layer_norm = nn.LayerNorm(hidden_size, eps= 1e-12)\n",
    "    self.dropout = nn.Dropout()\n",
    "\n",
    "  def forward(self,pin_ids,action_ids):\n",
    "    # pin_ids = [batch_size, seq_len]\n",
    "    # action_ids = [batch_size, seq_len]\n",
    "    # position_ids = [batch_size, seq_len]\n",
    "    batch_size, seq_len = pin_ids.shape\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).expand((batch_size, seq_len))\n",
    "    # position_ids = [batch_size, seq_len]\n",
    "    pin_embeddings = self.pin_embedding(pin_ids)\n",
    "    pin_embeddings = self.linear1(pin_embeddings) #linear layer to get the embeddings to the desired hidden size\n",
    "    # pin_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    action_embeddings = self.action_type_embedding(action_ids)\n",
    "    # action_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    position_embeddings = self.position_embedding(position_ids)\n",
    "    #print(position_embeddings.shape)\n",
    "\n",
    "    #concatenate all the pin and action embeddings\n",
    "    embeddings = torch.cat([pin_embeddings, action_embeddings], dim=-1)\n",
    "    #apply a linear layer to get the embeddings to the desired hidden size\n",
    "\n",
    "    embeddings = self.linear(embeddings)\n",
    "    #print(embeddings.shape)\n",
    "\n",
    "    #add the position embeddings\n",
    "    embeddings = embeddings + position_embeddings\n",
    "\n",
    "    #embeddings = pin_embeddings + action_embeddings + position_embeddings\n",
    "    # embeddings = [batch_size, seq_len, hidden_size]\n",
    "    embeddings = self.layer_norm(embeddings)\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pin_embedding = pickle.load(open(\"embeddings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5, 3])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "position_ids = torch.arange(5, dtype=torch.long).expand((8, 5))\n",
    "embedded =  nn.Embedding(5, 3)\n",
    "position_embeddings = embedded(position_ids)\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 12])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded = Embeddings(pin_embedding,3,3,12)\n",
    "\n",
    "#sample embeddings\n",
    "\n",
    "pin_ids = torch.tensor([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "print(pin_ids.shape)\n",
    "action_ids = torch.tensor([[1,2,0,2,1],[1,2,0,2,1]])\n",
    "user_embeddings = embedded(pin_ids,action_ids)\n",
    "user_embeddings.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pinformer_dataset(Dataset):\n",
    "  \"\"\"\n",
    "  Creates a dataset for the pinformer model\n",
    "  \"\"\"\n",
    "  def __init__(self,df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    pin_ids = torch.tensor(self.df.iloc[idx]['pin'])\n",
    "    action_ids = torch.tensor(self.df.iloc[idx]['action'])\n",
    "\n",
    "    target_pin_ids = torch.tensor(self.df.iloc[idx]['pin']) \n",
    "    #pick one random non-zero pin from the list of pins and make it the target pin\n",
    "    target_pin_ids = target_pin_ids[target_pin_ids != 0]\n",
    "    target_pin_ids = target_pin_ids[torch.randperm(len(target_pin_ids))]\n",
    "    target_pin_ids = target_pin_ids[0]\n",
    "\n",
    "    return pin_ids,action_ids, target_pin_ids\n",
    "\n",
    "train_df = pinformer_dataset(X_train)\n",
    "test_df = pinformer_dataset(X_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_df, batch_size=8, shuffle=False) #shuffle is false because we want to preserve the order of the sequence so user 1 embeddings are in sequence and user 2 embeddings are in sequence and so on and can be pulled \n",
    "test_data_loader = DataLoader(test_df, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, embed_dim, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim #dimension of one head \n",
    "    #infeatures=embed_dim\n",
    "    #outfeatures=head_dim\n",
    "    self.q = nn.Linear(embed_dim, head_dim)\n",
    "    self.k = nn.Linear(embed_dim, head_dim)\n",
    "    self.v = nn.Linear(embed_dim, head_dim)\n",
    "    \n",
    "  \n",
    "  def causal_mask(self,batch_size,size, dtype):  \n",
    "    mask = torch.tril(torch.ones(size,size)).unsqueeze(0)\n",
    "    return mask\n",
    "    \n",
    "  \n",
    "      \n",
    "  def scaled_dot_product_attention(self,query, key, value):\n",
    "    dim_k = query.size(dim=-1)  \n",
    "    #print(dim_k)    \n",
    "    #print(f'Dimension of the q,k,v Matrix [Batch_size, seq_len, Head_dim] of One Head {dim_k}')\n",
    "    scores = torch.bmm(query,key.transpose(1,2))/ sqrt(dim_k)  #[(1,5,768)*(1,768,5)]/sqrt(768) >>> [batch_size,5,5] \n",
    "    \n",
    "    mask = self.causal_mask(scores.size(0),scores.size(1),dtype=torch.int32)\n",
    "    #print(mask)\n",
    "    scores = scores.masked_fill(mask==0, float(0)) \n",
    "    weights = F.softmax(scores, dim=-1) #[batch_size,5,5]\n",
    "    #print(weights)\n",
    "    #print(f'Softmax for each column across one row {weights}')\n",
    "    weights_dot_values = torch.bmm(weights,value)  #[batch_size,5,5]*[batch_size,5,64] >>> [batch_size,5,64]\n",
    "    #print(f'Last Step is to multiply weights and values {weights_dot_values.shape}')\n",
    "    return weights_dot_values \n",
    "\n",
    "  def forward(self, hidden_state):\n",
    "    #head_state = [batch_size, seq_len, embed_dim]\n",
    "    #print(f'Input Embedding for Each Token with X Matrix {hidden_state.size()}')\n",
    "    #q = X*W_q\n",
    "    q = self.q(hidden_state) #q = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Query Matrix W_q {q.size()}')\n",
    "    k = self.k(hidden_state) #k = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Key Matrix W_k {k.size()}')\n",
    "    v = self.k(hidden_state) #v = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Value Matrix W_k {v.size()}')\n",
    "    #print('-----------------Calculating Self Attention--------------------')\n",
    "    attn_outputs = self.scaled_dot_product_attention(q,k,v) #attn_outputs = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the attention Output with one Head and Head Dimension {self.head_dim} is {attn_outputs.size()}')\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0239, -0.4851, -0.0187,  0.6908, -0.4658,  0.4746,  0.1577,\n",
       "           0.5094, -0.0035, -0.1617,  0.1693, -0.3923],\n",
       "         [ 0.1838, -0.7917,  0.1611,  0.5430,  0.1771,  0.7911, -0.3787,\n",
       "           0.6660, -0.0925, -0.6892,  0.6260,  0.0477],\n",
       "         [ 0.0618, -0.5948,  0.0668,  0.4992, -0.1073,  0.5267, -0.0878,\n",
       "           0.4542, -0.0959, -0.3523,  0.3172, -0.1377],\n",
       "         [-0.0432, -0.4621, -0.0591,  0.7151, -0.6762,  0.3814,  0.3483,\n",
       "           0.4264, -0.0356, -0.0203,  0.0213, -0.4844],\n",
       "         [ 0.0149, -0.5436,  0.0121,  0.6700, -0.3609,  0.5335,  0.0693,\n",
       "           0.5398, -0.0206, -0.2555,  0.2491, -0.3177]],\n",
       "\n",
       "        [[-0.2882, -0.1613, -0.1095,  0.0969, -0.4174,  0.1628,  0.5075,\n",
       "           0.0136, -0.3349,  0.0481, -0.1548, -0.0401],\n",
       "         [-0.2584, -0.2051, -0.0520,  0.0963, -0.3581,  0.2062,  0.4672,\n",
       "          -0.0204, -0.3337,  0.0266, -0.1997,  0.0265],\n",
       "         [-0.3277, -0.1233, -0.1639,  0.1292, -0.5274,  0.1351,  0.5573,\n",
       "           0.0520, -0.2855,  0.0793, -0.0824, -0.1393],\n",
       "         [-0.4199, -0.0753, -0.3025,  0.3020, -0.7690,  0.1080,  0.6503,\n",
       "           0.2196, -0.0388,  0.1481,  0.2128, -0.4270],\n",
       "         [-0.0567, -0.3426,  0.1532,  0.2849,  0.0483,  0.1907,  0.0939,\n",
       "          -0.0969, -0.3553,  0.1323, -0.3606,  0.3601]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a single attention head\n",
    "attention_head = AttentionHead(hidden_size,12)\n",
    "\n",
    "#sample input first calculate the embeddings for the input\n",
    "pin_ids = torch.tensor([[1,2,3,4,5],[1,2,3,4,5]])\n",
    "print(pin_ids.shape)\n",
    "action_ids = torch.tensor([[1,2,0,2,1],[1,2,0,2,1]])\n",
    "embedded(pin_ids,action_ids)\n",
    "\n",
    "#pass the embeddings to the attention head\n",
    "attention_head(embedded(pin_ids,action_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super().__init__()\n",
    "    embed_dim = hidden_size\n",
    "    num_heads = num_attention_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "    self.heads = [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] #initializing all the heads\n",
    "    self.w_0 = nn.Linear(embed_dim,embed_dim) #the purpose of this linear layer is to concatenate all the heads together to form the final output of the multihead attention\n",
    "\n",
    "  def forward(self,hidden_state):\n",
    "    '''\n",
    "    hidden_state: Input Embedding with dimensions [batch_size, seq_len, embedding_dimension]\n",
    "    '''\n",
    "    attention_outputs = [head(hidden_state) for head in self.heads] #Calculating Self-Attention on each head\n",
    "    contcat_attn_outputs_allheads = torch.cat(attention_outputs, dim=-1) #[batch_size,seq_len, embed_dim]\n",
    "    Z =   self.w_0(contcat_attn_outputs_allheads) #[batch_size, seq_len, embed_dim]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,hidden_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(hidden_size, 3072)\n",
    "    self.linear2 = nn.Linear(3072, hidden_size)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  \n",
    "  def forward(self, attention_outputs):\n",
    "    output_l1 = self.linear1(attention_outputs)\n",
    "    activated_outputs = self.gelu(output_l1)\n",
    "    output_l2 = self.linear2(activated_outputs)\n",
    "    output = self.dropout(output_l2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super(TransformerDecoderLayer,self).__init__()\n",
    "    self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "    self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "    self.multi_attention = MultiHeadAttention(hidden_size, num_attention_heads)\n",
    "    self.feedforward = FeedForward(hidden_size)\n",
    "\n",
    "  def forward(self, input_embeddings):\n",
    "     #pre-layer normalization approach\n",
    "     \n",
    "     #Step 1: Applying Layer Normalization to Input Embeddings\n",
    "     normalized_input_embeddings = self.layer_norm1(input_embeddings)\n",
    "     \n",
    "     #Step 2: Applying MultiHeadAttention to Normalized Output\n",
    "     multi_head_attn = self.multi_attention(normalized_input_embeddings)\n",
    "     \n",
    "     #Step 3: Add input embeddings to the Multihead Attention Output\n",
    "     skip_connection_1 = input_embeddings + multi_head_attn\n",
    "\n",
    "     #step 4: Pass the output to another Layer Normalization \n",
    "     layer_norm_2 = self.layer_norm2(skip_connection_1)\n",
    "\n",
    "     #Step 5: Adding skip connection 1 outputs to the output of the FeedForward Network (applied on Step 4)\n",
    "     skip_connection_2 = skip_connection_1 + self.feedforward(layer_norm_2)\n",
    "     #print(f'output of MultiHeadAttention and FeedForward Network is {skip_connection_2.shape}')\n",
    "     return skip_connection_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDecoder(nn.Module):\n",
    "  def __init__(self,num_attention_heads,num_layers, pin_embedding,actions_vocab,action_embedding_dim, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = Embeddings(pin_embedding,actions_vocab,action_embedding_dim, hidden_size)\n",
    "    self.layers = nn.ModuleList([TransformerDecoderLayer(hidden_size, num_attention_heads) for _ in range(num_layers)]) \n",
    "                                \n",
    "  def forward(self, pin_ids, action_ids):\n",
    "    embeddings = self.embedding(pin_ids, action_ids) #in: [batch_size, seq_len] out: [batch_size, seq_len, hidden_size]\n",
    "    for layer in self.layers:\n",
    "      embeddings = layer(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[239, 985, 999, 674, 994]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0]['pin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_decoder = TransferDecoder(num_attention_heads,num_layers, pins_vocab,actions_vocab, hidden_size)\n",
    "embedding_decoder = TransferDecoder(num_attention_heads,num_layers, pin_embedding,actions_vocab,action_embedding_dim, hidden_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#need to pull embeddings for each user and then store it in a dictionary\n",
    "user1_pins = torch.tensor(X_train.iloc[0]['pin']).unsqueeze(0) #unserequeeze is used to add a dimension to the tensor for batch size dimension is [1,5] for batch size 1 and sequence length 5\n",
    "user1_actions = torch.tensor(X_train.iloc[0]['action']).unsqueeze(0) #unserequeeze is used to add a dimension to the tensor for batch size dimension is [1,5] for batch size 1 and sequence length 5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 12])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1_embeddings = embedding_decoder(user1_pins,user1_actions)\n",
    "user1_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for the first batch\n",
    "for batch in train_data_loader:\n",
    "    pin_ids,action_ids, target_pin_ids = batch\n",
    "    #print(pin_ids.shape)\n",
    "    #print(action_ids.shape)\n",
    "    #print(target_pin_ids.shape)\n",
    "    #print(target_pin_ids)\n",
    "    #embedding_decoder(pin_ids,action_ids)\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>708</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-29 23:07:29.687074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>533</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-28 23:07:29.687086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-27 23:07:29.687089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>click</td>\n",
       "      <td>2023-07-26 23:07:29.687091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>833</td>\n",
       "      <td>closeup</td>\n",
       "      <td>2023-07-25 23:07:29.687093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user  pin   action                  timestamp\n",
       "0    1  708    click 2023-07-29 23:07:29.687074\n",
       "1    1  533    click 2023-07-28 23:07:29.687086\n",
       "2    1  298    click 2023-07-27 23:07:29.687089\n",
       "3    1  356    click 2023-07-26 23:07:29.687091\n",
       "4    1  833  closeup 2023-07-25 23:07:29.687093"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user']\n",
    "        pin_ids = torch.tensor(self.df.iloc[idx]['pin_train'])\n",
    "        action_ids = torch.tensor(self.df.iloc[idx]['action_train'])\n",
    "        target_pin_ids = torch.tensor(self.df.iloc[idx]['pin_target'])\n",
    "        target_action_ids = torch.tensor(self.df.iloc[idx]['action_target'])\n",
    "        return {'user': user, 'pin_ids': pin_ids, 'action_ids': action_ids, 'target_pin_ids': target_pin_ids, 'target_action_ids': target_action_ids}\n",
    "       \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[239, 985, 999, 674, 994]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.448844, 2023-07-17 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.453074, 2023-07-17 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[993, 179, 297, 760, 124]</td>\n",
       "      <td>[1, 0, 2, 0, 0]</td>\n",
       "      <td>[2023-07-20 19:04:36.454374, 2023-07-21 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[630, 738, 798, 344, 885]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.455611, 2023-07-17 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[439, 332, 36, 380, 460]</td>\n",
       "      <td>[0, 2, 1, 0, 1]</td>\n",
       "      <td>[2023-07-21 19:04:36.456907, 2023-07-22 19:04:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                        pin           action  \\\n",
       "0     1  [239, 985, 999, 674, 994]  [1, 0, 1, 2, 2]   \n",
       "1     2   [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "2     3  [993, 179, 297, 760, 124]  [1, 0, 2, 0, 0]   \n",
       "3     4  [630, 738, 798, 344, 885]  [2, 0, 2, 1, 2]   \n",
       "4     5   [439, 332, 36, 380, 460]  [0, 2, 1, 0, 1]   \n",
       "\n",
       "                                           timestamp  \n",
       "0  [2023-07-16 19:04:36.448844, 2023-07-17 19:04:...  \n",
       "1  [2023-07-16 19:04:36.453074, 2023-07-17 19:04:...  \n",
       "2  [2023-07-20 19:04:36.454374, 2023-07-21 19:04:...  \n",
       "3  [2023-07-16 19:04:36.455611, 2023-07-17 19:04:...  \n",
       "4  [2023-07-21 19:04:36.456907, 2023-07-22 19:04:...  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = X_train.merge(y_train, on='user', how='inner')\n",
    "merged_test = X_test.merge(y_test, on='user', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.rename(columns={'pin_x': 'pin_train', 'action_x': 'action_train', 'timestamp_x': 'timestamp_train', 'pin_y':'pin_target', 'action_y': 'action_target', 'timestamp_y': 'timestamp_target'}, inplace=True)\n",
    "merged_test.rename(columns={'pin_x': 'pin_train', 'action_x': 'action_train', 'timestamp_x': 'timestamp_train', 'pin_y':'pin_target', 'action_y': 'action_target', 'timestamp_y': 'timestamp_target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin_train</th>\n",
       "      <th>action_train</th>\n",
       "      <th>timestamp_train</th>\n",
       "      <th>pin_target</th>\n",
       "      <th>action_target</th>\n",
       "      <th>timestamp_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[239, 985, 999, 674, 994]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.448844, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 0, 862]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 2023-07-15 19:04:36.448845]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.453074, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 352, 780, 664]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-13 19:04:36.453078, 2023-07-14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[630, 738, 798, 344, 885]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.455611, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 988, 978, 271]</td>\n",
       "      <td>[0, 0, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 2023-07-13 19:04:36.455615, 2023-07-14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>[705, 442, 385, 103, 938]</td>\n",
       "      <td>[0, 1, 2, 2, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.469569, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 26, 510]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-14 19:04:36.469571, 2023-07-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>[558, 986, 672, 125, 354]</td>\n",
       "      <td>[1, 0, 2, 0, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.472624, 2023-07-17 19:04:...</td>\n",
       "      <td>[650, 123, 475, 865, 442]</td>\n",
       "      <td>[2, 0, 2, 1, 0]</td>\n",
       "      <td>[2023-07-11 19:04:36.472630, 2023-07-12 19:04:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                  pin_train     action_train  \\\n",
       "0     1  [239, 985, 999, 674, 994]  [1, 0, 1, 2, 2]   \n",
       "1     2   [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "2     4  [630, 738, 798, 344, 885]  [2, 0, 2, 1, 2]   \n",
       "3    17  [705, 442, 385, 103, 938]  [0, 1, 2, 2, 2]   \n",
       "4    20  [558, 986, 672, 125, 354]  [1, 0, 2, 0, 2]   \n",
       "\n",
       "                                     timestamp_train  \\\n",
       "0  [2023-07-16 19:04:36.448844, 2023-07-17 19:04:...   \n",
       "1  [2023-07-16 19:04:36.453074, 2023-07-17 19:04:...   \n",
       "2  [2023-07-16 19:04:36.455611, 2023-07-17 19:04:...   \n",
       "3  [2023-07-16 19:04:36.469569, 2023-07-17 19:04:...   \n",
       "4  [2023-07-16 19:04:36.472624, 2023-07-17 19:04:...   \n",
       "\n",
       "                  pin_target    action_target  \\\n",
       "0          [0, 0, 0, 0, 862]  [0, 0, 0, 0, 1]   \n",
       "1      [0, 0, 352, 780, 664]  [0, 0, 0, 1, 0]   \n",
       "2      [0, 0, 988, 978, 271]  [0, 0, 0, 2, 1]   \n",
       "3         [0, 0, 0, 26, 510]  [0, 0, 0, 0, 1]   \n",
       "4  [650, 123, 475, 865, 442]  [2, 0, 2, 1, 0]   \n",
       "\n",
       "                                    timestamp_target  \n",
       "0           [0, 0, 0, 0, 2023-07-15 19:04:36.448845]  \n",
       "1  [0, 0, 2023-07-13 19:04:36.453078, 2023-07-14 ...  \n",
       "2  [0, 0, 2023-07-13 19:04:36.455615, 2023-07-14 ...  \n",
       "3  [0, 0, 0, 2023-07-14 19:04:36.469571, 2023-07-...  \n",
       "4  [2023-07-11 19:04:36.472630, 2023-07-12 19:04:...  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecommenderDataset(merged_train)\n",
    "test_dataset = RecommenderDataset(merged_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin_train</th>\n",
       "      <th>action_train</th>\n",
       "      <th>timestamp_train</th>\n",
       "      <th>pin_target</th>\n",
       "      <th>action_target</th>\n",
       "      <th>timestamp_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>[925, 279, 516, 14, 422]</td>\n",
       "      <td>[1, 2, 1, 2, 1]</td>\n",
       "      <td>[2023-07-16 19:04:36.464736, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 345, 98]</td>\n",
       "      <td>[0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-14 19:04:36.464738, 2023-07-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>[289, 872, 511, 798, 891]</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.475362, 2023-07-17 19:04:...</td>\n",
       "      <td>[448, 917, 861, 976, 888]</td>\n",
       "      <td>[0, 0, 1, 1, 2]</td>\n",
       "      <td>[2023-07-10 19:04:36.475369, 2023-07-11 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>[342, 942, 490, 141, 78]</td>\n",
       "      <td>[2, 2, 0, 0, 1]</td>\n",
       "      <td>[2023-07-16 19:04:36.492680, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 210, 914, 945]</td>\n",
       "      <td>[0, 0, 2, 0, 0]</td>\n",
       "      <td>[0, 0, 2023-07-13 19:04:36.492684, 2023-07-14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>[493, 375, 755, 106, 973]</td>\n",
       "      <td>[1, 2, 2, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.499065, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 133, 669]</td>\n",
       "      <td>[0, 0, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-14 19:04:36.499067, 2023-07-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>[251, 553, 760, 127, 332]</td>\n",
       "      <td>[1, 1, 2, 0, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.517386, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 0, 157]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 2023-07-15 19:04:36.517387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70</td>\n",
       "      <td>[742, 217, 562, 615, 211]</td>\n",
       "      <td>[2, 0, 1, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.524893, 2023-07-17 19:04:...</td>\n",
       "      <td>[807, 473, 703, 28, 165]</td>\n",
       "      <td>[2, 0, 1, 2, 1]</td>\n",
       "      <td>[2023-07-10 19:04:36.524900, 2023-07-11 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82</td>\n",
       "      <td>[675, 514, 671, 620, 222]</td>\n",
       "      <td>[1, 0, 0, 2, 1]</td>\n",
       "      <td>[2023-07-16 19:04:36.536892, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 0, 486, 965]</td>\n",
       "      <td>[0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-14 19:04:36.536895, 2023-07-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88</td>\n",
       "      <td>[311, 22, 584, 95, 213]</td>\n",
       "      <td>[2, 1, 1, 0, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.543118, 2023-07-17 19:04:...</td>\n",
       "      <td>[0, 0, 369, 489, 883]</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 2023-07-13 19:04:36.543122, 2023-07-14 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89</td>\n",
       "      <td>[167, 988, 599, 266, 999]</td>\n",
       "      <td>[0, 0, 1, 1, 2]</td>\n",
       "      <td>[2023-07-16 19:04:36.544120, 2023-07-17 19:04:...</td>\n",
       "      <td>[870, 826, 963, 164, 20]</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[2023-07-11 19:04:36.544126, 2023-07-12 19:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94</td>\n",
       "      <td>[16, 661, 831, 808, 529]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[2023-07-16 19:04:36.549058, 2023-07-17 19:04:...</td>\n",
       "      <td>[416, 947, 171, 677, 574]</td>\n",
       "      <td>[2, 2, 1, 2, 2]</td>\n",
       "      <td>[2023-07-10 19:04:36.549065, 2023-07-11 19:04:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                  pin_train     action_train  \\\n",
       "0    12   [925, 279, 516, 14, 422]  [1, 2, 1, 2, 1]   \n",
       "1    22  [289, 872, 511, 798, 891]  [1, 1, 0, 0, 0]   \n",
       "2    39   [342, 942, 490, 141, 78]  [2, 2, 0, 0, 1]   \n",
       "3    45  [493, 375, 755, 106, 973]  [1, 2, 2, 1, 0]   \n",
       "4    63  [251, 553, 760, 127, 332]  [1, 1, 2, 0, 2]   \n",
       "5    70  [742, 217, 562, 615, 211]  [2, 0, 1, 1, 0]   \n",
       "6    82  [675, 514, 671, 620, 222]  [1, 0, 0, 2, 1]   \n",
       "7    88    [311, 22, 584, 95, 213]  [2, 1, 1, 0, 2]   \n",
       "8    89  [167, 988, 599, 266, 999]  [0, 0, 1, 1, 2]   \n",
       "9    94   [16, 661, 831, 808, 529]  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                     timestamp_train  \\\n",
       "0  [2023-07-16 19:04:36.464736, 2023-07-17 19:04:...   \n",
       "1  [2023-07-16 19:04:36.475362, 2023-07-17 19:04:...   \n",
       "2  [2023-07-16 19:04:36.492680, 2023-07-17 19:04:...   \n",
       "3  [2023-07-16 19:04:36.499065, 2023-07-17 19:04:...   \n",
       "4  [2023-07-16 19:04:36.517386, 2023-07-17 19:04:...   \n",
       "5  [2023-07-16 19:04:36.524893, 2023-07-17 19:04:...   \n",
       "6  [2023-07-16 19:04:36.536892, 2023-07-17 19:04:...   \n",
       "7  [2023-07-16 19:04:36.543118, 2023-07-17 19:04:...   \n",
       "8  [2023-07-16 19:04:36.544120, 2023-07-17 19:04:...   \n",
       "9  [2023-07-16 19:04:36.549058, 2023-07-17 19:04:...   \n",
       "\n",
       "                  pin_target    action_target  \\\n",
       "0         [0, 0, 0, 345, 98]  [0, 0, 0, 1, 1]   \n",
       "1  [448, 917, 861, 976, 888]  [0, 0, 1, 1, 2]   \n",
       "2      [0, 0, 210, 914, 945]  [0, 0, 2, 0, 0]   \n",
       "3        [0, 0, 0, 133, 669]  [0, 0, 0, 2, 1]   \n",
       "4          [0, 0, 0, 0, 157]  [0, 0, 0, 0, 1]   \n",
       "5   [807, 473, 703, 28, 165]  [2, 0, 1, 2, 1]   \n",
       "6        [0, 0, 0, 486, 965]  [0, 0, 0, 1, 1]   \n",
       "7      [0, 0, 369, 489, 883]  [0, 0, 1, 1, 1]   \n",
       "8   [870, 826, 963, 164, 20]  [1, 1, 0, 0, 0]   \n",
       "9  [416, 947, 171, 677, 574]  [2, 2, 1, 2, 2]   \n",
       "\n",
       "                                    timestamp_target  \n",
       "0  [0, 0, 0, 2023-07-14 19:04:36.464738, 2023-07-...  \n",
       "1  [2023-07-10 19:04:36.475369, 2023-07-11 19:04:...  \n",
       "2  [0, 0, 2023-07-13 19:04:36.492684, 2023-07-14 ...  \n",
       "3  [0, 0, 0, 2023-07-14 19:04:36.499067, 2023-07-...  \n",
       "4           [0, 0, 0, 0, 2023-07-15 19:04:36.517387]  \n",
       "5  [2023-07-10 19:04:36.524900, 2023-07-11 19:04:...  \n",
       "6  [0, 0, 0, 2023-07-14 19:04:36.536895, 2023-07-...  \n",
       "7  [0, 0, 2023-07-13 19:04:36.543122, 2023-07-14 ...  \n",
       "8  [2023-07-11 19:04:36.544126, 2023-07-12 19:04:...  \n",
       "9  [2023-07-10 19:04:36.549065, 2023-07-11 19:04:...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for the first batch\n",
    "#for batch in train_data_loader:\n",
    "#\n",
    "#    pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "#    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "#    #print(user)\n",
    "#    #print(pin_ids.shape)\n",
    "#    #print(action_ids.shape)\n",
    "#    print(target_pin_ids.shape)\n",
    "#    #print(target_pin_ids)\n",
    "#\n",
    "#    #user_embeddings = embedding_decoder(pin_ids,action_ids)\n",
    "#    #print(user_embeddings.shape)\n",
    "#\n",
    "#    #for each user select the last embedding\n",
    "#    #user_embeddings = user_embeddings[:,-1,:]\n",
    "#    # Create a mask of non-zero elements\n",
    "#    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "#    # Get the indices of non-zero elements\n",
    "#    indices = mask.nonzero() #shape [num_non_zero_elements, 2]\n",
    "#\n",
    "#    #print(mask)\n",
    "#    #print(indices.shape)\n",
    "#    #print(indices)  \n",
    "#    # Get the unique row indices and the counts of non-zero elements in each row\n",
    "#    unique_rows, counts = indices[:, 0].unique(return_counts=True) #shape [num_unique_rows, 1] #this is pulling the unique row indices and the counts of non-zero elements in each row\n",
    "#    print(indices[:,0])\n",
    "#    #print(unique_rows.size(0))\n",
    "#    print(counts)\n",
    "#    print(counts.cumsum(0)[:-1])\n",
    "#    ## Get the starting indices of non-zero elements in each row in the 'indices' tensor\n",
    "#    starts = torch.cat((torch.tensor([0]), counts.cumsum(0)[:-1]))\n",
    "##\n",
    "#    ## Generate a random number for each row in the range of the number of non-zero elements in the row\n",
    "#    rand_num = torch.randint(counts.min(), size=(unique_rows.size(0),)).to(target_pin_ids.device)\n",
    "#    print(rand_num)\n",
    "#    ## Add the starting indices to the random numbers to get the indices of the selected elements in the 'indices' tensor\n",
    "#    selected_indices = starts + rand_num\n",
    "#    print(selected_indices)\n",
    "#    ## Get the indices of the selected elements in the 'input_tensor'\n",
    "#    selected_elements_indices = indices[selected_indices]\n",
    "#    print(selected_elements_indices)\n",
    "##\n",
    "#    ## Get the selected elements\n",
    "#    #selected_elements =target_pin_ids[selected_elements_indices[:, 0], selected_elements_indices[:, 1]]\n",
    "#    #print(selected_elements.unsqueeze(1))\n",
    "#\n",
    "#    #for every user embedding multiply all the pin embeddings\n",
    "#\n",
    "#\n",
    "#\n",
    "#    \n",
    "#    \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in train_data_loader:\n",
    "#\n",
    "#    pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "#    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "#    #print(user)\n",
    "#    #print(pin_ids.shape)\n",
    "#    #print(action_ids.shape)\n",
    "#    #print(target_pin_ids.shape)\n",
    "#    #print(target_pin_ids)\n",
    "#\n",
    "#    #user_embeddings = embedding_decoder(pin_ids,action_ids)\n",
    "#    #print(user_embeddings.shape)\n",
    "#\n",
    "#    #for each user select the last embedding\n",
    "#    #user_embeddings = user_embeddings[:,-1,:]\n",
    "#    # Create a mask of non-zero elements\n",
    "#    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "#    print(target_pin_ids)\n",
    "#    non_zero_elements = target_pin_ids[mask]\n",
    "#    print(non_zero_elements)\n",
    "#    indices = torch.multinomial(mask.float(), 1)\n",
    "#    indices[indices < 0] = 0\n",
    "#    print(indices)\n",
    "#    print(indices.shape)\n",
    "#    #print(indices.squeeze())\n",
    "#    print(indices.squeeze().shape)\n",
    "#    #print(torch.arange(indices.size(0)))\n",
    "#    # Reshape to 8x1\n",
    "#    print(torch.arange(target_pin_ids.shape[0]))\n",
    "#    random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "#    random_numbers = random_numbers.view(-1, 1)\n",
    "#\n",
    "#    #print(random_numbers)   \n",
    "#    #embedded = pin_embeddings(random_numbers)\n",
    "#    #print(embedded.shape)\n",
    "#    #print(embedded.transpose(2,1).shape)\n",
    "#    # Get unique pins seen by user\n",
    "#    #negative sampling for each user\n",
    "#    # Find where the matrix has zeros\n",
    "#    unique_pins = torch.unique(target_pin_ids, dim=1)\n",
    "#\n",
    "#       \n",
    "#\n",
    "#  \n",
    "#\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyModel(nn.Module):\n",
    "#    def __init__(self, pins_vocab, pin_embedding_dim):\n",
    "#        super(MyModel, self).__init__()\n",
    "#        self.pin_embeddings = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "#\n",
    "#    def forward(self, pin_ids):\n",
    "#        return self.pin_embeddings(pin_ids)\n",
    "#\n",
    "## Outside your loop\n",
    "#model = MyModel(pins_vocab, pin_embedding_dim)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#\n",
    "#for batch in train_data_loader:\n",
    "#\n",
    "#    user, pin_ids, action_ids, target_pin_ids, target_action_ids = batch\n",
    "#\n",
    "#    mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "#    indices = torch.multinomial(mask.float(), 1)\n",
    "#    indices[indices < 0] = 0\n",
    "#\n",
    "#    random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "#    random_numbers = random_numbers.view(-1, 1)\n",
    "#\n",
    "#    embedded = model(random_numbers)\n",
    "#\n",
    "#    dot_product = torch.bmm(embedded, embedded.transpose(2,1))\n",
    "#\n",
    "#    sigmoid = torch.sigmoid(dot_product)\n",
    "#    #print(sigmoid.shape)\n",
    "#    #squeeze the sigmoid\n",
    "#    preds = sigmoid.squeeze(1)\n",
    "#    #print(preds)\n",
    "#    print(preds.shape)\n",
    "#\n",
    "#\n",
    "#    negative_pin_embeddings = self.pin_embedding(torch.randint(0,  pin_embedding_shape, (8,)))\n",
    "#        negative_pin_dot_product = torch.bmm(user_embeddings, negative_pin_embeddings.transpose(2,1))\n",
    "#        negative_pin_sigmoid = torch.sigmoid(negative_pin_dot_product)\n",
    "#\n",
    "#        positiv_target = torch.ones_like(sigmoid)\n",
    "#        negative_target = torch.zeros_like(negative_pin_sigmoid)\n",
    "#\n",
    "#\n",
    "#\n",
    "#    target = torch.ones_like(preds) # or torch.zeros_like(preds) if you want the model to output values close to 0\n",
    "#    loss = F.binary_cross_entropy(preds, target)\n",
    "#    print(loss)\n",
    "#\n",
    "#    optimizer.zero_grad() #zero out the gradients\n",
    "#    loss.backward() #calculate the gradients\n",
    "#    optimizer.step() #update the weights\n",
    "#\n",
    "#    break\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pin_embedding = nn.Embedding(pins_vocab, pin_embedding_dim)\n",
    "#pin_embedding_shape = pin_embedding.weight.shape[0]\n",
    "#print(pin_embedding_shape)\n",
    "#\n",
    "##sample negative pin embeddings for batch size 8\n",
    "#negative_indices = torch.randint(0,  pin_embedding_shape, (8,))\n",
    "#print(negative_indices)\n",
    "#negative_pin_embeddings = pin_embedding(negative_indices)\n",
    "#print(negative_pin_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 384])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the pin_embeddings\n",
    "import pickle\n",
    "pin_embeddings = pickle.load(open(\"embeddings.pkl\", \"rb\"))\n",
    "pin_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/c5w_1rg1087580hdbqj2wg0h0000gn/T/ipykernel_85402/260347766.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin_embeddings = torch.tensor(pin_embeddings)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | user_embedding | TransferDecoder | 1.3 M \n",
      "1 | pin_embedding  | Embedding       | 384 K \n",
      "2 | linear         | Linear          | 4.6 K \n",
      "---------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "384 K     Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.807     Total estimated model params size (MB)\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f225e45aaad2417c86d4bb6002bb713f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n",
      "  user_embeddings  torch.Size([8, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([8, 1, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_embeddings  torch.Size([7, 1, 12])\n",
      "  positive_pin_embeddings  torch.Size([7, 1, 12])\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "class RecommenderSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, pin_embedding_dim, action_embedding_dim, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.user_embedding = TransferDecoder(num_attention_heads,num_layers, pin_embeddings,actions_vocab,action_embedding_dim, hidden_size)\n",
    "        #convert the numpy array to a tensor and then to an embedding layer\n",
    "        pin_embeddings = torch.tensor(pin_embeddings)\n",
    "        self.pin_embedding = nn.Embedding.from_pretrained(pin_embeddings)\n",
    "        self.pin_embedding.weight.requires_grad = False\n",
    "        self.linear = nn.Linear(384, hidden_size)\n",
    "        \n",
    "    def forward(self, user ,pin_ids, action_ids,target_pin_ids):\n",
    "        #print(f'user shape {user.shape[0]}')\n",
    "         #positive pins sampled from the target pins\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "    \n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "    \n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids)\n",
    "        #print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "        #extract the last embedding for each user\n",
    "        user_embeddings = user_embeddings[:,-1,:]\n",
    "        #print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "\n",
    "        \n",
    "        user_embeddings = user_embeddings.unsqueeze(1)\n",
    "        print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "       \n",
    "\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers)\n",
    "        #print(f'  positive_pin_embeddings  {positive_pin_embeddings.shape}')\n",
    "        #positive_pin_embeddings = positive_pin_embeddings.squeeze(1)\n",
    "        positive_pin_embedding  = self.linear(positive_pin_embeddings)\n",
    "        print(f'  positive_pin_embeddings  {positive_pin_embedding.shape}')\n",
    "    \n",
    "        dot_product = torch.bmm(user_embeddings, positive_pin_embedding.transpose(2,1))\n",
    "        dot_product = dot_product.squeeze(1)\n",
    "        #print(f'  dot_product  {dot_product.shape}')\n",
    "        sigmoid = torch.sigmoid(dot_product)\n",
    "        #negative pins sampled randomly\n",
    "        \n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0]\n",
    "        #print(pin_embedding_shape)\n",
    "\n",
    "        #sample negative pin embeddings for batch size 8\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],))\n",
    "        #print(negative_indices)\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices)\n",
    "        negative_pin_embeddings = self.linear(negative_pin_embeddings)\n",
    "        \n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1)\n",
    "        #print(f'  negative_pin_embeddings  {negative_pin_embeddings.shape}')\n",
    "        #print(f'  user_embeddings  {user_embeddings.shape}')\n",
    "\n",
    "        negative_pin_dot_product = torch.bmm(user_embeddings, negative_pin_embeddings.transpose(2,1))\n",
    "        negative_pin_dot_product = negative_pin_dot_product.squeeze(1)\n",
    "        negative_pin_sigmoid = torch.sigmoid(negative_pin_dot_product)\n",
    "\n",
    "        total_sigmoid = torch.cat([sigmoid, negative_pin_sigmoid], dim=1)\n",
    "        \n",
    "        targets = torch.ones_like(sigmoid)\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_sigmoid)], dim=1)\n",
    "\n",
    "\n",
    "        preds = total_sigmoid.squeeze(1)\n",
    "        \n",
    "        return preds, targets\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \n",
    "        user = batch['user']\n",
    "        pin_ids = batch['pin_ids']\n",
    "        action_ids = batch['action_ids']\n",
    "        target_pin_ids = batch['target_pin_ids']\n",
    "        preds, targets  = self.forward(user,pin_ids, action_ids, target_pin_ids)\n",
    "        #targets = torch.ones_like(preds)\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, 384, action_embedding_dim, learning_rate=0.0001)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(model, train_data_loader, test_data_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/c5w_1rg1087580hdbqj2wg0h0000gn/T/ipykernel_85402/3111819979.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin_embeddings = torch.tensor(pin_embeddings)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | user_embedding | TransferDecoder | 1.3 M \n",
      "1 | pin_embedding  | Embedding       | 384 K \n",
      "2 | linear         | Linear          | 4.6 K \n",
      "---------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "384 K     Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.807     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5b76b4a61846c18cce22b0e7b59e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b46cd4e4dfa47e8833a16a21225aa95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fcdd06f0b04f81ba683109b4c0c55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27d30ec60a345ad96f4119be825e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd657fa6b036485f987f5a4973451aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001e4a2fc79d4507a91d2fc6145a7b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98b4bb7656425699f39d9678f5f539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2f06b786e471cbc11bb1730766fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c4427c777948c4aece3a69015f7aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e487e8718da47a1a5c8f10543b00efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011fdbf887d64dcc8d71abb67135f9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a02351e7c6b4287b9808ea281facff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "class RecommenderSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, pin_embedding_dim, action_embedding_dim, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.user_embedding = TransferDecoder(num_attention_heads,num_layers, pin_embeddings,actions_vocab,action_embedding_dim, hidden_size)\n",
    "        #convert the numpy array to a tensor and then to an embedding layer\n",
    "        pin_embeddings = torch.tensor(pin_embeddings)\n",
    "        self.pin_embedding = nn.Embedding.from_pretrained(pin_embeddings)\n",
    "        self.pin_embedding.weight.requires_grad = False\n",
    "        self.linear = nn.Linear(384, hidden_size)\n",
    "        \n",
    "    def forward(self, user_embeddings, pin_embeddings):\n",
    "        dot_product = torch.bmm(user_embeddings, pin_embeddings.transpose(2,1))\n",
    "        dot_product = dot_product.squeeze(1)\n",
    "        sigmoid = torch.sigmoid(dot_product)\n",
    "        preds = sigmoid.squeeze(1)\n",
    "        return preds\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \n",
    "        user = batch['user']\n",
    "        pin_ids = batch['pin_ids']\n",
    "        action_ids = batch['action_ids']\n",
    "        target_pin_ids = batch['target_pin_ids']\n",
    "        \n",
    "        \n",
    "        #user embeddings\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids) #in: [batch_size, seq_len] out: mean_pooling [batch_size, seq_len, hidden_size]\n",
    "        user_embeddings = user_embeddings[:,-1,:] #extract the last embedding for each user [batch_size, hidden_size]\n",
    "        user_embeddings = user_embeddings.unsqueeze(1) #add a dimension for the sequence length [batch_size, 1, hidden_size]\n",
    "\n",
    "        #positive pin embeddings\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers) #shape [batch_size, 1, hidden_size]\n",
    "        positive_pin_embedding  = self.linear(positive_pin_embeddings) #shape [batch_size, 1, hidden_size]\n",
    "        positive_preds = self.forward(user_embeddings, positive_pin_embedding) #shape [batch_size, 1]\n",
    "        \n",
    "\n",
    "        #negative pin embeddings\n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0] #shape [num_pins, hidden_size]\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],)) #shape [batch_size,]\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = self.linear(negative_pin_embeddings) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1) #shape [batch_size, 1, hidden_size]\n",
    "        negative_pin_preds = self.forward(user_embeddings, negative_pin_embeddings) #shape [batch_size, 1]\n",
    "        \n",
    "        #concatenate the positive and negative pin preds\n",
    "        preds = torch.cat([positive_preds, negative_pin_preds]) #shape [batch_size*2, 1]\n",
    "        targets = torch.ones_like(positive_preds) #shape [batch_size, 1]\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_preds)]) #shape [batch_size*2, 1]\n",
    "         #shape [batch_size*2, 1]\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user = batch['user']\n",
    "        pin_ids = batch['pin_ids']\n",
    "        action_ids = batch['action_ids']\n",
    "        target_pin_ids = batch['target_pin_ids']\n",
    "\n",
    "        #user embeddings\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids) #in: [batch_size, seq_len] out: mean_pooling [batch_size, seq_len, hidden_size]\n",
    "        user_embeddings = user_embeddings[:,-1,:] #extract the last embedding for each user [batch_size, hidden_size]\n",
    "        user_embeddings = user_embeddings.unsqueeze(1) #add a dimension for the sequence length [batch_size, 1, hidden_size]\n",
    "\n",
    "        #positive pin embeddings\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers) #shape [batch_size, 1, hidden_size]\n",
    "        positive_pin_embedding  = self.linear(positive_pin_embeddings) #shape [batch_size, 1, hidden_size]\n",
    "        positive_preds = self.forward(user_embeddings, positive_pin_embedding) #shape [batch_size, 1]\n",
    "\n",
    "        #negative pin embeddings\n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0] #shape [num_pins, hidden_size]\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],)) #shape [batch_size,]\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = self.linear(negative_pin_embeddings) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1) #shape [batch_size, 1, hidden_size]\n",
    "        negative_pin_preds = self.forward(user_embeddings, negative_pin_embeddings) #shape [batch_size, 1]\n",
    "\n",
    "        #concatenate the positive and negative pin preds\n",
    "        preds = torch.cat([positive_preds, negative_pin_preds]) #shape [batch_size*2, 1]\n",
    "        targets = torch.ones_like(positive_preds) #shape [batch_size, 1]\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_preds)]) #shape [batch_size*2, 1]\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, 384, action_embedding_dim, learning_rate=0.0001)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(model, train_data_loader, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/c5w_1rg1087580hdbqj2wg0h0000gn/T/ipykernel_85402/3111819979.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin_embeddings = torch.tensor(pin_embeddings)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "#load the model\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, 384, action_embedding_dim, learning_rate=0.0001)\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[681, 155, 733,  62, 727]])\n"
     ]
    }
   ],
   "source": [
    "#provide a user id and get the top 10 recommendations\n",
    "#random new user with 5 pins randomly selected\n",
    "\n",
    "#sample a new user\n",
    "new_user = torch.randint(0,  pins_vocab, (1,5))\n",
    "new_user_actions = torch.randint(0,  actions_vocab, (1,5))\n",
    "print(new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000, 12])\n",
      "torch.Size([1, 1000])\n",
      "tensor([[970, 229,  44, 641, 194, 483, 397, 331, 583,  16]])\n"
     ]
    }
   ],
   "source": [
    "#make top 10 recommendations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_embeddings = model.user_embedding(new_user,new_user_actions)\n",
    "    user_embeddings = user_embeddings[:,-1,:]\n",
    "    user_embeddings = user_embeddings.unsqueeze(1)\n",
    "    \n",
    "    #get the pin embeddings\n",
    "    pin_embeddings = model.pin_embedding.weight\n",
    "    pin_embeddings = model.linear(pin_embeddings)\n",
    "    pin_embeddings = pin_embeddings.unsqueeze(0)\n",
    "    print(pin_embeddings.shape)\n",
    "\n",
    "    dot_product = torch.bmm(user_embeddings, pin_embeddings.transpose(2,1))\n",
    "    dot_product = dot_product.squeeze(1)\n",
    "\n",
    "    sigmoid = torch.sigmoid(dot_product)\n",
    "    #print(sigmoid.shape)\n",
    "    #squeeze the sigmoid\n",
    "    preds = sigmoid.squeeze(1)\n",
    "    #print(preds)\n",
    "    print(preds.shape)\n",
    "\n",
    "    #get the top 10 recommendations\n",
    "    top_10 = torch.topk(preds, 10)\n",
    "\n",
    "    #get the indices of the top 10 recommendations\n",
    "    top_10_indices = top_10.indices\n",
    "    print(top_10_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
