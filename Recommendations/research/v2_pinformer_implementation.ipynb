{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling  \n",
    "from transformers import Trainer, TrainingArguments \n",
    "import torch  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer \n",
    "import torch.nn.functional as F \n",
    "import os\n",
    "#os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline,  AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoConfig, EarlyStoppingCallback, TrainerCallback\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create the list of pins, users and actions\n",
    "pins = list(range(0,1000))\n",
    "users = list(range(1,101))\n",
    "actions = ['click', 'closeup', 'save']\n",
    "\n",
    "# Creating empty DataFrame\n",
    "df = pd.DataFrame(columns=['user', 'pin', 'action', 'timestamp'])\n",
    "\n",
    "# Populating the DataFrame\n",
    "for user in users:\n",
    "    num_pins = np.random.randint(3, 21)  # user engages with min 3 pins and max 20 pins\n",
    "    engaged_pins = np.random.choice(pins, num_pins, replace=False)  # engaged pins for this user\n",
    "    engaged_actions = np.random.choice(actions, num_pins)  # actions for this user\n",
    "    timestamps = [datetime.now() - timedelta(days=x) for x in range(num_pins)]  # random timestamps for user engagement\n",
    "    \n",
    "    temp_df = pd.DataFrame({\n",
    "        'user': user,\n",
    "        'pin': engaged_pins,\n",
    "        'action': engaged_actions,\n",
    "        'timestamp': timestamps\n",
    "    })\n",
    "    \n",
    "    df = pd.concat([df, temp_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timestap to date \n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map actions to integers\n",
    "action_to_int = {'click': 0, 'closeup': 1, 'save': 2}\n",
    "df['action'] = df['action'].map(action_to_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split users into train and test users\n",
    "train_users, test_users = train_test_split(users, test_size=0.2)\n",
    "\n",
    "train_df = df[df['user'].isin(train_users)]\n",
    "test_df = df[df['user'].isin(test_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the max and min timestamp in train_df\n",
    "#cutoff_times is 2 weeks prior to the max timestamp\n",
    "max_timestamp = train_df['timestamp'].max()\n",
    "min_timestamp = train_df['timestamp'].min()\n",
    "cut_off_train = max_timestamp - timedelta(days=14)\n",
    "\n",
    "# Create X_train and y_train\n",
    "X_train = train_df[train_df['timestamp'] > cut_off_train]\n",
    "y_train = train_df[(train_df['timestamp'] <= cut_off_train)]\n",
    "\n",
    "# Create X_test and y_test\n",
    "X_test = test_df[test_df['timestamp'] > cut_off_train]\n",
    "y_test = test_df[(test_df['timestamp'] <= cut_off_train)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>862</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>664</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  pin  action   timestamp\n",
       "14    1  862       1  2023-07-17\n",
       "14    2  664       0  2023-07-17\n",
       "15    2  780       1  2023-07-16\n",
       "16    2  352       0  2023-07-15\n",
       "14    4  271       1  2023-07-17"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#if the list is empty add 5 0's or list is less then length 5 add 0 to make it 5 and it if greater then 5 trim it\n",
    "\n",
    "def pad_or_trim(lst):\n",
    "    if len(lst) == 0:\n",
    "        return [0,0,0,0,0]\n",
    "    elif len(lst) < 5:\n",
    "        return [0]*(5-len(lst)) + lst\n",
    "    else:\n",
    "        return lst[:5]\n",
    "\n",
    "X_train = X_train.sort_values(by=['timestamp'])\n",
    "X_train = X_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_train['pin'] = X_train['pin'].apply(lambda x: pad_or_trim(x))\n",
    "X_train['action'] = X_train['action'].apply(pad_or_trim)\n",
    "X_train['timestamp'] = X_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_train = y_train.sort_values(by=['timestamp'])\n",
    "y_train = y_train.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_train['pin'] = y_train['pin'].apply(pad_or_trim)\n",
    "y_train['action'] = y_train['action'].apply(pad_or_trim)\n",
    "y_train['timestamp'] = y_train['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "\n",
    "X_test = X_test.sort_values(by=['timestamp'])   \n",
    "X_test = X_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "X_test['pin'] = X_test['pin'].apply(pad_or_trim)\n",
    "X_test['action'] = X_test['action'].apply(pad_or_trim)\n",
    "X_test['timestamp'] = X_test['timestamp'].apply(pad_or_trim)\n",
    "\n",
    "y_test = y_test.sort_values(by=['timestamp'])\n",
    "y_test = y_test.groupby('user').agg({'pin': list, 'action': list,'timestamp': list}).reset_index()\n",
    "y_test['pin'] = y_test['pin'].apply(pad_or_trim)\n",
    "y_test['action'] = y_test['action'].apply(pad_or_trim)\n",
    "y_test['timestamp'] = y_test['timestamp'].apply(pad_or_trim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                       pin           action  \\\n",
       "1     2  [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "\n",
       "                                           timestamp  \n",
       "1  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...  "
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 352, 780, 664]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-15, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                    pin           action  \\\n",
       "1     2  [0, 0, 352, 780, 664]  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                    timestamp  \n",
       "1  [0, 0, 2023-07-15, 2023-07-16, 2023-07-17]  "
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train['user'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE ALL THE PARAMETERS FOR THE MODEL\n",
    "hidden_size = 12 #hidden size of transformer and the final output coming from the transformer\n",
    "pin_embedding_dim = 12 #dimension of the pin embedding\n",
    "action_embedding_dim = 3 #dimension of the action embedding\n",
    "num_attention_heads = 12 #number of attention heads in transformer that is concatenated together to form the final attention head of dimension 768\n",
    "pins_vocab = 1001 #number of pins\n",
    "actions_vocab = 3 #number of actions\n",
    "num_layers = 12 #number of transformer layers in the model which is a replication of the same transformer layer whose input is the output of the previous layer and the output is the input to the next layer\n",
    "dropout = 0.1\n",
    "max_length = 5 #maximum length of the input sequence\n",
    "batch_size = 8 #batch size for training\n",
    "epochs = 1\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Embeddings(nn.Module):\n",
    "  \"\"\"\n",
    "  Creates a single Dense Embedding for each token --> Token Embedding + Positional Embedding\n",
    "  \"\"\"\n",
    "  def __init__(self, pin_embeddings,actions_vocab, action_embedding_dim, periods, hidden_size):\n",
    "    super().__init__()\n",
    "    self.pin_embedding = nn.Embedding.from_pretrained(pin_embeddings) #batch_size, seq_len, pin_embedding_dim i.e 5, 5, 384\n",
    "    self.linear1 = nn.Linear(384, hidden_size) #linear layer to get the embeddings to the desired hidden size i.e 5, 5, 12\n",
    "    self.pin_embedding.weight.requires_grad = True\n",
    "    \n",
    "    self.periods = periods\n",
    "    self.linear2 = nn.Linear(len(periods)*2 + 1, 50) # Linear layer with output size 50 (arbitrarily chosen)  # 5, 5, 50\n",
    "    self.action_type_embedding = nn.Embedding(actions_vocab, action_embedding_dim) #batch_size, seq_len, action_embedding_dim i.e 5, 5, 3\n",
    "\n",
    "    #concatenate all the pin and action embeddings + time embeddings\n",
    "    self.linear3 = nn.Linear(pin_embedding_dim+action_embedding_dim+50, hidden_size) #linear layer to get the embeddings to the desired hidden size i.e 5, 5, 12\n",
    "\n",
    "    #position embedding\n",
    "    self.position_embedding = nn.Embedding(5, hidden_size) #batch_size, seq_len, hidden_size i.e 5, 5, 12\n",
    "    self.layer_norm = nn.LayerNorm(hidden_size, eps= 1e-12) #batch_size, seq_len, hidden_size i.e 5, 5, 12\n",
    "    self.dropout = nn.Dropout()\n",
    "\n",
    "  def forward(self,pin_ids,action_ids, timestamps):\n",
    "    unix_timestamps = timestamps\n",
    "    unix_timestamps = torch.unsqueeze(unix_timestamps, dim=-1) # 5, 5, 1\n",
    "    features = []\n",
    "    for period in self.periods:\n",
    "        cos_features = torch.cos((2*np.pi*unix_timestamps)/period)\n",
    "        sin_features = torch.sin((2*np.pi*unix_timestamps)/period)\n",
    "        features.extend([cos_features, sin_features])\n",
    "    features = torch.cat(features, dim=-1) # 5, 5, 10\n",
    "    log_feature = torch.log1p(unix_timestamps) #squueze to add another dimension\n",
    "    #torch.Size([1, 5, 8])  + torch.Size([1, 5, 1]) = torch.Size([1, 5, 9])\n",
    "    features = torch.cat([features, log_feature], dim=-1) # 5, 5, 11\n",
    "    # Pass features through linear layer\n",
    "    out = self.linear2(features) # 5, 5, 50\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch_size, seq_len = pin_ids.shape\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long).expand((batch_size, seq_len))\n",
    "    # position_ids = [batch_size, seq_len]\n",
    "    pin_embeddings = self.pin_embedding(pin_ids) #batch_size, seq_len, pin_embedding_dim i.e 5, 5, 384\n",
    "    #print(pin_embeddings.shape)\n",
    "    pin_embeddings = self.linear1(pin_embeddings) #linear layer to get the embeddings to the desired hidden size i.e 5, 5, 12\n",
    "    #print(pin_embeddings.shape)\n",
    "    # pin_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    action_embeddings = self.action_type_embedding(action_ids) #batch_size, seq_len, action_embedding_dim i.e 5, 5, 3\n",
    "    # action_embeddings = [batch_size, seq_len, hidden_size]\n",
    "    position_embeddings = self.position_embedding(position_ids) #batch_size, seq_len, hidden_size i.e 5, 5, 12\n",
    "    #print(position_embeddings.shape)\n",
    "    \n",
    "    \n",
    "    #concatenate all the pin and action embeddings\n",
    "    embeddings = torch.cat([pin_embeddings, action_embeddings , out], dim=-1) #batch_size, seq_len, hidden_size i.e 5, 5, 3+12+50 = 65\n",
    "    #apply a linear layer to get the embeddings to the desired hidden size\n",
    "    \n",
    "    embeddings = self.linear3(embeddings) #linear layer to get the embeddings to the desired hidden size i.e 5, 5, 12\n",
    "    \n",
    "    #print(embeddings.shape)\n",
    "\n",
    "    #add the position embeddings\n",
    "    embeddings = embeddings + position_embeddings\n",
    "\n",
    "    #embeddings = pin_embeddings + action_embeddings + position_embeddings\n",
    "    # embeddings = [batch_size, seq_len, hidden_size]\n",
    "    embeddings = self.layer_norm(embeddings)\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sample pinids of btch size 1, seq_len 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pin Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pin_embeddings_pretrained = pickle.load(open('embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 12])"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pin_ids = torch.tensor([[1,2,3,4,5]])\n",
    "print(pin_ids.shape)\n",
    "action_ids = torch.tensor([[1,2,0,2,1]])\n",
    "print(action_ids.shape)\n",
    "timestamps = torch.tensor([[1,2,3,4,5]])\n",
    "print(timestamps.shape)\n",
    "periods = [1, 7, 30, 365]\n",
    "embeddings = Embeddings(pin_embeddings_pretrained,actions_vocab, action_embedding_dim, periods, hidden_size)\n",
    "embeddings.forward(pin_ids,action_ids, timestamps).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, embed_dim, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim #dimension of one head \n",
    "    #infeatures=embed_dim\n",
    "    #outfeatures=head_dim\n",
    "    self.q = nn.Linear(embed_dim, head_dim)\n",
    "    self.k = nn.Linear(embed_dim, head_dim)\n",
    "    self.v = nn.Linear(embed_dim, head_dim)\n",
    "    \n",
    "  \n",
    "  def causal_mask(self,batch_size,size, dtype):  \n",
    "    mask = torch.tril(torch.ones(size,size)).unsqueeze(0)\n",
    "    return mask\n",
    "    \n",
    "  \n",
    "      \n",
    "  def scaled_dot_product_attention(self,query, key, value):\n",
    "    dim_k = query.size(dim=-1)  \n",
    "    #print(dim_k)    \n",
    "    #print(f'Dimension of the q,k,v Matrix [Batch_size, seq_len, Head_dim] of One Head {dim_k}')\n",
    "    scores = torch.bmm(query,key.transpose(1,2))/ sqrt(dim_k)  #[(1,5,768)*(1,768,5)]/sqrt(768) >>> [batch_size,5,5] \n",
    "    \n",
    "    mask = self.causal_mask(scores.size(0),scores.size(1),dtype=torch.int32)\n",
    "    #print(mask)\n",
    "    scores = scores.masked_fill(mask==0, float(0)) \n",
    "    weights = F.softmax(scores, dim=-1) #[batch_size,5,5]\n",
    "    #print(weights)\n",
    "    #print(f'Softmax for each column across one row {weights}')\n",
    "    weights_dot_values = torch.bmm(weights,value)  #[batch_size,5,5]*[batch_size,5,64] >>> [batch_size,5,64]\n",
    "    #print(f'Last Step is to multiply weights and values {weights_dot_values.shape}')\n",
    "    return weights_dot_values \n",
    "\n",
    "  def forward(self, hidden_state):\n",
    "    #head_state = [batch_size, seq_len, embed_dim]\n",
    "    #print(f'Input Embedding for Each Token with X Matrix {hidden_state.size()}')\n",
    "    #q = X*W_q\n",
    "    q = self.q(hidden_state) #q = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Query Matrix W_q {q.size()}')\n",
    "    k = self.k(hidden_state) #k = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Key Matrix W_k {k.size()}')\n",
    "    v = self.k(hidden_state) #v = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the Value Matrix W_k {v.size()}')\n",
    "    #print('-----------------Calculating Self Attention--------------------')\n",
    "    attn_outputs = self.scaled_dot_product_attention(q,k,v) #attn_outputs = [batch_size, seq_len, head_dim]\n",
    "    #print(f'Shape of the attention Output with one Head and Head Dimension {self.head_dim} is {attn_outputs.size()}')\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super().__init__()\n",
    "    embed_dim = hidden_size\n",
    "    num_heads = num_attention_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "    self.heads = [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)] #initializing all the heads\n",
    "    self.w_0 = nn.Linear(embed_dim,embed_dim) #the purpose of this linear layer is to concatenate all the heads together to form the final output of the multihead attention\n",
    "\n",
    "  def forward(self,hidden_state):\n",
    "    '''\n",
    "    hidden_state: Input Embedding with dimensions [batch_size, seq_len, embedding_dimension]\n",
    "    '''\n",
    "    attention_outputs = [head(hidden_state) for head in self.heads] #Calculating Self-Attention on each head\n",
    "    contcat_attn_outputs_allheads = torch.cat(attention_outputs, dim=-1) #[batch_size,seq_len, embed_dim]\n",
    "    Z =   self.w_0(contcat_attn_outputs_allheads) #[batch_size, seq_len, embed_dim]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,hidden_size):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(hidden_size, 3072)\n",
    "    self.linear2 = nn.Linear(3072, hidden_size)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "  \n",
    "  def forward(self, attention_outputs):\n",
    "    output_l1 = self.linear1(attention_outputs)\n",
    "    activated_outputs = self.gelu(output_l1)\n",
    "    output_l2 = self.linear2(activated_outputs)\n",
    "    output = self.dropout(output_l2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Connections + MHAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self, hidden_size, num_attention_heads):\n",
    "    super(TransformerDecoderLayer,self).__init__()\n",
    "    self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "    self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "    self.multi_attention = MultiHeadAttention(hidden_size, num_attention_heads)\n",
    "    self.feedforward = FeedForward(hidden_size)\n",
    "\n",
    "  def forward(self, input_embeddings):\n",
    "     #pre-layer normalization approach\n",
    "     \n",
    "     #Step 1: Applying Layer Normalization to Input Embeddings\n",
    "     normalized_input_embeddings = self.layer_norm1(input_embeddings)\n",
    "     \n",
    "     #Step 2: Applying MultiHeadAttention to Normalized Output\n",
    "     multi_head_attn = self.multi_attention(normalized_input_embeddings)\n",
    "     \n",
    "     #Step 3: Add input embeddings to the Multihead Attention Output\n",
    "     skip_connection_1 = input_embeddings + multi_head_attn\n",
    "\n",
    "     #step 4: Pass the output to another Layer Normalization \n",
    "     layer_norm_2 = self.layer_norm2(skip_connection_1)\n",
    "\n",
    "     #Step 5: Adding skip connection 1 outputs to the output of the FeedForward Network (applied on Step 4)\n",
    "     skip_connection_2 = skip_connection_1 + self.feedforward(layer_norm_2)\n",
    "     #print(f'output of MultiHeadAttention and FeedForward Network is {skip_connection_2.shape}')\n",
    "     return skip_connection_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder ( Putting it all together )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDecoder(nn.Module):\n",
    "  def __init__(self,num_attention_heads,num_layers, pin_embedding,actions_vocab,action_embedding_dim, periods, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = Embeddings(pin_embedding,actions_vocab,action_embedding_dim, periods, hidden_size)\n",
    "    self.layers = nn.ModuleList([TransformerDecoderLayer(hidden_size, num_attention_heads) for _ in range(num_layers)]) \n",
    "                                \n",
    "  def forward(self, pin_ids, action_ids, timestamps):\n",
    "    embeddings = self.embedding(pin_ids, action_ids, timestamps) #in: [batch_size, seq_len] out: [batch_size, seq_len, hidden_size]\n",
    "    for layer in self.layers:\n",
    "      embeddings = layer(embeddings)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import date \n",
    "from datetime import date\n",
    "class RecommenderDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user']\n",
    "        pin_ids = torch.tensor(self.df.iloc[idx]['pin_train'])\n",
    "        action_ids = torch.tensor(self.df.iloc[idx]['action_train'])\n",
    "        target_pin_ids = torch.tensor(self.df.iloc[idx]['pin_target'])\n",
    "        target_action_ids = torch.tensor(self.df.iloc[idx]['action_target'])\n",
    "        # Convert datetime.date to UNIX timestamps (in days)\n",
    "        time_stamps = [(ts - date(1970,1,1)).total_seconds() / 86400.0 for ts in self.df.iloc[idx]['timestamp_train']]\n",
    "        time_stamps = torch.tensor(time_stamps)\n",
    "        #timestamp shpild be batch_size, seq_len i.e 5, 5\n",
    "        #time_stamps = torch.unsqueeze(time_stamps, dim=-1)\n",
    "        return {'user': user, 'pin_ids': pin_ids, 'action_ids': action_ids, 'target_pin_ids': target_pin_ids, 'target_action_ids': target_action_ids, 'time_stamps': time_stamps}\n",
    "       \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin</th>\n",
       "      <th>action</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[239, 985, 999, 674, 994]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[993, 179, 297, 760, 124]</td>\n",
       "      <td>[1, 0, 2, 0, 0]</td>\n",
       "      <td>[2023-07-22, 2023-07-23, 2023-07-24, 2023-07-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[630, 738, 798, 344, 885]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[439, 332, 36, 380, 460]</td>\n",
       "      <td>[0, 2, 1, 0, 1]</td>\n",
       "      <td>[2023-07-23, 2023-07-24, 2023-07-25, 2023-07-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                        pin           action  \\\n",
       "0     1  [239, 985, 999, 674, 994]  [1, 0, 1, 2, 2]   \n",
       "1     2   [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "2     3  [993, 179, 297, 760, 124]  [1, 0, 2, 0, 0]   \n",
       "3     4  [630, 738, 798, 344, 885]  [2, 0, 2, 1, 2]   \n",
       "4     5   [439, 332, 36, 380, 460]  [0, 2, 1, 0, 1]   \n",
       "\n",
       "                                           timestamp  \n",
       "0  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...  \n",
       "1  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...  \n",
       "2  [2023-07-22, 2023-07-23, 2023-07-24, 2023-07-2...  \n",
       "3  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...  \n",
       "4  [2023-07-23, 2023-07-24, 2023-07-25, 2023-07-2...  "
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train = X_train.merge(y_train, on='user', how='inner')\n",
    "merged_test = X_test.merge(y_test, on='user', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train.rename(columns={'pin_x': 'pin_train', 'action_x': 'action_train', 'timestamp_x': 'timestamp_train', 'pin_y':'pin_target', 'action_y': 'action_target', 'timestamp_y': 'timestamp_target'}, inplace=True)\n",
    "merged_test.rename(columns={'pin_x': 'pin_train', 'action_x': 'action_train', 'timestamp_x': 'timestamp_train', 'pin_y':'pin_target', 'action_y': 'action_target', 'timestamp_y': 'timestamp_target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin_train</th>\n",
       "      <th>action_train</th>\n",
       "      <th>timestamp_train</th>\n",
       "      <th>pin_target</th>\n",
       "      <th>action_target</th>\n",
       "      <th>timestamp_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[239, 985, 999, 674, 994]</td>\n",
       "      <td>[1, 0, 1, 2, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 0, 862]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[531, 163, 687, 778, 18]</td>\n",
       "      <td>[2, 2, 0, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 352, 780, 664]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 2023-07-15, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>[630, 738, 798, 344, 885]</td>\n",
       "      <td>[2, 0, 2, 1, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 988, 978, 271]</td>\n",
       "      <td>[0, 0, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 2023-07-15, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>[705, 442, 385, 103, 938]</td>\n",
       "      <td>[0, 1, 2, 2, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 26, 510]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>[558, 986, 672, 125, 354]</td>\n",
       "      <td>[1, 0, 2, 0, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[650, 123, 475, 865, 442]</td>\n",
       "      <td>[2, 0, 2, 1, 0]</td>\n",
       "      <td>[2023-07-13, 2023-07-14, 2023-07-15, 2023-07-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                  pin_train     action_train  \\\n",
       "0     1  [239, 985, 999, 674, 994]  [1, 0, 1, 2, 2]   \n",
       "1     2   [531, 163, 687, 778, 18]  [2, 2, 0, 1, 0]   \n",
       "2     4  [630, 738, 798, 344, 885]  [2, 0, 2, 1, 2]   \n",
       "3    17  [705, 442, 385, 103, 938]  [0, 1, 2, 2, 2]   \n",
       "4    20  [558, 986, 672, 125, 354]  [1, 0, 2, 0, 2]   \n",
       "\n",
       "                                     timestamp_train  \\\n",
       "0  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "1  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "2  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "3  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "4  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "\n",
       "                  pin_target    action_target  \\\n",
       "0          [0, 0, 0, 0, 862]  [0, 0, 0, 0, 1]   \n",
       "1      [0, 0, 352, 780, 664]  [0, 0, 0, 1, 0]   \n",
       "2      [0, 0, 988, 978, 271]  [0, 0, 0, 2, 1]   \n",
       "3         [0, 0, 0, 26, 510]  [0, 0, 0, 0, 1]   \n",
       "4  [650, 123, 475, 865, 442]  [2, 0, 2, 1, 0]   \n",
       "\n",
       "                                    timestamp_target  \n",
       "0                           [0, 0, 0, 0, 2023-07-17]  \n",
       "1         [0, 0, 2023-07-15, 2023-07-16, 2023-07-17]  \n",
       "2         [0, 0, 2023-07-15, 2023-07-16, 2023-07-17]  \n",
       "3                  [0, 0, 0, 2023-07-16, 2023-07-17]  \n",
       "4  [2023-07-13, 2023-07-14, 2023-07-15, 2023-07-1...  "
      ]
     },
     "execution_count": 830,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecommenderDataset(merged_train)\n",
    "test_dataset = RecommenderDataset(merged_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "#sample = next(iter(train_data_loader))\n",
    "for batch in train_data_loader:\n",
    "  print(batch['pin_ids'].shape)\n",
    "  print(batch['action_ids'].shape)\n",
    "  print(batch['target_pin_ids'].shape)\n",
    "  print(batch['target_action_ids'].shape)\n",
    "  print(batch['time_stamps'].shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>pin_train</th>\n",
       "      <th>action_train</th>\n",
       "      <th>timestamp_train</th>\n",
       "      <th>pin_target</th>\n",
       "      <th>action_target</th>\n",
       "      <th>timestamp_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>[925, 279, 516, 14, 422]</td>\n",
       "      <td>[1, 2, 1, 2, 1]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 345, 98]</td>\n",
       "      <td>[0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>[289, 872, 511, 798, 891]</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[448, 917, 861, 976, 888]</td>\n",
       "      <td>[0, 0, 1, 1, 2]</td>\n",
       "      <td>[2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>[342, 942, 490, 141, 78]</td>\n",
       "      <td>[2, 2, 0, 0, 1]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 210, 914, 945]</td>\n",
       "      <td>[0, 0, 2, 0, 0]</td>\n",
       "      <td>[0, 0, 2023-07-15, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>[493, 375, 755, 106, 973]</td>\n",
       "      <td>[1, 2, 2, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 133, 669]</td>\n",
       "      <td>[0, 0, 0, 2, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>[251, 553, 760, 127, 332]</td>\n",
       "      <td>[1, 1, 2, 0, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 0, 157]</td>\n",
       "      <td>[0, 0, 0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70</td>\n",
       "      <td>[742, 217, 562, 615, 211]</td>\n",
       "      <td>[2, 0, 1, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[807, 473, 703, 28, 165]</td>\n",
       "      <td>[2, 0, 1, 2, 1]</td>\n",
       "      <td>[2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82</td>\n",
       "      <td>[675, 514, 671, 620, 222]</td>\n",
       "      <td>[1, 0, 0, 2, 1]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 0, 486, 965]</td>\n",
       "      <td>[0, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88</td>\n",
       "      <td>[311, 22, 584, 95, 213]</td>\n",
       "      <td>[2, 1, 1, 0, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[0, 0, 369, 489, 883]</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "      <td>[0, 0, 2023-07-15, 2023-07-16, 2023-07-17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89</td>\n",
       "      <td>[167, 988, 599, 266, 999]</td>\n",
       "      <td>[0, 0, 1, 1, 2]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[870, 826, 963, 164, 20]</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[2023-07-13, 2023-07-14, 2023-07-15, 2023-07-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>94</td>\n",
       "      <td>[16, 661, 831, 808, 529]</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...</td>\n",
       "      <td>[416, 947, 171, 677, 574]</td>\n",
       "      <td>[2, 2, 1, 2, 2]</td>\n",
       "      <td>[2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user                  pin_train     action_train  \\\n",
       "0    12   [925, 279, 516, 14, 422]  [1, 2, 1, 2, 1]   \n",
       "1    22  [289, 872, 511, 798, 891]  [1, 1, 0, 0, 0]   \n",
       "2    39   [342, 942, 490, 141, 78]  [2, 2, 0, 0, 1]   \n",
       "3    45  [493, 375, 755, 106, 973]  [1, 2, 2, 1, 0]   \n",
       "4    63  [251, 553, 760, 127, 332]  [1, 1, 2, 0, 2]   \n",
       "5    70  [742, 217, 562, 615, 211]  [2, 0, 1, 1, 0]   \n",
       "6    82  [675, 514, 671, 620, 222]  [1, 0, 0, 2, 1]   \n",
       "7    88    [311, 22, 584, 95, 213]  [2, 1, 1, 0, 2]   \n",
       "8    89  [167, 988, 599, 266, 999]  [0, 0, 1, 1, 2]   \n",
       "9    94   [16, 661, 831, 808, 529]  [0, 0, 0, 1, 0]   \n",
       "\n",
       "                                     timestamp_train  \\\n",
       "0  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "1  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "2  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "3  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "4  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "5  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "6  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "7  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "8  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "9  [2023-07-18, 2023-07-19, 2023-07-20, 2023-07-2...   \n",
       "\n",
       "                  pin_target    action_target  \\\n",
       "0         [0, 0, 0, 345, 98]  [0, 0, 0, 1, 1]   \n",
       "1  [448, 917, 861, 976, 888]  [0, 0, 1, 1, 2]   \n",
       "2      [0, 0, 210, 914, 945]  [0, 0, 2, 0, 0]   \n",
       "3        [0, 0, 0, 133, 669]  [0, 0, 0, 2, 1]   \n",
       "4          [0, 0, 0, 0, 157]  [0, 0, 0, 0, 1]   \n",
       "5   [807, 473, 703, 28, 165]  [2, 0, 1, 2, 1]   \n",
       "6        [0, 0, 0, 486, 965]  [0, 0, 0, 1, 1]   \n",
       "7      [0, 0, 369, 489, 883]  [0, 0, 1, 1, 1]   \n",
       "8   [870, 826, 963, 164, 20]  [1, 1, 0, 0, 0]   \n",
       "9  [416, 947, 171, 677, 574]  [2, 2, 1, 2, 2]   \n",
       "\n",
       "                                    timestamp_target  \n",
       "0                  [0, 0, 0, 2023-07-16, 2023-07-17]  \n",
       "1  [2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...  \n",
       "2         [0, 0, 2023-07-15, 2023-07-16, 2023-07-17]  \n",
       "3                  [0, 0, 0, 2023-07-16, 2023-07-17]  \n",
       "4                           [0, 0, 0, 0, 2023-07-17]  \n",
       "5  [2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...  \n",
       "6                  [0, 0, 0, 2023-07-16, 2023-07-17]  \n",
       "7         [0, 0, 2023-07-15, 2023-07-16, 2023-07-17]  \n",
       "8  [2023-07-13, 2023-07-14, 2023-07-15, 2023-07-1...  \n",
       "9  [2023-07-12, 2023-07-13, 2023-07-14, 2023-07-1...  "
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/c5w_1rg1087580hdbqj2wg0h0000gn/T/ipykernel_36284/925709018.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin_embeddings = torch.tensor(pin_embeddings)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type            | Params\n",
      "---------------------------------------------------\n",
      "0 | user_embedding | TransferDecoder | 1.3 M \n",
      "1 | pin_embedding  | Embedding       | 384 K \n",
      "2 | linear         | Linear          | 4.6 K \n",
      "---------------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.812     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22db8ad59e0476a87d4ba357b8cdfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19000f471a6749b7b940ad3a663827a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca637e0968f43c0a1064b277cd23631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "class RecommenderSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_attention_heads,num_layers, pin_embeddings ,actions_vocab, hidden_size, action_embedding_dim,periods, learning_rate=0.0001):\n",
    "        super().__init__()\n",
    "        self.user_embedding = TransferDecoder(num_attention_heads,num_layers, pin_embeddings,actions_vocab,action_embedding_dim, periods, hidden_size)\n",
    "        #convert the numpy array to a tensor and then to an embedding layer\n",
    "        pin_embeddings = torch.tensor(pin_embeddings)\n",
    "        self.pin_embedding = nn.Embedding.from_pretrained(pin_embeddings)\n",
    "        self.pin_embedding.weight.requires_grad = True\n",
    "        self.linear = nn.Linear(384, hidden_size)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, user_embeddings, pin_embeddings):\n",
    "        dot_product = torch.bmm(user_embeddings, pin_embeddings.transpose(2,1))\n",
    "        dot_product = dot_product.squeeze(1)\n",
    "        sigmoid = torch.sigmoid(dot_product)\n",
    "        preds = sigmoid.squeeze(1)\n",
    "        return preds\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        \n",
    "        user = batch['user']\n",
    "        pin_ids = batch['pin_ids']\n",
    "        action_ids = batch['action_ids']\n",
    "        target_pin_ids = batch['target_pin_ids']\n",
    "        timestamps = batch['time_stamps']\n",
    "        \n",
    "        \n",
    "        #user embeddings\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids, timestamps) #in: [batch_size, seq_len] out: mean_pooling [batch_size, seq_len, hidden_size]\n",
    "        user_embeddings = user_embeddings[:,-1,:] #extract the last embedding for each user [batch_size, hidden_size]\n",
    "        user_embeddings = user_embeddings.unsqueeze(1) #add a dimension for the sequence length [batch_size, 1, hidden_size]\n",
    "\n",
    "        #positive pin embeddings\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers) #shape [batch_size, 1, hidden_size]\n",
    "        positive_pin_embedding  = self.linear(positive_pin_embeddings) #shape [batch_size, 1, hidden_size]\n",
    "        positive_preds = self.forward(user_embeddings, positive_pin_embedding) #shape [batch_size, 1]\n",
    "        \n",
    "\n",
    "        #negative pin embeddings\n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0] #shape [num_pins, hidden_size]\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],)) #shape [batch_size,]\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = self.linear(negative_pin_embeddings) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1) #shape [batch_size, 1, hidden_size]\n",
    "        negative_pin_preds = self.forward(user_embeddings, negative_pin_embeddings) #shape [batch_size, 1]\n",
    "        \n",
    "        #concatenate the positive and negative pin preds\n",
    "        preds = torch.cat([positive_preds, negative_pin_preds]) #shape [batch_size*2, 1]\n",
    "        targets = torch.ones_like(positive_preds) #shape [batch_size, 1]\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_preds)]) #shape [batch_size*2, 1]\n",
    "         #shape [batch_size*2, 1]\n",
    "\n",
    "        #calculate the loss\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        user = batch['user']\n",
    "        pin_ids = batch['pin_ids']\n",
    "        action_ids = batch['action_ids']\n",
    "        target_pin_ids = batch['target_pin_ids']\n",
    "        timestamps = batch['time_stamps']\n",
    "\n",
    "        #user embeddings\n",
    "        mask = target_pin_ids != 0 #shape [batch_size, seq_len]\n",
    "        indices = torch.multinomial(mask.float(), 1)\n",
    "        indices[indices < 0] = 0\n",
    "        random_numbers = target_pin_ids[torch.arange(target_pin_ids.shape[0]), indices.squeeze()]\n",
    "        random_numbers = random_numbers.view(-1, 1)\n",
    "        user_embeddings = self.user_embedding(pin_ids,action_ids, timestamps) #in: [batch_size, seq_len] out: mean_pooling [batch_size, seq_len, hidden_size]\n",
    "        user_embeddings = user_embeddings[:,-1,:] #extract the last embedding for each user [batch_size, hidden_size]\n",
    "        user_embeddings = user_embeddings.unsqueeze(1) #add a dimension for the sequence length [batch_size, 1, hidden_size]\n",
    "\n",
    "        #positive pin embeddings\n",
    "        positive_pin_embeddings = self.pin_embedding(random_numbers) #shape [batch_size, 1, hidden_size]\n",
    "        positive_pin_embedding  = self.linear(positive_pin_embeddings) #shape [batch_size, 1, hidden_size]\n",
    "        positive_preds = self.forward(user_embeddings, positive_pin_embedding) #shape [batch_size, 1]\n",
    "\n",
    "        #negative pin embeddings\n",
    "        pin_embedding_shape = self.pin_embedding.weight.shape[0] #shape [num_pins, hidden_size]\n",
    "        negative_indices = torch.randint(0,  pin_embedding_shape, (user.shape[0],)) #shape [batch_size,]\n",
    "        negative_pin_embeddings = self.pin_embedding(negative_indices) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = self.linear(negative_pin_embeddings) #shape [batch_size, hidden_size]\n",
    "        negative_pin_embeddings = negative_pin_embeddings.unsqueeze(1) #shape [batch_size, 1, hidden_size]\n",
    "        negative_pin_preds = self.forward(user_embeddings, negative_pin_embeddings) #shape [batch_size, 1]\n",
    "\n",
    "        #concatenate the positive and negative pin preds\n",
    "        preds = torch.cat([positive_preds, negative_pin_preds]) #shape [batch_size*2, 1]\n",
    "        targets = torch.ones_like(positive_preds) #shape [batch_size, 1]\n",
    "        targets = torch.cat([targets, torch.zeros_like(negative_pin_preds)]) #shape [batch_size*2, 1]\n",
    "\n",
    "        \n",
    "\n",
    "        #calculate the loss\n",
    "        loss = F.binary_cross_entropy(preds, targets)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "#load the model\n",
    "periods =  [1, 7, 30, 90, 365]\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings_pretrained ,actions_vocab, hidden_size, action_embedding_dim,periods, learning_rate=0.0001)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=1, log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(model, train_data_loader, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 1,\n",
       " 'pin_ids': tensor([239, 985, 999, 674, 994]),\n",
       " 'action_ids': tensor([1, 0, 1, 2, 2]),\n",
       " 'target_pin_ids': tensor([  0,   0,   0,   0, 862]),\n",
       " 'target_action_ids': tensor([0, 0, 0, 0, 1]),\n",
       " 'time_stamps': tensor([19556., 19557., 19558., 19559., 19560.])}"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/c5w_1rg1087580hdbqj2wg0h0000gn/T/ipykernel_36284/925709018.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pin_embeddings = torch.tensor(pin_embeddings)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b1a6ed0c43440bad70685cfbaa1d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: user_embeddings,pin_embeddings,label_ids,label.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[837], line 29\u001b[0m\n\u001b[1;32m     11\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     12\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,          \u001b[39m# output directory\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,              \u001b[39m# total number of training epochs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[39m=\u001b[39mmodel,                         \u001b[39m# the instantiated  Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     args\u001b[39m=\u001b[39margs,                           \u001b[39m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,         \u001b[39m# training dataset\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtest_dataset            \u001b[39m# evaluation dataset\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2752\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2735\u001b[0m \u001b[39mPerform a training step on a batch of inputs.\u001b[39;00m\n\u001b[1;32m   2736\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2749\u001b[0m \u001b[39m    `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m-> 2752\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_inputs(inputs)\n\u001b[1;32m   2754\u001b[0m \u001b[39mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2755\u001b[0m     loss_mb \u001b[39m=\u001b[39m smp_forward_backward(model, inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2697\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_input(inputs)\n\u001b[1;32m   2698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2699\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2700\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe batch received was empty, your model won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be able to train on it. Double-check that your \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2701\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtraining dataset contains keys expected by the model: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signature_columns)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2702\u001b[0m     )\n\u001b[1;32m   2703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_past \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2704\u001b[0m     inputs[\u001b[39m\"\u001b[39m\u001b[39mmems\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_past\n",
      "\u001b[0;31mValueError\u001b[0m: The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: user_embeddings,pin_embeddings,label_ids,label."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollator\n",
    "\n",
    "train_dataset = RecommenderDataset(merged_train)\n",
    "test_dataset = RecommenderDataset(merged_test)\n",
    "data_collator =  DataCollator()\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings_pretrained ,actions_vocab, hidden_size, action_embedding_dim,periods, learning_rate=0.0001)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=args,                           # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(all_preds, true_positives, k):\n",
    "        top_k = all_preds.topk(k, dim=-1) # Get top k predictions\n",
    "        true_positives = true_positives.unsqueeze(1).expand_as(top_k) # Expand true positives to match size of top_k\n",
    "        recalls = (top_k == true_positives).sum(dim=-1).float() / true_positives.size(-1) # Compute recalls\n",
    "        return recalls.mean() # Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sample preds as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "#load the model\n",
    "periods =  [1, 7, 30, 90, 365]\n",
    "model = RecommenderSystem(num_attention_heads,num_layers, pin_embeddings_pretrained ,actions_vocab, hidden_size, action_embedding_dim,periods, learning_rate=0.0001)\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide a user id and get the top 10 recommendations\n",
    "#random new user with 5 pins randomly selected\n",
    "\n",
    "#sample a new user\n",
    "new_user = torch.randint(0,  pins_vocab, (1,5))\n",
    "new_user_actions = torch.randint(0,  actions_vocab, (1,5))\n",
    "new_user_timestamps = torch.randint(0,  100, (1,5))\n",
    "print(new_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TOP 10 Recommendations for the new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make top 10 recommendations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    user_embeddings = model.user_embedding(new_user,new_user_actions, new_user_timestamps)\n",
    "    user_embeddings = user_embeddings[:,-1,:]\n",
    "    user_embeddings = user_embeddings.unsqueeze(1)\n",
    "    print(user_embeddings.shape)\n",
    "    \n",
    "    #get the pin embeddings\n",
    "    pin_embeddings = model.pin_embedding.weight\n",
    "    pin_embeddings = model.linear(pin_embeddings)\n",
    "    pin_embeddings = pin_embeddings.unsqueeze(0)\n",
    "    print(pin_embeddings.shape)\n",
    "\n",
    "    dot_product = torch.bmm(user_embeddings, pin_embeddings.transpose(2,1))\n",
    "    dot_product = dot_product.squeeze(1)\n",
    "\n",
    "    sigmoid = torch.sigmoid(dot_product)\n",
    "    #print(sigmoid.shape)\n",
    "    #squeeze the sigmoid\n",
    "    preds = sigmoid.squeeze(1)\n",
    "    #print(preds)\n",
    "    print(preds.shape)\n",
    "\n",
    "    #get the top 10 recommendations\n",
    "    top_10 = torch.topk(preds, 10)\n",
    "\n",
    "    #get the indices of the top 10 recommendations\n",
    "    top_10_indices = top_10.indices\n",
    "    print(top_10_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"topic_facts_3.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conver the new_user to a list of indexes\n",
    "new_user = new_user.tolist()[0]\n",
    "data.iloc[new_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the top 10 pins for the new user\n",
    "data.iloc[top_10_indices.tolist()[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, periods):\n",
    "        super(Time2Vec, self).__init__()\n",
    "        \n",
    "        self.periods = periods\n",
    "        self.linear = nn.Linear(len(periods)*2 + 1, 50) # Linear layer with output size 50 (arbitrarily chosen)\n",
    "\n",
    "    def forward(self, timestamps):\n",
    "        # Convert 'YYYY-MM-DD' format to UNIX timestamps\n",
    "        unix_timestamps = torch.tensor([[(datetime.strptime(ts, \"%Y-%m-%d\") - datetime(1970,1,1)).total_seconds() for ts in batch] for batch in timestamps])\n",
    "        unix_timestamps = unix_timestamps / 86400.0 # convert seconds to days\n",
    "        unix_timestamps = unix_timestamps.unsqueeze(-1) # add an extra dimension for broadcasting\n",
    "        print(unix_timestamps.shape)\n",
    "        # Calculate the periodic features\n",
    "        features = []\n",
    "        for period in self.periods:\n",
    "            cos_features = torch.cos((2*np.pi*unix_timestamps)/period)\n",
    "            sin_features = torch.sin((2*np.pi*unix_timestamps)/period)\n",
    "            features.extend([cos_features, sin_features])\n",
    "\n",
    "\n",
    "        features = torch.cat(features, dim=-1)\n",
    "        print(features.shape)\n",
    "\n",
    "        # Calculate the log feature\n",
    "        log_feature = torch.log1p(unix_timestamps)\n",
    "        features = torch.cat([features, log_feature], dim=-1)\n",
    "        print(features.shape)\n",
    "\n",
    "        # Pass features through linear layer\n",
    "        out = self.linear(features)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Define the periods\n",
    "p = [1, 7, 30, 90, 365]\n",
    "\n",
    "# Initialize the Time2Vec module\n",
    "time2vec = Time2Vec(p)\n",
    "\n",
    "# Create a batch of 'YYYY-MM-DD' timestamps\n",
    "\n",
    "# Create a batch of 'YYYY-MM-DD' timestamps\n",
    "timestamps = [[\"2023-07-12\", \"2023-07-13\", \"2023-07-14\"], [\"2023-07-15\", \"2023-07-16\", \"2023-07-17\"]]\n",
    "\n",
    "\n",
    "# Create time embeddings\n",
    "embeddings = time2vec(timestamps)\n",
    "\n",
    "print(embeddings.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
