{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import spacy\n",
    "import tqdm as notebook_tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import  torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this Notebook\n",
    "- Provide Skeleton of the pytorch training Architecture. \n",
    "- Understanding the shapes of different layers \n",
    "- Understanding the datatype needed at each step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "alpha2int = {alphabet[i]:i for i in range(len(alphabet))}\n",
    "int2alpha = {i:alphabet[i] for i in range(len(alphabet))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ---> B\n",
      "B ---> C\n",
      "C ---> D\n",
      "D ---> E\n",
      "E ---> F\n",
      "F ---> G\n",
      "G ---> H\n",
      "H ---> I\n",
      "I ---> J\n",
      "J ---> K\n",
      "K ---> L\n",
      "L ---> M\n",
      "M ---> N\n",
      "N ---> O\n",
      "O ---> P\n",
      "P ---> Q\n",
      "Q ---> R\n",
      "R ---> S\n",
      "S ---> T\n",
      "T ---> U\n",
      "U ---> V\n",
      "V ---> W\n",
      "W ---> X\n",
      "X ---> Y\n",
      "Y ---> Z\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(1,len(alphabet)):\n",
    "    print(f'{alphabet[i-1]} ---> {alphabet[i]}')\n",
    "    X.append(alpha2int[alphabet[i-1]])\n",
    "    y.append(alpha2int[alphabet[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 1, 1)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length=1\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X_train = np.reshape(X, (len(X), seq_length, 1))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chardataset(Dataset):\n",
    "    def __init__(self, X,y):\n",
    "        # Initialize the dataset \n",
    "        self.X_train = torch.tensor(X,dtype=torch.float32)  # X is a numpy array converted to a tensor of type float\n",
    "        self.y_train = torch.tensor(y,dtype=torch.long)  # y is a numpy array converted to a tensor of type long\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.X_train[index] \n",
    "        label = self.y_train[index]\n",
    "    \n",
    "        \n",
    "\n",
    "        return text, label # return the item at index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = chardataset(X_train,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader  \n",
    "- Loading the Dataset Object in the the DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': 1,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params) # create a dataloader for the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Custom Model LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charPred(nn.Module):\n",
    "    def __init__(self, features, n_hidden, n_layers, n_outputs):\n",
    "        super(charPred, self).__init__()\n",
    "        self.T = seq_length # length of the sequence\n",
    "        self.L = n_layers # number of layers\n",
    "        self.D = features # input dimension[features] eg. x1, x2, x3\n",
    "        self.M = n_hidden # hidden layer dimension\n",
    "        self.K = n_outputs # output dimension\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.D, #The number of expected features in one sameple. ex. x1, x2, x3\n",
    "            hidden_size=n_hidden, #The number of features in the hidden state h.\n",
    "            num_layers=n_layers, #The number of recurrent layers.\n",
    "            batch_first=True\n",
    "            ) #If set True, he input and output tensors are provided as (batch, seq_len, feature)\n",
    "            #output of the LSTM is (batch_size, seq_len, hidden_size) \n",
    "        self.fc = nn.Linear(self.M, self.K) # fully connected layer of shape (M, K)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # initial hidden states\n",
    "        \n",
    "        out, hidden = self.lstm(X) # out is of shape (batch_size, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) # out is of shape (batch_size, hidden_size)\n",
    "    \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = charPred(features=1, n_hidden=128, n_layers=2, n_outputs=26)\n",
    "cost_function = torch.nn.CrossEntropyLoss() # loss function expects logits and labels to be of shape (batch_size, num_classes) and (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512, 128])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape) #shape of the parameters of the model matirx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1201 # number of epochs\n",
    "learning_rate = 0.01 # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # optimizer is a class that implements an update rule for parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Step: 25, Loss: 3.3621 , Accuracy: 0.0000\n",
      "Epoch: 101, Step: 25, Loss: 1.4832 , Accuracy: 0.3830\n",
      "Epoch: 201, Step: 25, Loss: 1.1161 , Accuracy: 0.5459\n",
      "Epoch: 301, Step: 25, Loss: 0.9463 , Accuracy: 0.6249\n",
      "Epoch: 401, Step: 25, Loss: 0.8335 , Accuracy: 0.6823\n",
      "Epoch: 501, Step: 25, Loss: 0.7563 , Accuracy: 0.7182\n",
      "Epoch: 601, Step: 25, Loss: 0.7109 , Accuracy: 0.7383\n",
      "Epoch: 701, Step: 25, Loss: 0.6624 , Accuracy: 0.7609\n",
      "Epoch: 801, Step: 25, Loss: 0.6219 , Accuracy: 0.7776\n",
      "Epoch: 901, Step: 25, Loss: 0.5982 , Accuracy: 0.7919\n",
      "Epoch: 1001, Step: 25, Loss: 0.5720 , Accuracy: 0.8047\n",
      "Epoch: 1101, Step: 25, Loss: 0.5593 , Accuracy: 0.8122\n",
      "Epoch: 1201, Step: 25, Loss: 0.5375 , Accuracy: 0.8217\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "loss_per_epoch = []\n",
    "accuracy_list = []\n",
    "accuracy_per_epoch = []\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(training_loader): # data is a tuple of (text, label)\n",
    "        # Forward pass\n",
    "        y_pred = model(data[0]).reshape(-1,26) # reshape the output to be (batch_size, num_classes)\n",
    "        \n",
    "        loss = cost_function(y_pred,data[:][1]) # calculate the loss using the cost function shape (batch_size, num_classes) and (batch_size,)\n",
    "        loss_list.append(loss.item()) # append the loss to the list of losses for each epoch (to be used for plotting)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad() # set all gradients to zero before updating the parameters (to prevent gradient accumulation)\n",
    "        # Backpropagation\n",
    "        loss.backward() # backpropagate the loss to the model\n",
    "        # Update weights\n",
    "        optimizer.step() # update the weights of the model using the optimizer\n",
    "\n",
    "        pred = torch.argmax(y_pred, dim=1) # get the index of the max logit\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "        corrects = torch.sum(pred == data[1]) # calculate the number of correct predictions\n",
    "        accuracy_list.append(corrects.item()/len(data[1])) # append the accuracy to the list of accuracies for each epoch (to be used for plotting)\n",
    "    \n",
    "\n",
    "    loss_per_epoch.append(np.mean(loss_list))\n",
    "    accuracy_per_epoch.append(np.mean(accuracy_list))\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch+1}, Step: {i+1}, Loss: {np.mean(loss_list):.4f} , Accuracy: {np.mean(accuracy_list):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
