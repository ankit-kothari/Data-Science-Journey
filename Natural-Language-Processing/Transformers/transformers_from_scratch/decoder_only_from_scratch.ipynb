{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling  \n",
    "from transformers import Trainer, TrainingArguments \n",
    "import torch  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer \n",
    "import torch.nn.functional as F \n",
    "import os\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_ckpt = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# If the tokenizer does not have a padding token, set it to be the same as the EOS token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "config = AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [  \n",
    "    \"Sachin Tendulkar is regarded as one of the greatest batsmen in the history of cricket.\",  \n",
    "    \"He holds numerous records, including the highest number of runs scored in both Test and One-Day Internationals.\",  \n",
    "    \"Tendulkar made his debut for the Indian cricket team in 1989 and played for 24 years before retiring in 2013.\",  \n",
    "    \"Throughout his career, he received numerous awards and accolades, cementing his legacy as a cricketing legend.\"  \n",
    "]  \n",
    "  \n",
    "text = \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in text: 68\n"
     ]
    }
   ],
   "source": [
    "# I need to count the number of words in the text using the split() method\n",
    "\n",
    "words = text.split()\n",
    "print('Number of words in text:', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TARGET SEQUENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text sequences: 59\n",
      "Number of targets: 59\n"
     ]
    }
   ],
   "source": [
    "train_len = 9\n",
    "\n",
    "text_sequences = []\n",
    "for i in range(train_len, len(words)):\n",
    "    seq = words[i-train_len:i]\n",
    "    text_sequences.append(\" \".join(seq))\n",
    "\n",
    "print('Number of text sequences:', len(text_sequences))\n",
    "\n",
    "target = []\n",
    "for i in range(train_len, len(words)):\n",
    "    target.append(words[i])\n",
    "\n",
    "print('Number of targets:', len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sequence: Sachin Tendulkar is regarded as one of the greatest\n",
      "First target sequence: batsmen\n"
     ]
    }
   ],
   "source": [
    "train_sequences = text_sequences[0]\n",
    "print('First training sequence:', train_sequences)\n",
    "target_sequences = target[0]\n",
    "print('First target sequence:', target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):  \n",
    "    def __init__(self, train_text,target_text ,tokenizer,max_len):  \n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_sequences = train_text\n",
    "        self.labels = target_text\n",
    "        self.max_len = max_len\n",
    "  \n",
    "    def __len__(self):  \n",
    "        return len(self.train_sequences) \n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        train_seq = str(self.train_sequences[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            train_seq,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        encoding_label = self.tokenizer.encode_plus(\n",
    "            label,\n",
    "            add_special_tokens=True,\n",
    "            max_length=1,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        #input_ids=encoding['input_ids'] #shape: (1,9)\n",
    "        #print(f' shape of input_ids: {input_ids.shape}') \n",
    "        #input_ids=encoding['input_ids'].flatten() #shape: (9,)\n",
    "        #print(f' shape of input_ids after flattening: {input_ids.shape}')\n",
    "        return dict(  \n",
    "            input_ids=encoding['input_ids'].flatten(), \n",
    "            attention_mask=encoding['attention_mask'].flatten(),\n",
    "            label=encoding_label['input_ids'].flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = GPT2Dataset(train_sequences, target_sequences, tokenizer, 9)\n",
    "#print(len(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  \"\"\"\n",
    "  Creates a single Dense Embedding for each token --> Token Embedding + Positional Embedding\n",
    "  \"\"\"\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "    self.position_embedding = nn.Embedding(config.n_positions, config.hidden_size)\n",
    "    self.layer_norm = nn.LayerNorm(config.hidden_size, eps= 1e-12)\n",
    "    self.dropout = nn.Dropout()\n",
    "\n",
    "  def forward(self,input_ids):\n",
    "    token_embeddings = self.token_embedding(input_ids)\n",
    "    seq_length = token_embeddings.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0) #shape: [1,seq_length]\n",
    "    position_embeddings = self.position_embedding(position_ids) #shape: [1,seq_length,embedding_dim]\n",
    "    combined_embeddings = token_embeddings + position_embeddings #shape: [1,seq_length,embedding_dim]\n",
    "    normalized_embedding = self.layer_norm(combined_embeddings) #shape: [1,seq_length,embedding_dim]\n",
    "    normalized_embedding = self.dropout(normalized_embedding) #shape: [1,seq_length,embedding_dim]\n",
    "    return normalized_embedding #shape: [1,seq_length,embedding_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of embedding \n",
    "- Intital sentence is tokenized and input ids are passed to embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate same embedding for all the tokens in the sequence\n",
    "\n",
    "#sample input ids\n",
    "input_ids = torch.tensor([[31,51,99],[15,5,0]])\n",
    "print(input_ids.size())\n",
    "embeddings = Embeddings(config)\n",
    "embeddings(input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, embed_dim, head_dim):\n",
    "    super().__init__()\n",
    "    self.head_dim = head_dim #dimension of one head \n",
    "    #infeatures=embed_dim\n",
    "    #outfeatures=head_dim\n",
    "    self.q = nn.Linear(embed_dim, head_dim)\n",
    "    self.k = nn.Linear(embed_dim, head_dim)\n",
    "    self.v = nn.Linear(embed_dim, head_dim)\n",
    "    \n",
    "  \n",
    "  def causal_mask(self,batch_size,size, dtype):  \n",
    "    mask = torch.tril(torch.ones(size,size)).unsqueeze(0)\n",
    "    return mask\n",
    "    \n",
    "  \n",
    "      \n",
    "  def scaled_dot_product_attention(self,query, key, value):\n",
    "    dim_k = query.size(dim=-1)  \n",
    "    #print(dim_k)    \n",
    "    #print(f'Dimension of the q,k,v Matrix [Batch_size, seq_len, Head_dim] of One Head {dim_k}')\n",
    "    scores = torch.bmm(query,key.transpose(1,2))/ sqrt(dim_k)  #[(1,5,768)*(1,768,5)]/sqrt(768) >>> [batch_size,5,5] \n",
    "    \n",
    "    mask = self.causal_mask(scores.size(0),scores.size(1),dtype=torch.int32)\n",
    "    #print(mask)\n",
    "    scores = scores.masked_fill(mask==0, float(0)) \n",
    "    weights = F.softmax(scores, dim=-1) #[batch_size,5,5]\n",
    "    #print(weights)\n",
    "    #print(f'Softmax for each column across one row {weights.shape}')\n",
    "    weights_dot_values = torch.bmm(weights,value) \n",
    "    #print(f'Last Step is to multiply weights and values {weights_dot_values.shape}')\n",
    "    return weights_dot_values \n",
    "\n",
    "  def forward(self, hidden_state):\n",
    "    #print(f'Input Embedding for Each Token with X Matrix {hidden_state.size()}')\n",
    "    #q = X*W_q\n",
    "    q = self.q(hidden_state)\n",
    "    #print(f'Shape of the Query Matrix W_q {q.size()}')\n",
    "    k = self.k(hidden_state)\n",
    "    #print(f'Shape of the Key Matrix W_k {k.size()}')\n",
    "    v = self.k(hidden_state)\n",
    "    #print(f'Shape of the Value Matrix W_k {v.size()}')\n",
    "    #print('-----------------Calculating Self Attention--------------------')\n",
    "    attn_outputs = self.scaled_dot_product_attention(q,k,v)\n",
    "    #print(f'Shape of the attention Output with one Head and Head Dimension {self.head_dim} is {attn_outputs.size()}')\n",
    "    return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one head output example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "from transformers import AutoConfig\n",
    "#\n",
    "text=sentences[0][0:4]\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "#\n",
    "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "token_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "#infeatures= embed_dim---> making of the X matrix \n",
    "input_embedding = token_embedding(inputs.input_ids)\n",
    "head_1 = AttentionHead(768,64)\n",
    "attn_outputs_1 = head_1(input_embedding)\n",
    "attn_outputs_1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    embed_dim = config.hidden_size\n",
    "    num_heads = config.num_attention_heads\n",
    "    head_dim = embed_dim // num_heads\n",
    "    self.heads = [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "    self.w_0 = nn.Linear(embed_dim,embed_dim)\n",
    "\n",
    "  def forward(self,hidden_state):\n",
    "    '''\n",
    "    hidden_state: Input Embedding with dimensions [batch_size, seq_len, embedding_dimension]\n",
    "    '''\n",
    "    attention_outputs = [head(hidden_state) for head in self.heads] #Calculating Self-Attention on each head\n",
    "    contcat_attn_outputs_allheads = torch.cat(attention_outputs, dim=-1) #[batch_size,seq_len, embed_dim]\n",
    "    Z =   self.w_0(contcat_attn_outputs_allheads) #[batch_size, seq_len, embed_dim]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    self.linear1 = nn.Linear(config.hidden_size, 3072)\n",
    "    self.linear2 = nn.Linear(3072, config.hidden_size)\n",
    "    self.gelu = nn.GELU()\n",
    "    self.dropout = nn.Dropout(config.embd_pdrop)\n",
    "  \n",
    "  def forward(self, attention_outputs):\n",
    "    output_l1 = self.linear1(attention_outputs)\n",
    "    activated_outputs = self.gelu(output_l1)\n",
    "    output_l2 = self.linear2(activated_outputs)\n",
    "    output = self.dropout(output_l2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.n_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer of the Decoder Transformer \n",
    "- consist of Multihead Attention: concatenation of all individual attention heads\n",
    "- Feedforward layer: final output layer\n",
    "- Input Embedding : size --> [batch_size, seq_len, embedding_dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super(TransformerDecoderLayer,self).__init__()\n",
    "    self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n",
    "    self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n",
    "    self.multi_attention = MultiHeadAttention(config)\n",
    "    self.feedforward = FeedForward(config)\n",
    "\n",
    "  def forward(self, input_embeddings):\n",
    "     #pre-layer normalization approach\n",
    "     \n",
    "     #Step 1: Applying Layer Normalization to Input Embeddings\n",
    "     normalized_input_embeddings = self.layer_norm1(input_embeddings)\n",
    "     \n",
    "     #Step 2: Applying MultiHeadAttention to Normalized Output\n",
    "     multi_head_attn = self.multi_attention(normalized_input_embeddings)\n",
    "     \n",
    "     #Step 3: Add input embeddings to the Multihead Attention Output\n",
    "     skip_connection_1 = input_embeddings + multi_head_attn\n",
    "\n",
    "     #step 4: Pass the output to another Layer Normalization \n",
    "     layer_norm_2 = self.layer_norm2(skip_connection_1)\n",
    "\n",
    "     #Step 5: Adding skip connection 1 outputs to the output of the FeedForward Network (applied on Step 4)\n",
    "     skip_connection_2 = skip_connection_1 + self.feedforward(layer_norm_2)\n",
    "     #print(f'output of MultiHeadAttention and FeedForward Network is {skip_connection_2.shape}')\n",
    "     return skip_connection_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Module\n",
    "- n_layers: number of layers of the decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferDecoder(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    self.embedding = Embeddings(config)\n",
    "    self.layers = nn.ModuleList([TransformerDecoderLayer(config) for _ in range(config.n_layer)])\n",
    "                                \n",
    "  def forward(self, input_ids):\n",
    "    embeddings = self.embedding(input_ids)\n",
    "    for layer in self.layers:\n",
    "      embeddings = layer(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | decoder_embeddings | TransferDecoder | 103 M \n",
      "1 | dropout            | Dropout         | 0     \n",
      "2 | classifier         | Linear          | 38.6 M\n",
      "-------------------------------------------------------\n",
      "141 M     Trainable params\n",
      "0         Non-trainable params\n",
      "141 M     Total params\n",
      "567.305   Total estimated model params size (MB)\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ankitkothari/Library/Python/3.9/lib/python/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fa5cd204ff4d7681cbc4d98cc409c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 8 and Sequence Length 9\n",
      "Batch Size 3 and Sequence Length 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoderForNextTokenPrediction(LightningModule):\n",
    "  def __init__(self,config):\n",
    "    super().__init__()\n",
    "    self.decoder_embeddings = TransferDecoder(config)\n",
    "    self.dropout = nn.Dropout(config.embd_pdrop)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    N, L = input_ids.shape  # get the batch size and sequence length\n",
    "    print(f'Batch Size {N} and Sequence Length {L}')\n",
    "    decoder_embeddings = self.decoder_embeddings(input_ids) #shape: [batch_size, seq_len, embedding_dim]\n",
    "    drop = self.dropout(decoder_embeddings) #shape: [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "    # Reshape drop to [-1, drop.size(-1)] before applying the classifier\n",
    "    drop = drop.view(-1, drop.size(-1)) #shape: [batch_size*seq_len, embedding_dim]\n",
    "\n",
    "    # Reshape classify back to [N, L, C]\n",
    "    classify =  self.classifier(drop) #shape: [batch_size*seq_len, vocab_size]\n",
    "    \n",
    "    classify = classify.view(N, L, -1) #shape: [batch_size, seq_len, vocab_size]\n",
    "\n",
    "    # Average over the sequence dimension\n",
    "    logits = classify.mean(dim=1) #shape: [batch_size, vocab_size]\n",
    "\n",
    "    return logits\n",
    "  \n",
    "  def training_step(self, batch):\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['label']\n",
    "    logits = self.forward(input_ids) #shape: [batch_size, vocab_size]\n",
    "    labels = labels.view(-1)   #shape: [batch_size, vocab_size]\n",
    "    loss = F.cross_entropy(logits, labels) #shape: [batch_size, vocab_size]\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = TransformerDecoderForNextTokenPrediction(config)\n",
    "train_dataset = GPT2Dataset(train_text=text_sequences,target_text=target,tokenizer=tokenizer,max_len=9)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    ")\n",
    "trainer = Trainer(\n",
    "                  max_epochs=1\n",
    "                  )\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"gpt2_model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 1 and Sequence Length 12\n",
      "leg\n"
     ]
    }
   ],
   "source": [
    "#predicting the next word using a sample text\n",
    "\n",
    "text = \"Sachin Tendulkar is regarded as one of \"\n",
    "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "#load the checkpoint\n",
    "model = TransformerDecoderForNextTokenPrediction.load_from_checkpoint(\"gpt2_model.ckpt\",config=config)\n",
    "logits = model(inputs.input_ids)\n",
    "predicted_index = torch.argmax(logits, dim=-1).item()\n",
    "predicted_text = tokenizer.decode(predicted_index)\n",
    "print(predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1455"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
