{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install transformers\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import openai\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import transformers\n",
        "import ast\n",
        "import pprint\n",
        "import polars as pl\n",
        "import gdown"
      ],
      "metadata": {
        "id": "sgyO55ZwI7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from spacy.lang.en import English\n",
        "import string\n",
        "import xgboost\n",
        "import warnings\n",
        "# Suppress only specific warning types\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "# Add\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "x32Kn2xWOLRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning"
      ],
      "metadata": {
        "id": "BxjUTmPZxifd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(filename):\n",
        "    stopwords = []\n",
        "    with open(filename, \"r\") as f:\n",
        "      stopwords = []\n",
        "      for line in tqdm(f):\n",
        "        line = re.sub(r\"\\n\",\"\",line, flags=re.I)\n",
        "        stopwords.append(line)\n",
        "      return set(stopwords)"
      ],
      "metadata": {
        "id": "6RcPajwIOBsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_file = \"/content/drive/Shareddrives/MSML641 Project/msml_641_project_scripts/mallet_en_stoplist.txt\"\n",
        "stopwords= load_stopwords(stopwords_file)\n",
        "nlp = English(parser=False)\n",
        "def spacy_preprocessing(text):\n",
        "    '''\n",
        "    text: accepts stings text\n",
        "    stopwords: list of stopwords\n",
        "    proceduralwords: list of procedural words in politics\n",
        "    exclude_list: Custom list of words to include ex: ['mr','managers']\n",
        "    clean_tokens: maps words like you're to you are\n",
        "    returns a clean string\n",
        "\n",
        "    Parameters\n",
        "    remove_punctuations: yes removes all puntuations\n",
        "    remove_stopwords:  yes removes all stopwords\n",
        "    remove_nonalpha: yes removes all characters execpt uppercase and lowercase letters\n",
        "    Example: text = text = \"I am soooooo excited Mr. , to learn nlp. s123 2003 you're doing      great. He will be awesome!!   managers for life\"\n",
        "\n",
        "    '''\n",
        "\n",
        "    exclude_list=[]\n",
        "    remove_punctuations='no'\n",
        "    remove_stopwords='no'\n",
        "    remove_nonalpha='yes'\n",
        "\n",
        "\n",
        "    #removing any websit\n",
        "    text = re.sub(r\"http[s]://[a-zA-Z.\\/0-9?=]*\\b\", \" \", text)\n",
        "\n",
        "    # replaces special characters with spaces\n",
        "    if remove_nonalpha == 'yes':\n",
        "        text = re.sub(r'\\b(?![\\w\\']+)\\s*\\W+\\s*\\b', lambda match: ' ' if match.group().strip() else match.group(), text)\n",
        "        #text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "\n",
        "    # replaces multiple character with a word with one like pooooost will be post\n",
        "    text = re.sub(r\"(.)\\1{3,}\", r\"\\1\", text)\n",
        "\n",
        "    # replaces multiple space in the line with single space\n",
        "    text = re.sub(r\"\\s{2,}\", r\" \", text)\n",
        "\n",
        "    clean_text = []\n",
        "\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if (remove_punctuations == 'yes') & (remove_stopwords == 'yes'):\n",
        "            if (token.orth_ not in string.punctuation) & (token.orth_.lower() not in stopwords) & (token.orth_.lower() not in exclude_list):\n",
        "                clean_text.append(token.orth_.lower())\n",
        "        elif (remove_punctuations == 'yes') & (remove_stopwords == 'no'):\n",
        "            if (token.orth_ not in string.punctuation):\n",
        "                clean_text.append(token.orth_.lower())\n",
        "        elif (remove_punctuations == 'no') & (remove_stopwords == 'yes') & (token.orth_.lower() not in exclude_list):\n",
        "            if (token.orth_ not in stopwords) & (\n",
        "                    token.orth_ not in string.punctuation):\n",
        "                clean_text.append(token.orth_.lower())\n",
        "        else:\n",
        "            clean_text.append(token.orth_.lower())\n",
        "            continue\n",
        "    clean_string = \" \".join(clean_text).lstrip()\n",
        "\n",
        "    return clean_string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK268gTqOFYA",
        "outputId": "dcf80c4d-fecd-4822-a1dc-ccdba722c4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "524it [00:00, 145262.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Data"
      ],
      "metadata": {
        "id": "rifOlGA1xmQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "original_data =    (\n",
        "          pl.scan_csv(\"/content/drive/MyDrive/data/Tweets.csv\")\n",
        "          .select(pl.col('text'),\n",
        "                  pl.col('airline_sentiment').alias(\"label\"))\n",
        "          .collect()\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neI6SqgYZZ4B",
        "outputId": "c5ddc42b-254f-451a-9cd8-40e3b3ffe0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15.2 ms, sys: 9.68 ms, total: 24.8 ms\n",
            "Wall time: 49.8 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaned Data"
      ],
      "metadata": {
        "id": "heyK2CNZxoUP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAWdR2R2I3yF",
        "outputId": "1438b71e-d6d9-4a3d-c04a-dcf0eb740855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.54 s, sys: 32.8 ms, total: 3.57 s\n",
            "Wall time: 3.67 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "mapping={\"positive\":1,\"neutral\":2,\"negative\":0}\n",
        "data =    (\n",
        "          pl.scan_csv(\"/content/drive/MyDrive/data/Tweets.csv\")\n",
        "          .select(pl.col('text').str.replace(r'@\\w+\\b',''),\n",
        "                  pl.col('airline_sentiment').alias(\"label\"))\n",
        "          .with_columns(pl.col(\"label\").map_dict(mapping))\n",
        "          .with_columns(pl.col(\"text\").apply(spacy_preprocessing))\n",
        "          .collect()\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_data['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zf1SCnp8ZgSJ",
        "outputId": "2da3a882-c5e2-41ae-ed24-1234f886efc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@VirginAmerica I didn't today... Must mean I need to take another trip!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oqV813DoQYzj",
        "outputId": "10e160e2-cb5c-4861-f590-ae41b19c69cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i did n't today must mean i need to take another trip !\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter OpenAI Key"
      ],
      "metadata": {
        "id": "-ujYAb38xGvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put down your API key here\n",
        "\n",
        "openai.api_key = \"<ENTER OPENAI API KEY>\""
      ],
      "metadata": {
        "id": "x3we-afXibyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Embeddings"
      ],
      "metadata": {
        "id": "748duj-VxKfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   response=openai.Embedding.create(input = [text], model=model)\n",
        "   return response['data'][0]['embedding']\n"
      ],
      "metadata": {
        "id": "JkwsJ709hzVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_emb =(\n",
        "            data\n",
        "            .select(pl.col('text'))\n",
        "            .with_columns(pl.col(\"text\").apply(get_embedding))\n",
        "            .select(pl.col(\"text\").reshape((data.shape[0], -1))\n",
        "                    .arr.to_struct(n_field_strategy=\"max_width\")\n",
        "                    ).unnest(\"text\")\n",
        "          )"
      ],
      "metadata": {
        "id": "_GZt39SojXlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKB33W-BdrVT",
        "outputId": "79e4cb51-7ca9-42d4-aeb1-86c73e7dd7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_emb.write_parquet(\"/content/drive/MyDrive/data/Tweets_openai_embeddings.parquet\", compression=\"zstd\")"
      ],
      "metadata": {
        "id": "9LNmFx8OfWi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "lg1bOqz8xOUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_emb, data['label'], test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "HlcI2u8zkKxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d_train = xgboost.DMatrix(X_train.to_arrow(), y_train.to_arrow())\n",
        "pred = xgboost.DMatrix(X_test.to_arrow())\n",
        "\n",
        "params = {\n",
        "    'objective': 'multi:softmax',   # Multi-class classification task (softmax output)\n",
        "    'num_class': 3,                 # Number of classes (labels)\n",
        "    'eval_metric': 'mlogloss',      # Multi-class logarithmic loss metric\n",
        "    'eta': 0.1,                     # Learning rate (Typical values: 0.01 - 0.3)\n",
        "    'max_depth': 6,                 # Maximum depth of a tree (Typical values: 3 - 10)\n",
        "    'min_child_weight': 1,          # Minimum sum of instance weight needed in a child (Typical values: 1 - 10)\n",
        "    'subsample': 0.8,               # Subsample ratio of the training instances (Typical values: 0.5 - 1.0)\n",
        "    'colsample_bytree': 0.8,        # Subsample ratio of columns when constructing each tree (Typical values: 0.5 - 1.0)\n",
        "    'gamma': 0.70,                     # Minimum loss reduction required to make a further partition on a leaf node\n",
        "    'seed': 42,\n",
        "    'alpha': 2e-05,\n",
        "    'max_depth': 6                    # Random seed for reproducibility\n",
        "}"
      ],
      "metadata": {
        "id": "SOUlxAci8e0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Watchlist to monitor the training and validation performance\n",
        "watchlist = [(d_train, 'train')]\n",
        "\n",
        "# Train the XGBoost model\n",
        "num_rounds = 80  # Number of boosting rounds (epochs)\n",
        "bst = xgboost.train(params, d_train, num_rounds, evals=watchlist, early_stopping_rounds=10, verbose_eval=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ5sTuM8XBt1",
        "outputId": "aa0ef053-0187-46be-cdd6-7ca2b63cce4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-mlogloss:1.01618\n",
            "[10]\ttrain-mlogloss:0.56195\n",
            "[20]\ttrain-mlogloss:0.37269\n",
            "[30]\ttrain-mlogloss:0.26961\n",
            "[40]\ttrain-mlogloss:0.20441\n",
            "[50]\ttrain-mlogloss:0.16080\n",
            "[60]\ttrain-mlogloss:0.12928\n",
            "[70]\ttrain-mlogloss:0.10629\n",
            "[79]\ttrain-mlogloss:0.09052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "EYWCf8fZxRxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bst.predict(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4p-o4IQseka",
        "outputId": "7a1ac671-e613-478b-d8e0-b86c14d44178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = bst.predict(pred)\n",
        "y_pred_prob = bst.predict(pred)\n",
        "preds = pl.Series(\"preds\", y_pred)\n",
        "pred_probs = pl.Series(\"preds\", y_pred_prob).to_numpy()"
      ],
      "metadata": {
        "id": "Kx8s95uLc7WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGEAsf6eeKwY",
        "outputId": "923b4118-7875-47fb-9e88-330336ffa2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92      2814\n",
            "           1       0.86      0.75      0.80       694\n",
            "           2       0.75      0.64      0.69       884\n",
            "\n",
            "    accuracy                           0.86      4392\n",
            "   macro avg       0.83      0.78      0.80      4392\n",
            "weighted avg       0.85      0.86      0.85      4392\n",
            "\n"
          ]
        }
      ]
    }
  ]
}