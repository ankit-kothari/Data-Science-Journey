{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5006d61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T12:28:12.936164Z",
     "start_time": "2022-03-18T12:28:05.106498Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import gzip\n",
    "import codecs\n",
    "import string\n",
    "from math import log2\n",
    "from collections import Counter\n",
    "from spacy.lang.en import English\n",
    "from traceback_with_variables import activate_by_import\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e39a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T12:28:12.940822Z",
     "start_time": "2022-03-18T12:28:12.938014Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_punctuation(ngrams):\n",
    "    # Input: assume ngrams is a list of ['token1','token2'] bigrams\n",
    "    # Removes ngrams like ['today','.'] where either token is a single punctuation character\n",
    "    # Note that this does not mean tokens that merely *contain* punctuation, e.g. \"'s\"\n",
    "    # Returns list with the items that were not removed\n",
    "    punct = string.punctuation\n",
    "    return [ngram.lower()   for ngram in ngrams   if ngram not in punct]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cf93d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T12:39:50.112889Z",
     "start_time": "2022-03-18T12:39:50.108654Z"
    }
   },
   "outputs": [],
   "source": [
    "def ngrams(tokens, n):\n",
    "    grams=[]\n",
    "    for i in range(1,len(tokens)):\n",
    "       gram = tokens[i-1:i+n-1]\n",
    "       if len(gram)==n:\n",
    "            grams.append(gram)\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca9052b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:10:42.669656Z",
     "start_time": "2022-03-19T03:10:42.655268Z"
    }
   },
   "outputs": [],
   "source": [
    "R='However, I consider that racist, sexist or other discriminatory language, and allegations against Members, are offensive.'\n",
    "A='However, I consider racist language, sexist or other discrimination, and allegations against Members to be offensive.'\n",
    "B='However, I regard racist language, language that discriminates on the basis of sex or on any other grounds, and allegations against Members, as offensive.'\n",
    "C='Racist Members consider that discriminatory allegations as language are the basis of offensive sexist allegations, however.'\n",
    "D='Allegations against members are offensive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbed971d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T12:59:35.582164Z",
     "start_time": "2022-03-18T12:59:35.576657Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tokens(line,n):\n",
    "    nlp= English(parser=False) # faster init with parse=False, if only using for tokenization\n",
    "    doc1 = nlp(line)\n",
    "    word_tokens=[]\n",
    "    for token in doc1:\n",
    "        word_tokens.append(token.text)\n",
    "    normalized_tokens = filter_punctuation(word_tokens)\n",
    "    sents = ngrams(normalized_tokens, n)\n",
    "    new_sents = [sent for sent_sublist in sents for sent in sent_sublist]\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ed754",
   "metadata": {},
   "source": [
    "### (R,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6712e781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T13:00:30.085176Z",
     "start_time": "2022-03-18T13:00:29.815331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'i', 'consider', 'that', 'racist', 'sexist', 'or', 'other', 'discriminatory', 'language', 'and', 'allegations', 'against', 'members', 'are']\n",
      "['however', 'i', 'consider', 'racist', 'language', 'sexist', 'or', 'other', 'discrimination', 'and', 'allegations', 'against', 'members', 'to', 'be']\n"
     ]
    }
   ],
   "source": [
    "one_n_R = clean_tokens(R,1)\n",
    "print(one_n_R)\n",
    "one_n_C = clean_tokens(A,1)\n",
    "print(one_n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b98d1da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-18T13:06:53.038474Z",
     "start_time": "2022-03-18T13:06:53.031614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.800000\n",
      "Individual 2-gram: 0.500000\n",
      "Individual 3-gram: 0.307692\n",
      "Individual 4-gram: 0.083333\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [one_n_R] # list of references for 1 sentence. # list of references for all sentences in corpus.\n",
    "candidate = one_n_C # list of hypotheses that corresponds to list of references.\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b62f5",
   "metadata": {},
   "source": [
    "### (R,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86053eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:10:47.839033Z",
     "start_time": "2022-03-19T03:10:47.449548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'i', 'consider', 'that', 'racist', 'sexist', 'or', 'other', 'discriminatory', 'language', 'and', 'allegations', 'against', 'members', 'are']\n",
      "['however', 'i', 'regard', 'racist', 'language', 'language', 'that', 'discriminates', 'on', 'the', 'basis', 'of', 'sex', 'or', 'on', 'any', 'other', 'grounds', 'and', 'allegations', 'against', 'members', 'as']\n"
     ]
    }
   ],
   "source": [
    "one_n_R = clean_tokens(R,1)\n",
    "print(one_n_R)\n",
    "one_n_C = clean_tokens(B,1)\n",
    "print(one_n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00f9cdc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:10:50.049182Z",
     "start_time": "2022-03-19T03:10:50.032659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.478261\n",
      "Individual 2-gram: 0.181818\n",
      "Individual 3-gram: 0.095238\n",
      "Individual 4-gram: 0.050000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [one_n_R] # list of references for 1 sentence. # list of references for all sentences in corpus.\n",
    "candidate = one_n_C # list of hypotheses that corresponds to list of references.\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdf7fa",
   "metadata": {},
   "source": [
    "### (R,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34c0f769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:11:01.537411Z",
     "start_time": "2022-03-19T03:11:01.233151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'i', 'consider', 'that', 'racist', 'sexist', 'or', 'other', 'discriminatory', 'language', 'and', 'allegations', 'against', 'members', 'are']\n",
      "['racist', 'members', 'consider', 'that', 'discriminatory', 'allegations', 'as', 'language', 'are', 'the', 'basis', 'of', 'offensive', 'sexist', 'allegations']\n"
     ]
    }
   ],
   "source": [
    "one_n_R = clean_tokens(R,1)\n",
    "print(one_n_R)\n",
    "one_n_C = clean_tokens(C,1)\n",
    "print(one_n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff30dc5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:11:09.058079Z",
     "start_time": "2022-03-19T03:11:09.047495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.600000\n",
      "Individual 2-gram: 0.071429\n",
      "Individual 3-gram: 0.000000\n",
      "Individual 4-gram: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [one_n_R] # list of references for 1 sentence. # list of references for all sentences in corpus.\n",
    "candidate = one_n_C # list of hypotheses that corresponds to list of references.\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54d2eb",
   "metadata": {},
   "source": [
    "### (R,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c58cbabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:11:24.043130Z",
     "start_time": "2022-03-19T03:11:23.745942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['however', 'i', 'consider', 'that', 'racist', 'sexist', 'or', 'other', 'discriminatory', 'language', 'and', 'allegations', 'against', 'members', 'are']\n",
      "['allegations', 'against', 'members', 'are']\n"
     ]
    }
   ],
   "source": [
    "one_n_R = clean_tokens(R,1)\n",
    "print(one_n_R)\n",
    "one_n_C = clean_tokens(D,1)\n",
    "print(one_n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62ea0b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T03:11:31.100193Z",
     "start_time": "2022-03-19T03:11:31.092594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.063928\n",
      "Individual 2-gram: 0.063928\n",
      "Individual 3-gram: 0.063928\n",
      "Individual 4-gram: 0.063928\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [one_n_R] # list of references for 1 sentence. # list of references for all sentences in corpus.\n",
    "candidate = one_n_C # list of hypotheses that corresponds to list of references.\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6de36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
