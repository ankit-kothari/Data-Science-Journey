{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ankitkothari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ankitkothari/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkothari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ankitkothari/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ankitkothari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import slack \n",
    "from slack_sdk import WebClient\n",
    "from slack_sdk import rtm # Real Time Messaging Client\n",
    "\n",
    "from slack_sdk.errors import SlackApiError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#SKLEARN\n",
    "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "\n",
    "#TENSORFLOW\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONNECT THE SLACKBOT BY THIS SECRET TOKEN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLACK_BOT_TOKEN = '<bot-token>'\n",
    "\n",
    "slack_clients = WebClient(token=SLACK_BOT_TOKEN)\n",
    "rtmclient = slack.RTMClient(token=SLACK_BOT_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<slack_sdk.rtm.RTMClient at 0x7f8ab9032610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtmclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA: QUESTIONS AND RESPONSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/14hq37js5_7g92qm54090zp00000gn/T/ipykernel_1058/887268334.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the best temperature to brew coffee?</td>\n",
       "      <td>According to chemical studies, the optimal water temperature for drip coffee is 95-98C. According to my notes, colder water doesn't extract enough caffeine/essential oils from the beans, and above such temperature the acidity increases wildly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quality of coffee</td>\n",
       "      <td>The quality of a brew depends on the following factors (in no particular order):\\nTime since grinding the beans.\\nTime since roasting.\\nCleanliness with brewing equipment.\\nBean quality (what crop, etc.).\\nWater quality.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the difference between arabica and robusta?</td>\n",
       "      <td>Arabica beans and robusta beans are two different species of coffee. They are the primary species of coffee that find their way into the American cup. The general differences are those of taste, and the conditions under which the two species differ in production.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just how much ground coffee do I need for x amount of coffee?</td>\n",
       "      <td>a. Whatever seems right to you. b. It may change slightly from coffee to coffee and according to freshness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the different between Preparation Methods\\n</td>\n",
       "      <td>Drip\\nFrench Press\\nEspresso\\nPercolator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Context  \\\n",
       "0  What is the best temperature to brew coffee?                    \n",
       "1  Quality of coffee                                               \n",
       "2  What is the difference between arabica and robusta?             \n",
       "3  Just how much ground coffee do I need for x amount of coffee?   \n",
       "4  What are the different between Preparation Methods\\n            \n",
       "\n",
       "                                                                                                                                                                                                                                                             Text Response  \n",
       "0  According to chemical studies, the optimal water temperature for drip coffee is 95-98C. According to my notes, colder water doesn't extract enough caffeine/essential oils from the beans, and above such temperature the acidity increases wildly.                      \n",
       "1  The quality of a brew depends on the following factors (in no particular order):\\nTime since grinding the beans.\\nTime since roasting.\\nCleanliness with brewing equipment.\\nBean quality (what crop, etc.).\\nWater quality.\\n                                           \n",
       "2  Arabica beans and robusta beans are two different species of coffee. They are the primary species of coffee that find their way into the American cup. The general differences are those of taste, and the conditions under which the two species differ in production.  \n",
       "3  a. Whatever seems right to you. b. It may change slightly from coffee to coffee and according to freshness.                                                                                                                                                              \n",
       "4  Drip\\nFrench Press\\nEspresso\\nPercolator                                                                                                                                                                                                                                 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "data = pd.read_csv('./coffee.csv')\n",
    "data.ffill(axis=0, inplace= True)\n",
    "context = data['Context'].values\n",
    "context_string = data['Context'].str.cat(sep='\\n')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    unk what best temperature brew coffee             \n",
       "1    unk quality coffee                                \n",
       "2    unk what difference arabica robusta               \n",
       "3    unk just much ground coffee i need x amount coffee\n",
       "4    unk what different preparation methods            \n",
       "Name: nltk_cleaning, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nltk_cleaning(text):\n",
    "  token_text = word_tokenize(text)\n",
    "  clean_text = [\"unk\"]\n",
    "  lemma = wordnet.WordNetLemmatizer()\n",
    "  tag_list = pos_tag(token_text, tagset=None)\n",
    "  for token, pos_token in tag_list:\n",
    "   if token not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*''+,-/:;``<=>[``?@[\\\\]^_`''{|}~\\t\\n`\\'\\'' and (token not in stopwords):\n",
    "     if pos_token.startswith('V'):  # Verb\n",
    "         pos_val='v'\n",
    "     elif pos_token.startswith('J'): # Adjective\n",
    "         pos_val='a'\n",
    "     elif pos_token.startswith('R'): # Adverb\n",
    "         pos_val='r'\n",
    "     else:\n",
    "         pos_val='n' # Noun\n",
    "     lemma_token= lemma.lemmatize(token,pos_val)\n",
    "     clean_text.append(lemma_token.lower())\n",
    "   else:\n",
    "      continue \n",
    "  return \" \".join(clean_text)\n",
    "data['nltk_cleaning']= data['Context'].apply(nltk_cleaning)\n",
    "data['nltk_cleaning'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMBEDDING DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "####word embeddings \n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('./glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of sequences 27\n",
      "[[1, 3, 17, 18, 9, 2], [1, 19, 2], [1, 3, 10, 20, 21], [1, 22, 5, 23, 2, 24, 25, 26, 27, 2], [1, 3, 6, 28, 29], [1, 30, 5, 2], [1, 31, 2], [1, 3, 32, 2, 11], [1, 3, 6, 11, 33], [1, 4, 5, 7, 8, 2], [1, 4, 12, 34, 8, 2], [1, 3, 35, 36, 2], [1, 37, 2, 13], [1, 38, 14, 2, 39], [1, 4, 40, 7, 41, 2], [1, 42, 2, 43], [1, 4, 2, 44], [1, 3, 10, 45, 46, 2], [1, 47, 2, 48, 49], [1, 3, 2, 7], [1, 4, 50, 2, 51], [1, 4, 12, 8, 2, 52, 14, 53], [1, 3, 54], [1, 3, 55, 56, 9, 6, 57, 2], [1, 15, 2, 58, 16, 13], [1, 15, 2, 59, 16, 60], [1, 61, 62, 2, 63]]\n",
      "63\n",
      "[['unk', 'what', 'best', 'temperature', 'brew', 'coffee'], ['unk', 'quality', 'coffee'], ['unk', 'what', 'difference', 'arabica', 'robusta'], ['unk', 'just', 'much', 'ground', 'coffee', 'i', 'need', 'x', 'amount', 'coffee'], ['unk', 'what', 'different', 'preparation', 'methods'], ['unk', 'effects', 'much', 'coffee'], ['unk', 'varieties', 'coffee'], ['unk', 'what', 'mean', 'coffee', 'roast'], ['unk', 'what', 'different', 'roast', 'degree'], ['unk', 'how', 'much', 'caffeine', 'cup', 'coffee'], ['unk', 'how', 'many', 'calorie', 'cup', 'coffee'], ['unk', 'what', 'fair', 'trade', 'coffee'], ['unk', 'is', 'coffee', 'bad'], ['unk', 'can', 'drink', 'coffee', 'pregnant'], ['unk', 'how', 'long', 'caffeine', 'last', 'coffee'], ['unk', 'where', 'coffee', 'come'], ['unk', 'how', 'coffee', 'decaffeinate'], ['unk', 'what', 'difference', 'filter', 'instant', 'coffee'], ['unk', 'why', 'coffee', 'become', 'popular'], ['unk', 'what', 'coffee', 'caffeine'], ['unk', 'how', 'remove', 'coffee', 'stain'], ['unk', 'how', 'many', 'cup', 'coffee', 'safe', 'drink', 'day'], ['unk', 'what', 'espresso'], ['unk', 'what', '’', 'cold', 'brew', 'different', 'iced', 'coffee'], ['unk', 'does', 'coffee', 'ever', 'go', 'bad'], ['unk', 'does', 'coffee', 'make', 'go', 'poop'], ['unk', 'who', 'produce', 'coffee', 'world']]\n"
     ]
    }
   ],
   "source": [
    "sentences = data['nltk_cleaning'].fillna(\"DUMMY_VALUE\").values\n",
    "tokenizer= tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"length of sequences\",len(sequences))\n",
    "max_len = [len(s) for s in sentences]\n",
    "#max_index = np.argmax(max_len)\n",
    "#print(max_len[max_index])\n",
    "print(sequences)\n",
    "print(len(tokenizer.index_word))\n",
    "token_words = [[tokenizer.index_word[i] for i in j] for j in sequences]\n",
    "print(token_words)\n",
    "data['question_tokens']=token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [unk, what, best, temperature, brew, coffee]                 \n",
       "1     [unk, quality, coffee]                                       \n",
       "2     [unk, what, difference, arabica, robusta]                    \n",
       "3     [unk, just, much, ground, coffee, i, need, x, amount, coffee]\n",
       "4     [unk, what, different, preparation, methods]                 \n",
       "5     [unk, effects, much, coffee]                                 \n",
       "6     [unk, varieties, coffee]                                     \n",
       "7     [unk, what, mean, coffee, roast]                             \n",
       "8     [unk, what, different, roast, degree]                        \n",
       "9     [unk, how, much, caffeine, cup, coffee]                      \n",
       "10    [unk, how, many, calorie, cup, coffee]                       \n",
       "11    [unk, what, fair, trade, coffee]                             \n",
       "12    [unk, is, coffee, bad]                                       \n",
       "13    [unk, can, drink, coffee, pregnant]                          \n",
       "14    [unk, how, long, caffeine, last, coffee]                     \n",
       "15    [unk, where, coffee, come]                                   \n",
       "16    [unk, how, coffee, decaffeinate]                             \n",
       "17    [unk, what, difference, filter, instant, coffee]             \n",
       "18    [unk, why, coffee, become, popular]                          \n",
       "19    [unk, what, coffee, caffeine]                                \n",
       "20    [unk, how, remove, coffee, stain]                            \n",
       "21    [unk, how, many, cup, coffee, safe, drink, day]              \n",
       "22    [unk, what, espresso]                                        \n",
       "23    [unk, what, ’, cold, brew, different, iced, coffee]          \n",
       "24    [unk, does, coffee, ever, go, bad]                           \n",
       "25    [unk, does, coffee, make, go, poop]                          \n",
       "26    [unk, who, produce, coffee, world]                           \n",
       "Name: question_tokens, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINING SOME SENTENCE EMBEDDING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPUTING VECTORS FOR THE QUESTIONS\n",
    "def question_embedding(ss1):\n",
    "  v1=np.mean([word2vec[word] if word in word2vec else word2vec['unk'] for word in ss1], axis=0)\n",
    "  #print(v1)\n",
    "  return v1\n",
    "\n",
    "def cosine_similarity_sentence(ss1,ss2):\n",
    "  v1=np.mean([word2vec[word] for word in ss1], axis=0)\n",
    "  v2=np.mean([word2vec[word] for word in ss2], axis=0)\n",
    "  cosine_sent = 1- pairwise_distances([v1],[v2], metric = 'cosine' )\n",
    "  return cosine_sent[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_list = [question_embedding(x) for x in data['question_tokens']]\n",
    "que_array = np.array(que_list)\n",
    "que_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30071 , -0.46867 , -0.20617 , -0.80978 , -0.23889 ,  0.24329 ,\n",
       "        0.016538, -0.035687, -0.22306 ,  0.95189 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assert('UNK' in word2idx)\n",
    "word2vec['unk'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= np.array([str(i)+\"_vector\" for i in range(100)])\n",
    "X = X.reshape(1,100)\n",
    "print(len(que_array))\n",
    "match_frame = pd.DataFrame(que_array)\n",
    "match_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.102502</td>\n",
       "      <td>0.175150</td>\n",
       "      <td>0.065827</td>\n",
       "      <td>-0.148238</td>\n",
       "      <td>-0.327283</td>\n",
       "      <td>0.180668</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.100618</td>\n",
       "      <td>0.100321</td>\n",
       "      <td>-0.716597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104339</td>\n",
       "      <td>0.157619</td>\n",
       "      <td>-0.077268</td>\n",
       "      <td>0.090150</td>\n",
       "      <td>0.083713</td>\n",
       "      <td>-0.128634</td>\n",
       "      <td>-0.027135</td>\n",
       "      <td>-0.116385</td>\n",
       "      <td>-0.288812</td>\n",
       "      <td>0.405632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.076347</td>\n",
       "      <td>0.214803</td>\n",
       "      <td>0.022590</td>\n",
       "      <td>-0.203693</td>\n",
       "      <td>-0.361413</td>\n",
       "      <td>0.288328</td>\n",
       "      <td>0.208896</td>\n",
       "      <td>0.083021</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>-0.646027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>0.244567</td>\n",
       "      <td>-0.331037</td>\n",
       "      <td>-0.016966</td>\n",
       "      <td>-0.047183</td>\n",
       "      <td>0.125363</td>\n",
       "      <td>0.192513</td>\n",
       "      <td>-0.123237</td>\n",
       "      <td>-0.193994</td>\n",
       "      <td>-0.009810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.284020</td>\n",
       "      <td>-0.108906</td>\n",
       "      <td>0.137059</td>\n",
       "      <td>-0.066694</td>\n",
       "      <td>-0.187683</td>\n",
       "      <td>0.359326</td>\n",
       "      <td>0.116956</td>\n",
       "      <td>-0.238365</td>\n",
       "      <td>-0.136817</td>\n",
       "      <td>-0.536491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114330</td>\n",
       "      <td>0.209220</td>\n",
       "      <td>-0.041963</td>\n",
       "      <td>0.091886</td>\n",
       "      <td>-0.205666</td>\n",
       "      <td>0.181544</td>\n",
       "      <td>0.111194</td>\n",
       "      <td>-0.081522</td>\n",
       "      <td>-0.220932</td>\n",
       "      <td>0.055634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.240735</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.026817</td>\n",
       "      <td>-0.070115</td>\n",
       "      <td>-0.247514</td>\n",
       "      <td>0.227813</td>\n",
       "      <td>0.126356</td>\n",
       "      <td>-0.065146</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>-1.372090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019242</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>-0.072460</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>0.170958</td>\n",
       "      <td>-0.092773</td>\n",
       "      <td>0.038488</td>\n",
       "      <td>-0.185478</td>\n",
       "      <td>-0.069213</td>\n",
       "      <td>0.151109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.165602</td>\n",
       "      <td>0.063018</td>\n",
       "      <td>0.225127</td>\n",
       "      <td>-0.284632</td>\n",
       "      <td>-0.012935</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>-0.177189</td>\n",
       "      <td>0.064056</td>\n",
       "      <td>-0.061336</td>\n",
       "      <td>-1.288962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067929</td>\n",
       "      <td>-0.188518</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.220693</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>-0.013229</td>\n",
       "      <td>-0.056367</td>\n",
       "      <td>-0.084657</td>\n",
       "      <td>-0.061982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.102502  0.175150  0.065827 -0.148238 -0.327283  0.180668  0.005131   \n",
       "1  0.076347  0.214803  0.022590 -0.203693 -0.361413  0.288328  0.208896   \n",
       "2 -0.284020 -0.108906  0.137059 -0.066694 -0.187683  0.359326  0.116956   \n",
       "3 -0.240735  0.099508  0.026817 -0.070115 -0.247514  0.227813  0.126356   \n",
       "4 -0.165602  0.063018  0.225127 -0.284632 -0.012935  0.018933 -0.177189   \n",
       "\n",
       "          7         8         9  ...       290       291       292       293  \\\n",
       "0  0.100618  0.100321 -0.716597  ... -0.104339  0.157619 -0.077268  0.090150   \n",
       "1  0.083021  0.009539 -0.646027  ... -0.060357  0.244567 -0.331037 -0.016966   \n",
       "2 -0.238365 -0.136817 -0.536491  ... -0.114330  0.209220 -0.041963  0.091886   \n",
       "3 -0.065146  0.114613 -1.372090  ... -0.019242 -0.002697 -0.072460  0.007849   \n",
       "4  0.064056 -0.061336 -1.288962  ... -0.067929 -0.188518  0.001878  0.220693   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.083713 -0.128634 -0.027135 -0.116385 -0.288812  0.405632  \n",
       "1 -0.047183  0.125363  0.192513 -0.123237 -0.193994 -0.009810  \n",
       "2 -0.205666  0.181544  0.111194 -0.081522 -0.220932  0.055634  \n",
       "3  0.170958 -0.092773  0.038488 -0.185478 -0.069213  0.151109  \n",
       "4  0.009107  0.054296 -0.013229 -0.056367 -0.084657 -0.061982  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pairwise distance provide distance between two array.so more pairwise distance means less similarity.while cosine similarity is 1-pairwise_distance so more cosine similarity means more similarity between two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'how many cups can i have in a day'\n",
    "Question = nltk_cleaning(sent).split(' ')\n",
    "q_array = np.array(Question)\n",
    "Question_tf = question_embedding(q_array)\n",
    "Question_tf=Question_tf.reshape(-1,EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On average, the regular person is safe drinking 3-5 cups of coffee a day. Ultimately, its up to you to understand how your body responds to coffee, and to judge when you shouldn’t have any more. If you respond well to large amounts of caffeine, and don’t become jittery or get stomach pain, then you can drink what you want, but if you know that drinking lots of coffee keeps you up at night and leads to headaches in the morning, cut down to 3 cups a day. Drinking more than 5 cups of coffee a day has been linked to a raise in cholesterol, so if you’re a heavy coffee drinker sticking to 5 is the recommended amount.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_value_tf = 1- pairwise_distances(match_frame, Question_tf, metric = 'cosine' )\n",
    "index_value = cosine_value_tf.argmax() \n",
    "answer = data['Text Response'].iloc[index_value]\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Incoming Commands from Slack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to interpret the incoming message from Slack and send appropriate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import slack\n",
    "RTM_READ_DELAY=30\n",
    "\n",
    "@slack.RTMClient.run_on(event='message')\n",
    "def coffee_helper_bot(**payload):\n",
    "    user_data = payload['data']\n",
    "    #print(data)\n",
    "    command =  user_data['text']\n",
    "    channel_id =  user_data['channel']\n",
    "    thread_ts =  user_data['ts']\n",
    "    user =  user_data['user']\n",
    "    event_subtype = user_data.get('subtype')\n",
    "    print(event_subtype)\n",
    "    \n",
    "\n",
    "    # Return data to only authorised users\n",
    "    accepted_users = ['UHYFFGK7S']\n",
    "    ending_text = ['bye','done', 'thanks', 'exit', 'ok', 'x']\n",
    "    \n",
    "    command = user_data['text']\n",
    "    #print(command)\n",
    "    \n",
    "    #Check if the user is authorised to use the bot\n",
    "    if user not in accepted_users:\n",
    "        response = \"Not an authorised user\"\n",
    "    elif command in ending_text:\n",
    "             response = \"Bye! Thanks for chatting\"\n",
    "    else:\n",
    "        #preprocess the command and clean it\n",
    "        Question = nltk_cleaning(command).split(' ')\n",
    "        #convert the question to array\n",
    "        q_array = np.array(Question)\n",
    "        #convert the question to embedding\n",
    "        Question_tf = question_embedding(q_array)\n",
    "        #reshape the question to 1,300\n",
    "        Question_tf=Question_tf.reshape(-1,EMBEDDING_DIM)\n",
    "        #calculate the cosine similarity\n",
    "        cosine_value_tf = 1- pairwise_distances(match_frame, Question_tf, metric = 'cosine' )\n",
    "        #get the highest value for the similarity\n",
    "        value = cosine_value_tf.max()\n",
    "        print(f' cosine value {value}')\n",
    "        if value > 0.3:\n",
    "            #get the index of the highest value\n",
    "            index_value = cosine_value_tf.argmax() \n",
    "        #   get the answer from the dataframe\n",
    "            response = data['Text Response'].iloc[index_value]\n",
    "        else:\n",
    "            response = \"Sorry, I don't understand the question\"\n",
    "        #print(response)\n",
    "\n",
    "    \n",
    "    webclient = payload['web_client']\n",
    "    # Send the response back to the channel where the message was posted\n",
    "    webclient.chat_postMessage(\n",
    "            channel=channel_id,\n",
    "            thread_ts=thread_ts, \n",
    "            text=response,\n",
    "            icon_emoji=':coffee:'\n",
    "        )\n",
    "    time.sleep(RTM_READ_DELAY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the RTM client and call the \n",
    "#rtmclient.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rtmclient.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
